{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette première session de NLP va vous donner les bases de la manipulation de texte en vue du data challenge qui vous attend après l'été !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download fr_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing avec spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement de la librairie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Importer la classe `French` de `spacy.lang.fr`\n",
    "- Créer l'objet `nlp` avec le constructeur de la classe `French`\n",
    "- Créer un `doc` et afficher son texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importez la classe de langage French\n",
    "from spacy.lang.____ import ____\n",
    "\n",
    "# Créez l'objet nlp\n",
    "nlp = ____\n",
    "\n",
    "# Traitement du texte\n",
    "doc = nlp(\"Ceci est une phrase narcissique puisqu'elle \"\n",
    "          \"ne parle que d'elle-même.\")\n",
    "\n",
    "# Affichez le texte du document\n",
    "print(____.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importez la classe de langage French\n",
    "from spacy.lang.fr import French\n",
    "\n",
    "# Créez l'objet nlp\n",
    "nlp = French()\n",
    "\n",
    "# Traitement du texte\n",
    "doc = nlp(\"Ceci est une phrase narcissique puisqu'elle \"\n",
    "          \"ne parle que d'elle-même.\")\n",
    "\n",
    "# Affichez le texte du document\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**L'objet NLP**\n",
    "\n",
    "- contient le 'pipeline' des calculs\n",
    "- inclut des règles spécifiques au langage pour la tokenization, etc.\n",
    "\n",
    "**L'objet doc**\n",
    "\n",
    "Contient un document, c'est-à-dire un ensemble de *tokens*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulation d'un objet `Doc`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objet `Doc`se manipule comme une liste. \n",
    "\n",
    "A l'aide des `[]`:\n",
    "- Accedez au premier token de doc\n",
    "- Selectionnez les mots `'est une'` dans une slice\n",
    "- Selectionnez les mots entre `'est une phrase narcissique'` dans une slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A slice of the Doc for 'est une'\n",
    "est_une = ____\n",
    "print(est_une.text)\n",
    "\n",
    "# A slice of the Doc for 'est une phrase narcissique'\n",
    "est_une_phrase_narcissique = ____\n",
    "print(est_une_phrase_narcissique.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A slice of the Doc for 'est une'\n",
    "est_une = doc[1:3]\n",
    "print(est_une.text)\n",
    "\n",
    "# A slice of the Doc for 'est une phrase narcissique'\n",
    "est_une_phrase_narcissique = doc[1:5]\n",
    "print(est_une_phrase_narcissique.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word tokenization\n",
    "from spacy.lang.fr import French\n",
    "\n",
    "# Charger English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = French()\n",
    "\n",
    "text = \"\"\"L'apprentissage de la data science requiert de ne pas se décourager.\n",
    "Les défis et les revers ne sont pas des échecs, ils font simplement partie du voyage!\"\"\"\n",
    "\n",
    "#  \"nlp\" Object est utilisé pour créer des documents avec des annotations linguistiques\n",
    "my_doc = nlp(text)\n",
    "\n",
    "# Créer la liste des tokens de mots\n",
    "token_list = []\n",
    "for token in my_doc:\n",
    "    token_list.append(token.text)\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization de phrases\n",
    "\n",
    "# Charger English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = French()\n",
    "\n",
    "# Créer le composant 'sentencizer' du pipeline\n",
    "sbd = nlp.create_pipe('sentencizer')\n",
    "\n",
    "# Ajouter le composant au pipeline\n",
    "nlp.add_pipe(sbd)\n",
    "\n",
    "text = \"\"\"L'apprentissage de la data science requiert de ne pas se décourager.\n",
    "Les défis et les revers ne sont pas des échecs, ils font simplement partie du voyage!\"\"\"\n",
    "#  L'objet \"nlp\" est utilisé pour créer un document avec des annotations linguistiques\n",
    "doc = nlp(text)\n",
    "\n",
    "# Créaer une liste de tokens de phrases\n",
    "sents_list = []\n",
    "for sent in doc.sents:\n",
    "    sents_list.append(sent.text)\n",
    "print(sents_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cet exemple, vous allez utiliser les objets `Doc` et `Token` de spaCy, et des attributs lexicaux pour identifier des pourcentages dans un texte. Vous devez chercher deux tokens qui se suivent: un nombre et un signe pourcentage.\n",
    "\n",
    "- Utiliser l'attribut de token `like_num` pour voir si le token dans le document ressemble à un nombre.\n",
    "- Lire le token qui suit immédiatement ce token. L'index du token suivant dans le `doc` est `token.i + 1`.\n",
    "- Regarder si l'attribut `text` du token suivant est un signe `”%“`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.fr import French\n",
    "\n",
    "nlp = French()\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(\n",
    "    \"En 1990, plus de 60% de la population d'Asie orientale vivait dans l'extrême pauvreté. \"\n",
    "    \"Cela représente désormais moins de 4%.\"\n",
    ")\n",
    "\n",
    "# Iterate over the tokens in the doc\n",
    "for token in doc:\n",
    "    # Check if the token resembles a number\n",
    "    if ____.____:\n",
    "        # Get the next token in the document\n",
    "        next_token = ____[____]\n",
    "        # Check if the next token's text equals \"%\"\n",
    "        if next_token.____ == \"%\":\n",
    "            print(\"Pourcentage trouvé:\", token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.fr import French\n",
    "\n",
    "nlp = French()\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(\n",
    "    \"En 1990, plus de 60% de la population d'Asie orientale vivait dans l'extrême pauvreté. \"\n",
    "    \"Cela représente désormais moins de 4%.\"\n",
    ")\n",
    "\n",
    "# Iterate over the tokens in the doc\n",
    "for token in doc:\n",
    "    # Check if the token resembles a number\n",
    "    if token.like_num:\n",
    "        # Get the next token in the document\n",
    "        next_token = doc[token.i + 1]\n",
    "        # Check if the next token's text equals \"%\"\n",
    "        if next_token.text == \"%\":\n",
    "            print(\"Pourcentage trouvé:\", token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suppression des stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "\n",
    "# print(list(STOP_WORDS)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implémentation des stop words:\n",
    "filtered_sent=[]\n",
    "\n",
    "#  L'objet \"nlp\" est utilisé pour créer des documents avec annotations\n",
    "doc = nlp(text)\n",
    "\n",
    "# filtering stop words\n",
    "for word in doc:\n",
    "    if word.is_stop or word.is_punct ==False:\n",
    "        filtered_sent.append(word)\n",
    "print(\"Filtered Sentence:\",filtered_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)\n",
    "tokens = [token for token in doc if not token.text in STOP_WORDS and not token.is_punct]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implémenter la lemmatization\n",
    "lem = nlp(\"run runs running runner\")\n",
    "lem = nlp(\"apprendre apprenait apprenons apprentissage\")\n",
    "# finding lemma for each word\n",
    "for word in lem:\n",
    "    print(word.text,word.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etiquetage morpho-synthaxique (POS tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Que sont les modèles statistiques ?**\n",
    "\n",
    "- Ils permettent à spaCy de prédir les attributs linguistiques dans un contexte\n",
    "    - Tags Part-of-speech (POS)\n",
    "    - Dépendances syntaxiques\n",
    "    - Entités nommées\n",
    "- Ils sont entraînés sur des textes labellisés\n",
    "- Ils peuvent être mis à jour avec des exemples additionnels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "import fr_core_news_md\n",
    "\n",
    "nlp = fr_core_news_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a text\n",
    "doc = nlp(\"Elle a mangé la pizza\")\n",
    "\n",
    "# Iterate over the tokens\n",
    "for token in doc:\n",
    "    # Print the text and the predicted part-of-speech tag\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)\n",
    "\n",
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Détection d'entités"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a text\n",
    "doc = nlp(\"Enedis cherche de nouveaux bureaux à Vaison-La-Romaine \"\n",
    "          \"avec l'aide de François Cordel.\")\n",
    "\n",
    "# Iterate over the predicted entities\n",
    "for ent in doc.ents:\n",
    "    # Print the entity text and its label\n",
    "    print(ent.text, ent.label_)\n",
    "    \n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain(\"ORG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain(\"LOC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain(\"PER\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking : noun chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Il est important pour les utilisateurs de véhicules électriques de disposer facilement \"\n",
    "          \"d'infrastructures de recharge pratiques.\")\n",
    "\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(\"Groupe nominal :\", chunk.text, \"; Nom : \", chunk.root.text, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nuage de mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q6UZFmRZlG2a"
   },
   "source": [
    "Nous allons scrapper la page wikipedia d'Enedis et créer un nuage de mots pour comprendre ce qu'il ressort de l'article. \n",
    "\n",
    "- Utilisez le code suivant pour charger la page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wfgPrBkwxqD2"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "r = requests.get(\"https://fr.wikipedia.org/wiki/Enedis\")\n",
    "html_doc = r.text\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "corpus = []\n",
    "for p in soup.find_all(\"p\"):\n",
    "    corpus.append(p.text)\n",
    "\n",
    "join_corpus = \" \".join(corpus)\n",
    "clean_corpus = join_corpus.strip().encode(\"utf-8\").decode(\"utf-8\")\n",
    "clean_corpus[:70]+\"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0tuzXlnM99ba"
   },
   "source": [
    "* Importez maintenant `fr_core_news_sm` --> [Documentation](https://spacy.io/models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ByvnYdaD7vBU"
   },
   "outputs": [],
   "source": [
    "# Import de Spacy et initialisation à Francais\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ByvnYdaD7vBU"
   },
   "outputs": [],
   "source": [
    "# Import de Spacy et initialisation à Francais\n",
    "import fr_core_news_sm\n",
    "nlp = fr_core_news_sm.load()\n",
    "print(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ni8wH3DG-81a"
   },
   "source": [
    "* Incorporez maintenant votre corpus complet dans spacy via `fr_core_news_sm.load()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une variable doc contenant les textes du corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JyH8NsmN8fuQ"
   },
   "outputs": [],
   "source": [
    "# Création d'une variable doc contenant les textes du corpus\n",
    "doc = nlp(clean_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Installez la librairie wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zYxAusV8_Igf"
   },
   "source": [
    "* Avec la librairie `wordcloud`, créez un premier nuage de mots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1726,
     "status": "ok",
     "timestamp": 1582107372818,
     "user": {
      "displayName": "Antoine Krajnc",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mC4XzNDVGvURzl4T5duDbMr6bUdhkYkDul_37G0OA=s64",
      "userId": "08465960390418158788"
     },
     "user_tz": -60
    },
    "id": "23x9RYa-8isd",
    "outputId": "8b5ffd2c-308f-473b-804f-7328eec81f0a"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "wd = WordCloud()\n",
    "cloud = wd.generate(doc.text)\n",
    "plt.imshow(cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XZyM_xpj_NHs"
   },
   "source": [
    "* Importez maintenant les stop-words présents dans `spacy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l4Zl2vxP7w17"
   },
   "outputs": [],
   "source": [
    "# Import des Stop words\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J0DoC9FL_SQ2"
   },
   "source": [
    "* Faites à nouveau un nuage de mots sans les stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1779,
     "status": "ok",
     "timestamp": 1582107378157,
     "user": {
      "displayName": "Antoine Krajnc",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mC4XzNDVGvURzl4T5duDbMr6bUdhkYkDul_37G0OA=s64",
      "userId": "08465960390418158788"
     },
     "user_tz": -60
    },
    "id": "qMZLqI8W8qUe",
    "outputId": "21f0e91f-e892-48f8-890f-6c17421bced6"
   },
   "outputs": [],
   "source": [
    "wd = WordCloud(stopwords=____)\n",
    "cloud = wd.generate(doc.text)\n",
    "plt.imshow(cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = WordCloud(stopwords=STOP_WORDS)\n",
    "cloud = wd.generate(doc.text)\n",
    "plt.imshow(cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z6JG4SSp_WFG"
   },
   "source": [
    "* Enfin nous aurions besoin d'avoir un corpus de mots lemmatisés, recréez un corpus de mots lemmatisés et visualisez votre nouveau nuage de mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VpDEY7gk8rn-"
   },
   "outputs": [],
   "source": [
    "lemma = [token.___ for token in doc if token.___ not in STOP_WORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = [token.lemma_ for token in doc if token.lemma_ not in STOP_WORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4YEXGT9MHODh"
   },
   "outputs": [],
   "source": [
    "corpus_lemma = \" \".join(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2032,
     "status": "ok",
     "timestamp": 1582110215852,
     "user": {
      "displayName": "Antoine Krajnc",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mC4XzNDVGvURzl4T5duDbMr6bUdhkYkDul_37G0OA=s64",
      "userId": "08465960390418158788"
     },
     "user_tz": -60
    },
    "id": "ADnFJsRjG-4f",
    "outputId": "ad82bb24-2462-4fc3-99db-456e0b112384"
   },
   "outputs": [],
   "source": [
    "wd = WordCloud()\n",
    "cloud = wd.generate(corpus_lemma)\n",
    "plt.imshow(cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification de texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm\n",
    "#!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python -m spacy download fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "from spacy.lang.fr import French\n",
    "\n",
    "# Création de la liste de signes de ponctuation\n",
    "punctuations = string.punctuation\n",
    "stop_words = spacy.lang.fr.stop_words.STOP_WORDS\n",
    "\n",
    "# Charger English tokenizer, tagger, parser, NER and word vectors\n",
    "parser = French()\n",
    "\n",
    "# Création de la fonction de tokenization\n",
    "def spacy_tokenizer(sentence):\n",
    "    # Création du token, utilisé pour créer des documents avec des annotations linguistiques\n",
    "    mytokens = parser(sentence)\n",
    "\n",
    "    # Lemmatisation de chaque token et conversion en caractères minuscules\n",
    "    mytokens = [word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "\n",
    "    # Suppression des stop words\n",
    "    mytokens = [word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "\n",
    "    # On retourne la liste des tokens préparés\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer personnalisé grâce à spaCy\n",
    "class predictors(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        # Nettoyage du texte\n",
    "        return [clean_text(text) for text in X]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "\n",
    "# Fonction basique pour nettoyer le texte\n",
    "def clean_text(text):\n",
    "    # Suppression des espaces et conversion des caractères en minuscules\n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('restaurants.csv')\n",
    "\n",
    "X = df['texte'] # les données que l'on veut analyser\n",
    "ylabels = df['label_2'] # les labels, ou réponses, pour lesquels on veut tester notre modèle\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "# Création du pipeline en utilisant des Bag of Words\n",
    "pipe = Pipeline([('cleaner', predictors()),\n",
    "                 ('vectorizer', bow_vector),\n",
    "                 ('classifier', classifier)])\n",
    "\n",
    "# Génération du modèle\n",
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "# Predicting avec un dataset de test\n",
    "predicted = pipe.predict(X_test)\n",
    "\n",
    "# Précision du modèle\n",
    "print(\"Confusion matrix:\\n\",metrics.confusion_matrix(y_test, predicted))\n",
    "print(\"\\nLogistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "# print(\"Logistic Regression Precision and Recall:\",metrics.precision_score(y_test, predicted, average = None))\n",
    "# print(\"Logistic Regression Precision and Recall:\",metrics.recall_score(y_test, predicted, average = None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarque**\n",
    "\n",
    "La qualité des données initiales a un rôle fondamental.\n",
    "\n",
    "*Exemple*\n",
    "- `' '.join(sentences)` --> 75% accuracy\n",
    "- `'. '.join(sentences)` --> 80% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-SNE sur TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['texte'] # les données que l'on veut analyser\n",
    "ylabels = df['label_3'] # les labels, ou réponses, pour lesquels on veut tester notre modèle\n",
    "print(df.loc[:, ['texte','label_3']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On calcule le tf-idf pour chaque mot de chaque texte\n",
    "tfidf = tfidf_vector.fit_transform(X)\n",
    "print(tfidf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des résultats pour le prmeier commentaire\n",
    "\n",
    "# Premier commentaire\n",
    "print(df.loc[0, 'texte'])\n",
    "\n",
    "# TF-IDF pour le premier commentaire\n",
    "df_tfidf = pd.DataFrame(tfidf[0].T.todense(), index = tfidf_vector.get_feature_names(), columns=[\"tf_idf\"])\n",
    "df_tfidf.sort_values(by=[\"tf_idf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un dataframe regroupant les tf-idf et les labels\n",
    "df_tfidf = pd.DataFrame(tfidf.todense(), columns = tfidf_vector.get_feature_names())\n",
    "df_tfidf = pd.concat([df_tfidf, ylabels], axis=1)\n",
    "print(df_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE sur les données\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "tsne_results = tsne.fit_transform(df_tfidf.drop(['label_3'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On considère les deux dimensions les plus importantes\n",
    "df_tfidf['Dim 1'] = tsne_results[:,0]\n",
    "df_tfidf['Dim 2'] = tsne_results[:,1]\n",
    "\n",
    "# On projette les données sur les deux axes, en colorant les points avec les labels\n",
    "plt.figure(figsize=(16,10))\n",
    "sns.scatterplot(\n",
    "    x = \"Dim 1\", y = \"Dim 2\",\n",
    "    hue = \"label_3\",\n",
    "    palette = sns.color_palette(\"hls\", 3),\n",
    "    data = df_tfidf,\n",
    "    legend = \"full\",\n",
    "    alpha = 1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m spacy download fr\n",
    "# python -m spacy download fr_core_news_md\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse factorielle (ACP, projection de texte)\n",
    "\n",
    "# Tokenization, Stemming, Lemmatization, POS-tagging, Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "# Stop words\n",
    "# Lemmatisation : regroupement des mots d une même famille dans un texte, afin de réduire ces mots à leur forme canonique (le lemme), comme petit, petite, petits, et petites.\n",
    "# Racinisation (stemming) : regroupement des mots ayant une racine commune et appartenant au même champ lexical.\n",
    "# Reconnaissance d entités nommées : détermination dans un texte des noms propres, tels que des personnes ou des endroits, ainsi que les quantités, valeurs, ou dates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ressources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quentin \n",
    "https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/\n",
    "\n",
    "Gabriel\n",
    "https://medium.com/@MSalnikov/text-clustering-with-k-means-and-tf-idf-f099bcf95183\n",
    "\n",
    "Complément\n",
    "https://www.actuia.com/contribution/victorbigand/tutoriel-tal-pour-les-debutants-classification-de-texte/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moins intéressantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuto cloudwords\n",
    "\n",
    "https://www.datacamp.com/community/tutorials/wordcloud-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/\n",
    "\n",
    "1. Basic feature extraction using text data\n",
    "    - Number of words\n",
    "    - Number of characters\n",
    "    - Average word length\n",
    "    - Number of stopwords\n",
    "    - Number of special characters\n",
    "    - Number of numerics\n",
    "    - Number of uppercase words\n",
    "2. Basic Text Pre-processing of text data\n",
    "    - Lower casing\n",
    "    - Punctuation removal\n",
    "    - Stopwords removal\n",
    "    - Frequent words removal\n",
    "    - Rare words removal\n",
    "    - Spelling correction\n",
    "    - Tokenization\n",
    "    - Stemming\n",
    "    - Lemmatization\n",
    "3. Advance Text Processing\n",
    "    - N-grams\n",
    "    - Term Frequency\n",
    "    - Inverse Document Frequency\n",
    "    - Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "    - Bag of Words\n",
    "    - Sentiment Analysis\n",
    "    - Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.actuia.com/contribution/victorbigand/tutoriel-tal-pour-les-debutants-classification-de-texte/\n",
    "\n",
    "Classification de texte (spams sur commentaires youtube)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/data-scientists-guide-to-summarization-fc0db952e363\n",
    "\n",
    "**NLTK Summarizer**\n",
    "\n",
    "We wanted to start our text summarization journey by trying something simple. So we turned to the popular NLP package in python — NLTK. The idea here was to summarize by identifying “top” sentences based on word frequency.\n",
    "\n",
    "1. tokenize words\n",
    "    - Pre-process the text by removing numbers, white spaces and ponctuation\n",
    "    - Remove stopwords\n",
    "    - Tokenize all words in the document\n",
    "2. Word frequency\n",
    "    - Calculate the frequency for every token in the document\n",
    "3. Sentence selection\n",
    "    - Sentence score is calculated for every sentence\n",
    "    - It is the sum of the word frequency of every word in the sentence\n",
    "    - Top 'n' sentences are selected based on highest sentence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('fr_core_news_md') # en_core_web_sm\n",
    "\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "STOP_WORDS.add(\"blabla\")\n",
    "stopwords = STOP_WORDS\n",
    "print(list(stopwords)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"Ceci est un exemple de texte destiné à tester les différentes librairies Python et à préparer une démonstration. Ce texte a été préparé par Gabriel pour LinkyStat.\"\n",
    "# text = 'European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices'\n",
    "# text = \"The engine coughs and shakes its head. The smoke, a plume of red and white, Waves madly in the face of night. And now the grave incurious stars Gleam on the groaning hurrying cars. Against the kind and awful reign Of darkness, this our angry train, A noisy little rebel, pouts Its brief defiance, flames and shouts— And passes on, and leaves no trace. For darkness holds its ancient place, Serene and absolute, the king Unchanged, of every living thing. The houses lie obscure and still In Rutherford and Carlton Hill. Our lamps intensify the dark Of slumbering Passaic Park. And quiet holds the weary feet That daily tramp through Prospect Street. What though we clang and clank and roar Through all Passaic's streets? No door Will open, not an eye will see Who this loud vagabond may be. Upon my crimson cushioned seat, In manufactured light and heat, I feel unnatural and mean. Outside the towns are cool and clean; Curtained awhile from sound and sight They take God's gracious gift of night. The stars are watchful over them. On Clifton as on Bethlehem The angels, leaning down the sky, Shed peace and gentle dreams. And I— I ride, I blasphemously ride Through all the silent countryside. The engine's shriek, the headlight s glare, Pollute the still nocturnal air. The cottages of Lake View sigh And sleeping, frown as we pass by. Why, even strident Paterson Rests quietly as any nun. Her foolish warring children keep The grateful armistice of sleep. For what tremendous errand's sake Are we so blatantly awake? What precious secret is our freight? What king must be abroad so late? Perhaps Death roams the hills to-night And we rush forth to give him fight. Or else, perhaps, we speed his way To some remote unthinking prey. Perhaps a woman writhes in pain And listens—listens for the train! The train, that like an angel sings, The train, with healing on its wings. Now \\\"Hawthorne!\\\" the conductor cries. My neighbor starts and rubs his eyes. He hurries yawning through the car And steps out where the houses are. This is the reason of our quest! Not wantonly we break the rest Of town and village, nor do we Lightly profane night's sanctity. What Love commands the train fulfills, And beautiful upon the hills Are these our feet of burnished steel. Subtly and certainly I feel That Glen Rock welcomes us to her And silent Ridgewood seems to stir And smile, because she knows the train Has brought her children back again. We carry people home—and so God speeds us, wheresoe'er we go. Hohokus, Waldwick, Allendale Lift sleepy heads to give us hail. In Ramsey, Mahwah, Suffern stand Houses that wistfully demand A father—son—some human thing That this, the midnight train, may bring. The trains that travel in the day They hurry folks to work or play. The midnight train is slow and old But of it let this thing be told, To its high honor be it said It carries people home to bed. My cottage lamp shines white and clear. God bless the train that brought me here.\"\n",
    "text = \"Ce jour-là, 25 mars dernier, Pétersbourg fut le théâtre d’une aventure des plus étranges. Le barbier Ivan Yakovlévitch, domicilié avenue de l’Ascension (son nom de famille est perdu et son enseigne ne porte que l’inscription : On pratique aussi les saignées, au-dessous d’un monsieur à la joue barbouillée de savon), le barbier Ivan Yakovlévitch se réveilla d’assez bonne heure et perçut une odeur de pain chaud. S’étant mis sur son séant, il vit que son épouse – personne plutôt respectable et qui prisait fort le café – défournait des pains tout frais cuits. « Aujourd’hui, Prascovie Ossipovna, je ne prendrai pas de café, déclara Ivan Yakovlévitch ; je préfère grignoter un bon pain chaud avec de la ciboule. » À la vérité, Ivan Yakovlévitch aurait bien voulu et pain et café, mais il jugeait impossible de demander les deux choses à la fois, Prascovie Ossipovna ne tolérant pas de semblables caprices. « Tant mieux, se dit la respectable épouse en jetant un pain sur la table. Que mon nigaud s’empiffre de pain ! Il me restera davantage de café. » Respectueux des convenances, Ivan Yakovlévitch passa son habit par-dessus sa chemise et se mit en devoir de déjeuner. Il posa devant lui une pincée de sel, nettoya deux oignons, prit son couteau et, la mine grave, coupa son pain en deux. Il aperçut alors, à sa grande surprise, un objet blanchâtre au beau milieu ; il le tâta précautionneusement du couteau, le palpa du doigt… « Qu’est-ce que cela peut bien être ? » se dit-il en éprouvant de la résistance. Il fourra alors ses doigts dans le pain et en retira… un nez ! Les bras lui en tombèrent.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "doc = nlp(text)\n",
    "tokens = [token for token in doc if not token.text in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "for token in tokens:\n",
    "    print('Original : %s, New: %s' % (token.text, token.lemma_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tags\n",
    "\n",
    "# NP: noun phrase\n",
    "# DT: determiner\n",
    "# JJ: adjective\n",
    "# JJS: adjective, superlative\n",
    "# NN: noun\n",
    "# NNP: proper noun, singular\n",
    "# NNS: noun, plural\n",
    "# IN: preposition or subordinating conjunction\n",
    "# VBD: verb, past tense\n",
    "# VBZ: verb, 3rd person singular present\n",
    "\n",
    "for token in tokens:\n",
    "    print('Word: %s, POS: %s' % (token.text, token.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization and POS tags\n",
    "[(token.orth_,token.pos_, token.lemma_) for token in\n",
    " [y for y in doc if not y.is_stop and y.pos_ != 'PUNCT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "for token in tokens:\n",
    "    print('Original : %s, Root form: %s' % (token.text, stemmer.stem(token.text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name-Entity Recognition (NER)\n",
    "\n",
    "for entity in doc.ents:\n",
    "    print(entity.text + ' - ' + entity.label_ + ' - ' + str(spacy.explain(entity.label_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting nouns\n",
    "\n",
    "for noun in doc.noun_chunks:\n",
    "    print(noun.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin: first token of a multi-token entity\n",
    "# In: inner token of a multi-token entity\n",
    "# Last: last token of a multi-token entity\n",
    "# Unit: single-token entity\n",
    "# Out: non-entity token\n",
    "\n",
    "print([(token, token.ent_iob_, token.ent_type_) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(nlp(str(text)), jupyter=True, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "displacy.render(nlp(str(text)), style='dep', jupyter = True, options = {'distance': 120})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus de spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarité"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `spaCy` can compare two objects and predict similarity\n",
    "- `Doc.similarity()`, `Span.similarity()` and `Token.similarity()`\n",
    "- Take another object and return a similarity score (`0` to `1`)\n",
    "- Important: needs a model that has word vectors included, for example:\n",
    "    - ✅ `en_core_web_md` (medium model)\n",
    "    - ✅ `en_core_web_lg` (large model)\n",
    "    - 🚫 NOT `en_core_web_sm` (small model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a larger model with vectors\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Compare two documents\n",
    "doc1 = nlp(\"I like fast food\")\n",
    "doc2 = nlp(\"I like pizza\")\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compare two tokens\n",
    "doc = nlp(\"I like pizza and pasta\")\n",
    "token1 = doc[2]\n",
    "token2 = doc[4]\n",
    "print(token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lists of dictionaries, one per token\n",
    "- Match exact token texts `[{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]`\n",
    "- Match lexical attributes `[{\"LOWER\": \"iphone\"}, {\"LOWER\": \"x\"}]`\n",
    "- Match any token attributes `[{\"LEMMA\": \"buy\"}, {\"POS\": \"NOUN\"}]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Import the Matcher\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Load a model and create the nlp object\n",
    "nlp = spacy.load(\"fr_core_news_md\")\n",
    "\n",
    "# Initialize the matcher with the shared vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "pattern = [{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]\n",
    "matcher.add(\"IPHONE_PATTERN\", None, pattern)\n",
    "\n",
    "# Process some text\n",
    "doc = nlp(\"Upcoming iPhone X release date leaked\")\n",
    "\n",
    "# Call the matcher on the doc\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [\n",
    "    {\"IS_DIGIT\": True},\n",
    "    {\"LOWER\": \"fifa\"},\n",
    "    {\"LOWER\": \"world\"},\n",
    "    {\"LOWER\": \"cup\"},\n",
    "    {\"IS_PUNCT\": True}\n",
    "]\n",
    "doc = nlp(\"2018 FIFA World Cup: France won!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pour aller plus loin avec spacy**\n",
    "\n",
    "N'hésitez pas à consulter ce tuto qui est très bien fait -> https://course.spacy.io/en/chapter1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brouillon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gabriel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"Ceci est un exemple de texte destiné à tester les différentes librairies Python et à préparer une démonstration. Ce texte a été préparé par Gabriel pour LinkyStat.\"\n",
    "# text = 'European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices'\n",
    "# text = \"The engine coughs and shakes its head. The smoke, a plume of red and white, Waves madly in the face of night. And now the grave incurious stars Gleam on the groaning hurrying cars. Against the kind and awful reign Of darkness, this our angry train, A noisy little rebel, pouts Its brief defiance, flames and shouts— And passes on, and leaves no trace. For darkness holds its ancient place, Serene and absolute, the king Unchanged, of every living thing. The houses lie obscure and still In Rutherford and Carlton Hill. Our lamps intensify the dark Of slumbering Passaic Park. And quiet holds the weary feet That daily tramp through Prospect Street. What though we clang and clank and roar Through all Passaic's streets? No door Will open, not an eye will see Who this loud vagabond may be. Upon my crimson cushioned seat, In manufactured light and heat, I feel unnatural and mean. Outside the towns are cool and clean; Curtained awhile from sound and sight They take God's gracious gift of night. The stars are watchful over them. On Clifton as on Bethlehem The angels, leaning down the sky, Shed peace and gentle dreams. And I— I ride, I blasphemously ride Through all the silent countryside. The engine's shriek, the headlight s glare, Pollute the still nocturnal air. The cottages of Lake View sigh And sleeping, frown as we pass by. Why, even strident Paterson Rests quietly as any nun. Her foolish warring children keep The grateful armistice of sleep. For what tremendous errand's sake Are we so blatantly awake? What precious secret is our freight? What king must be abroad so late? Perhaps Death roams the hills to-night And we rush forth to give him fight. Or else, perhaps, we speed his way To some remote unthinking prey. Perhaps a woman writhes in pain And listens—listens for the train! The train, that like an angel sings, The train, with healing on its wings. Now \\\"Hawthorne!\\\" the conductor cries. My neighbor starts and rubs his eyes. He hurries yawning through the car And steps out where the houses are. This is the reason of our quest! Not wantonly we break the rest Of town and village, nor do we Lightly profane night's sanctity. What Love commands the train fulfills, And beautiful upon the hills Are these our feet of burnished steel. Subtly and certainly I feel That Glen Rock welcomes us to her And silent Ridgewood seems to stir And smile, because she knows the train Has brought her children back again. We carry people home—and so God speeds us, wheresoe'er we go. Hohokus, Waldwick, Allendale Lift sleepy heads to give us hail. In Ramsey, Mahwah, Suffern stand Houses that wistfully demand A father—son—some human thing That this, the midnight train, may bring. The trains that travel in the day They hurry folks to work or play. The midnight train is slow and old But of it let this thing be told, To its high honor be it said It carries people home to bed. My cottage lamp shines white and clear. God bless the train that brought me here.\"\n",
    "\n",
    "text = \"Ce jour-là, 25 mars dernier, Pétersbourg fut le théâtre d’une aventure\"\n",
    "\"des plus étranges. Le barbier Ivan Yakovlévitch, domicilié avenue de\"\n",
    "\"l’Ascension (son nom de famille est perdu et son enseigne ne porte que\"\n",
    "\"l’inscription : On pratique aussi les saignées, au-dessous d’un monsieur\"\n",
    "\"à la joue barbouillée de savon), le barbier Ivan Yakovlévitch se réveilla\"\n",
    "\"d’assez bonne heure et perçut une odeur de pain chaud. S’étant mis sur\"\n",
    "\"son séant, il vit que son épouse – personne plutôt respectable et qui\"\n",
    "\"prisait fort le café – défournait des pains tout frais cuits.\"\n",
    "\" « Aujourd’hui, Prascovie Ossipovna, je ne prendrai pas de café, déclara\"\n",
    "\"Ivan Yakovlévitch ; je préfère grignoter un bon pain chaud avec de la\"\n",
    "\"ciboule. » À la vérité, Ivan Yakovlévitch aurait bien voulu et pain et\"\n",
    "\"café, mais il jugeait impossible de demander les deux choses à la fois,\"\n",
    "\"\"\" Prascovie Ossipovna ne tolérant pas de semblables caprices. « Tant\"\n",
    "\"mieux, se dit la respectable épouse en jetant un pain sur la table. Que\"\n",
    "\"mon nigaud s’empiffre de pain ! Il me restera davantage de café. »\"\n",
    "\" Respectueux des convenances, Ivan Yakovlévitch passa son habit\"\n",
    "\"par-dessus sa chemise et se mit en devoir de déjeuner. Il posa devant\"\n",
    "\"lui une pincée de sel, nettoya deux oignons, prit son couteau et, la\"\n",
    "\"mine grave, coupa son pain en deux. Il aperçut alors, à sa grande\"\n",
    "\"surprise, un objet blanchâtre au beau milieu ; il le tâta\"\n",
    "\"précautionneusement du couteau, le palpa du doigt… « Qu’est-ce que cela\"\n",
    "\"peut bien être ? » se dit-il en éprouvant de la résistance. Il fourra\"\n",
    "\"alors ses doigts dans le pain et en retira… un nez ! Les bras lui en\"\n",
    "\"tombèrent.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('fr_core_news_md') # en_core_web_sm\n",
    "\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "STOP_WORDS.add(\"blabla\")\n",
    "stopwords = STOP_WORDS\n",
    "print(list(stopwords)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"Ceci est un exemple de texte destiné à tester les différentes librairies Python et à préparer une démonstration. Ce texte a été préparé par Gabriel pour LinkyStat.\"\n",
    "# text = 'European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices'\n",
    "# text = \"The engine coughs and shakes its head. The smoke, a plume of red and white, Waves madly in the face of night. And now the grave incurious stars Gleam on the groaning hurrying cars. Against the kind and awful reign Of darkness, this our angry train, A noisy little rebel, pouts Its brief defiance, flames and shouts— And passes on, and leaves no trace. For darkness holds its ancient place, Serene and absolute, the king Unchanged, of every living thing. The houses lie obscure and still In Rutherford and Carlton Hill. Our lamps intensify the dark Of slumbering Passaic Park. And quiet holds the weary feet That daily tramp through Prospect Street. What though we clang and clank and roar Through all Passaic's streets? No door Will open, not an eye will see Who this loud vagabond may be. Upon my crimson cushioned seat, In manufactured light and heat, I feel unnatural and mean. Outside the towns are cool and clean; Curtained awhile from sound and sight They take God's gracious gift of night. The stars are watchful over them. On Clifton as on Bethlehem The angels, leaning down the sky, Shed peace and gentle dreams. And I— I ride, I blasphemously ride Through all the silent countryside. The engine's shriek, the headlight s glare, Pollute the still nocturnal air. The cottages of Lake View sigh And sleeping, frown as we pass by. Why, even strident Paterson Rests quietly as any nun. Her foolish warring children keep The grateful armistice of sleep. For what tremendous errand's sake Are we so blatantly awake? What precious secret is our freight? What king must be abroad so late? Perhaps Death roams the hills to-night And we rush forth to give him fight. Or else, perhaps, we speed his way To some remote unthinking prey. Perhaps a woman writhes in pain And listens—listens for the train! The train, that like an angel sings, The train, with healing on its wings. Now \\\"Hawthorne!\\\" the conductor cries. My neighbor starts and rubs his eyes. He hurries yawning through the car And steps out where the houses are. This is the reason of our quest! Not wantonly we break the rest Of town and village, nor do we Lightly profane night's sanctity. What Love commands the train fulfills, And beautiful upon the hills Are these our feet of burnished steel. Subtly and certainly I feel That Glen Rock welcomes us to her And silent Ridgewood seems to stir And smile, because she knows the train Has brought her children back again. We carry people home—and so God speeds us, wheresoe'er we go. Hohokus, Waldwick, Allendale Lift sleepy heads to give us hail. In Ramsey, Mahwah, Suffern stand Houses that wistfully demand A father—son—some human thing That this, the midnight train, may bring. The trains that travel in the day They hurry folks to work or play. The midnight train is slow and old But of it let this thing be told, To its high honor be it said It carries people home to bed. My cottage lamp shines white and clear. God bless the train that brought me here.\"\n",
    "text = \"Ce jour-là, 25 mars dernier, Pétersbourg fut le théâtre d’une aventure des plus étranges. Le barbier Ivan Yakovlévitch, domicilié avenue de l’Ascension (son nom de famille est perdu et son enseigne ne porte que l’inscription : On pratique aussi les saignées, au-dessous d’un monsieur à la joue barbouillée de savon), le barbier Ivan Yakovlévitch se réveilla d’assez bonne heure et perçut une odeur de pain chaud. S’étant mis sur son séant, il vit que son épouse – personne plutôt respectable et qui prisait fort le café – défournait des pains tout frais cuits. « Aujourd’hui, Prascovie Ossipovna, je ne prendrai pas de café, déclara Ivan Yakovlévitch ; je préfère grignoter un bon pain chaud avec de la ciboule. » À la vérité, Ivan Yakovlévitch aurait bien voulu et pain et café, mais il jugeait impossible de demander les deux choses à la fois, Prascovie Ossipovna ne tolérant pas de semblables caprices. « Tant mieux, se dit la respectable épouse en jetant un pain sur la table. Que mon nigaud s’empiffre de pain ! Il me restera davantage de café. » Respectueux des convenances, Ivan Yakovlévitch passa son habit par-dessus sa chemise et se mit en devoir de déjeuner. Il posa devant lui une pincée de sel, nettoya deux oignons, prit son couteau et, la mine grave, coupa son pain en deux. Il aperçut alors, à sa grande surprise, un objet blanchâtre au beau milieu ; il le tâta précautionneusement du couteau, le palpa du doigt… « Qu’est-ce que cela peut bien être ? » se dit-il en éprouvant de la résistance. Il fourra alors ses doigts dans le pain et en retira… un nez ! Les bras lui en tombèrent.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "doc = nlp(text)\n",
    "tokens = [token for token in doc if not token.text in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "for token in tokens:\n",
    "    print('Original : %s, New: %s' % (token.text, token.lemma_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tags\n",
    "\n",
    "# NP: noun phrase\n",
    "# DT: determiner\n",
    "# JJ: adjective\n",
    "# JJS: adjective, superlative\n",
    "# NN: noun\n",
    "# NNP: proper noun, singular\n",
    "# NNS: noun, plural\n",
    "# IN: preposition or subordinating conjunction\n",
    "# VBD: verb, past tense\n",
    "# VBZ: verb, 3rd person singular present\n",
    "\n",
    "for token in tokens:\n",
    "    print('Word: %s, POS: %s' % (token.text, token.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization and POS tags\n",
    "[(token.orth_,token.pos_, token.lemma_) for token in\n",
    " [y for y in doc if not y.is_stop and y.pos_ != 'PUNCT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "for token in tokens:\n",
    "    print('Original : %s, Root form: %s' % (token.text, stemmer.stem(token.text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name-Entity Recognition (NER)\n",
    "\n",
    "for entity in doc.ents:\n",
    "    print(entity.text + ' - ' + entity.label_ + ' - ' + str(spacy.explain(entity.label_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting nouns\n",
    "\n",
    "for noun in doc.noun_chunks:\n",
    "    print(noun.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin: first token of a multi-token entity\n",
    "# In: inner token of a multi-token entity\n",
    "# Last: last token of a multi-token entity\n",
    "# Unit: single-token entity\n",
    "# Out: non-entity token\n",
    "\n",
    "print([(token, token.ent_iob_, token.ent_type_) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(nlp(str(text)), jupyter=True, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "displacy.render(nlp(str(text)), style='dep', jupyter = True, options = {'distance': 120})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classif en anglais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ls datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon = pd.read_csv (\"datasets/amazon_alexa.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon.feedback.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Create our list of punctuation marks\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Create our list of stopwords\n",
    "nlp = spacy.load('en')\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "parser = English()\n",
    "\n",
    "# Creating our tokenizer function\n",
    "def spacy_tokenizer(sentence):\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    mytokens = parser(sentence)\n",
    "\n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "\n",
    "    # Removing stop words\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer using spaCy\n",
    "class predictors(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        # Cleaning Text\n",
    "        return [clean_text(text) for text in X]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "\n",
    "# Basic function to clean the text\n",
    "def clean_text(text):\n",
    "    # Removing spaces and converting text into lowercase\n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_amazon['verified_reviews'] # the features we want to analyze\n",
    "ylabels = df_amazon['feedback'] # the labels, or answers, we want to test against\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "# Create pipeline using Bag of Words\n",
    "pipe = Pipeline([(\"cleaner\", predictors()),\n",
    "                 ('vectorizer', bow_vector),\n",
    "                 ('classifier', classifier)])\n",
    "\n",
    "# model generation\n",
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "# Predicting with a test dataset\n",
    "predicted = pipe.predict(X_test)\n",
    "\n",
    "# Model Accuracy\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
