{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette premi√®re session de NLP va vous donner les bases de la manipulation de texte en vue du data challenge qui vous attend apr√®s l'√©t√© !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download fr_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing avec spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement de la librairie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Importer la classe `French` de `spacy.lang.fr`\n",
    "- Cr√©er l'objet `nlp` avec le constructeur de la classe `French`\n",
    "- Cr√©er un `doc` et afficher son texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importez la classe de langage French\n",
    "from spacy.lang.____ import ____\n",
    "\n",
    "# Cr√©ez l'objet nlp\n",
    "nlp = ____\n",
    "\n",
    "# Traitement du texte\n",
    "doc = nlp(\"Ceci est une phrase narcissique puisqu'elle \"\n",
    "          \"ne parle que d'elle-m√™me.\")\n",
    "\n",
    "# Affichez le texte du document\n",
    "print(____.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importez la classe de langage French\n",
    "from spacy.lang.fr import French\n",
    "\n",
    "# Cr√©ez l'objet nlp\n",
    "nlp = French()\n",
    "\n",
    "# Traitement du texte\n",
    "doc = nlp(\"Ceci est une phrase narcissique puisqu'elle \"\n",
    "          \"ne parle que d'elle-m√™me.\")\n",
    "\n",
    "# Affichez le texte du document\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**L'objet NLP**\n",
    "\n",
    "- contient le 'pipeline' des calculs\n",
    "- inclut des r√®gles sp√©cifiques au langage pour la tokenization, etc.\n",
    "\n",
    "**L'objet doc**\n",
    "\n",
    "Contient un document, c'est-√†-dire un ensemble de *tokens*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulation d'un objet `Doc`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objet `Doc`se manipule comme une liste. \n",
    "\n",
    "A l'aide des `[]`:\n",
    "- Accedez au premier token de doc\n",
    "- Selectionnez les mots `'est une'` dans une slice\n",
    "- Selectionnez les mots entre `'est une phrase narcissique'` dans une slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A slice of the Doc for 'est une'\n",
    "est_une = ____\n",
    "print(est_une.text)\n",
    "\n",
    "# A slice of the Doc for 'est une phrase narcissique'\n",
    "est_une_phrase_narcissique = ____\n",
    "print(est_une_phrase_narcissique.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A slice of the Doc for 'est une'\n",
    "est_une = doc[1:3]\n",
    "print(est_une.text)\n",
    "\n",
    "# A slice of the Doc for 'est une phrase narcissique'\n",
    "est_une_phrase_narcissique = doc[1:5]\n",
    "print(est_une_phrase_narcissique.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word tokenization\n",
    "from spacy.lang.fr import French\n",
    "\n",
    "# Charger English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = French()\n",
    "\n",
    "text = \"\"\"L'apprentissage de la data science requiert de ne pas se d√©courager.\n",
    "Les d√©fis et les revers ne sont pas des √©checs, ils font simplement partie du voyage!\"\"\"\n",
    "\n",
    "#  \"nlp\" Object est utilis√© pour cr√©er des documents avec des annotations linguistiques\n",
    "my_doc = nlp(text)\n",
    "\n",
    "# Cr√©er la liste des tokens de mots\n",
    "token_list = []\n",
    "for token in my_doc:\n",
    "    token_list.append(token.text)\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization de phrases\n",
    "\n",
    "# Charger English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = French()\n",
    "\n",
    "# Cr√©er le composant 'sentencizer' du pipeline\n",
    "sbd = nlp.create_pipe('sentencizer')\n",
    "\n",
    "# Ajouter le composant au pipeline\n",
    "nlp.add_pipe(sbd)\n",
    "\n",
    "text = \"\"\"L'apprentissage de la data science requiert de ne pas se d√©courager.\n",
    "Les d√©fis et les revers ne sont pas des √©checs, ils font simplement partie du voyage!\"\"\"\n",
    "#  L'objet \"nlp\" est utilis√© pour cr√©er un document avec des annotations linguistiques\n",
    "doc = nlp(text)\n",
    "\n",
    "# Cr√©aer une liste de tokens de phrases\n",
    "sents_list = []\n",
    "for sent in doc.sents:\n",
    "    sents_list.append(sent.text)\n",
    "print(sents_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cet exemple, vous allez utiliser les objets `Doc` et `Token` de spaCy, et des attributs lexicaux pour identifier des pourcentages dans un texte. Vous devez chercher deux tokens qui se suivent: un nombre et un signe pourcentage.\n",
    "\n",
    "- Utiliser l'attribut de token `like_num` pour voir si le token dans le document ressemble √† un nombre.\n",
    "- Lire le token qui suit imm√©diatement ce token. L'index du token suivant dans le `doc` est `token.i + 1`.\n",
    "- Regarder si l'attribut `text` du token suivant est un signe `‚Äù%‚Äú`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.fr import French\n",
    "\n",
    "nlp = French()\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(\n",
    "    \"En 1990, plus de 60% de la population d'Asie orientale vivait dans l'extr√™me pauvret√©. \"\n",
    "    \"Cela repr√©sente d√©sormais moins de 4%.\"\n",
    ")\n",
    "\n",
    "# Iterate over the tokens in the doc\n",
    "for token in doc:\n",
    "    # Check if the token resembles a number\n",
    "    if ____.____:\n",
    "        # Get the next token in the document\n",
    "        next_token = ____[____]\n",
    "        # Check if the next token's text equals \"%\"\n",
    "        if next_token.____ == \"%\":\n",
    "            print(\"Pourcentage trouv√©:\", token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.fr import French\n",
    "\n",
    "nlp = French()\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(\n",
    "    \"En 1990, plus de 60% de la population d'Asie orientale vivait dans l'extr√™me pauvret√©. \"\n",
    "    \"Cela repr√©sente d√©sormais moins de 4%.\"\n",
    ")\n",
    "\n",
    "# Iterate over the tokens in the doc\n",
    "for token in doc:\n",
    "    # Check if the token resembles a number\n",
    "    if token.like_num:\n",
    "        # Get the next token in the document\n",
    "        next_token = doc[token.i + 1]\n",
    "        # Check if the next token's text equals \"%\"\n",
    "        if next_token.text == \"%\":\n",
    "            print(\"Pourcentage trouv√©:\", token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suppression des stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "\n",
    "# print(list(STOP_WORDS)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impl√©mentation des stop words:\n",
    "filtered_sent=[]\n",
    "\n",
    "#  L'objet \"nlp\" est utilis√© pour cr√©er des documents avec annotations\n",
    "doc = nlp(text)\n",
    "\n",
    "# filtering stop words\n",
    "for word in doc:\n",
    "    if word.is_stop or word.is_punct ==False:\n",
    "        filtered_sent.append(word)\n",
    "print(\"Filtered Sentence:\",filtered_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)\n",
    "tokens = [token for token in doc if not token.text in STOP_WORDS and not token.is_punct]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impl√©menter la lemmatization\n",
    "lem = nlp(\"run runs running runner\")\n",
    "lem = nlp(\"apprendre apprenait apprenons apprentissage\")\n",
    "# finding lemma for each word\n",
    "for word in lem:\n",
    "    print(word.text,word.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etiquetage morpho-synthaxique (POS tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Que sont les mod√®les statistiques ?**\n",
    "\n",
    "- Ils permettent √† spaCy de pr√©dir les attributs linguistiques dans un contexte\n",
    "    - Tags Part-of-speech (POS)\n",
    "    - D√©pendances syntaxiques\n",
    "    - Entit√©s nomm√©es\n",
    "- Ils sont entra√Æn√©s sur des textes labellis√©s\n",
    "- Ils peuvent √™tre mis √† jour avec des exemples additionnels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "import fr_core_news_md\n",
    "\n",
    "nlp = fr_core_news_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a text\n",
    "doc = nlp(\"Elle a mang√© la pizza\")\n",
    "\n",
    "# Iterate over the tokens\n",
    "for token in doc:\n",
    "    # Print the text and the predicted part-of-speech tag\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)\n",
    "\n",
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D√©tection d'entit√©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a text\n",
    "doc = nlp(\"Enedis cherche de nouveaux bureaux √† Vaison-La-Romaine \"\n",
    "          \"avec l'aide de Fran√ßois Cordel.\")\n",
    "\n",
    "# Iterate over the predicted entities\n",
    "for ent in doc.ents:\n",
    "    # Print the entity text and its label\n",
    "    print(ent.text, ent.label_)\n",
    "    \n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain(\"ORG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain(\"LOC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain(\"PER\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking : noun chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Il est important pour les utilisateurs de v√©hicules √©lectriques de disposer facilement \"\n",
    "          \"d'infrastructures de recharge pratiques.\")\n",
    "\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(\"Groupe nominal :\", chunk.text, \"; Nom : \", chunk.root.text, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nuage de mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q6UZFmRZlG2a"
   },
   "source": [
    "Nous allons scrapper la page wikipedia d'Enedis et cr√©er un nuage de mots pour comprendre ce qu'il ressort de l'article. \n",
    "\n",
    "- Utilisez le code suivant pour charger la page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wfgPrBkwxqD2"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "r = requests.get(\"https://fr.wikipedia.org/wiki/Enedis\")\n",
    "html_doc = r.text\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "corpus = []\n",
    "for p in soup.find_all(\"p\"):\n",
    "    corpus.append(p.text)\n",
    "\n",
    "join_corpus = \" \".join(corpus)\n",
    "clean_corpus = join_corpus.strip().encode(\"utf-8\").decode(\"utf-8\")\n",
    "clean_corpus[:70]+\"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0tuzXlnM99ba"
   },
   "source": [
    "* Importez maintenant `fr_core_news_sm` --> [Documentation](https://spacy.io/models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ByvnYdaD7vBU"
   },
   "outputs": [],
   "source": [
    "# Import de Spacy et initialisation √† Francais\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ByvnYdaD7vBU"
   },
   "outputs": [],
   "source": [
    "# Import de Spacy et initialisation √† Francais\n",
    "import fr_core_news_sm\n",
    "nlp = fr_core_news_sm.load()\n",
    "print(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ni8wH3DG-81a"
   },
   "source": [
    "* Incorporez maintenant votre corpus complet dans spacy via `fr_core_news_sm.load()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation d'une variable doc contenant les textes du corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JyH8NsmN8fuQ"
   },
   "outputs": [],
   "source": [
    "# Cr√©ation d'une variable doc contenant les textes du corpus\n",
    "doc = nlp(clean_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Installez la librairie wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zYxAusV8_Igf"
   },
   "source": [
    "* Avec la librairie `wordcloud`, cr√©ez un premier nuage de mots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1726,
     "status": "ok",
     "timestamp": 1582107372818,
     "user": {
      "displayName": "Antoine Krajnc",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mC4XzNDVGvURzl4T5duDbMr6bUdhkYkDul_37G0OA=s64",
      "userId": "08465960390418158788"
     },
     "user_tz": -60
    },
    "id": "23x9RYa-8isd",
    "outputId": "8b5ffd2c-308f-473b-804f-7328eec81f0a"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "wd = WordCloud()\n",
    "cloud = wd.generate(doc.text)\n",
    "plt.imshow(cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XZyM_xpj_NHs"
   },
   "source": [
    "* Importez maintenant les stop-words pr√©sents dans `spacy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l4Zl2vxP7w17"
   },
   "outputs": [],
   "source": [
    "# Import des Stop words\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J0DoC9FL_SQ2"
   },
   "source": [
    "* Faites √† nouveau un nuage de mots sans les stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1779,
     "status": "ok",
     "timestamp": 1582107378157,
     "user": {
      "displayName": "Antoine Krajnc",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mC4XzNDVGvURzl4T5duDbMr6bUdhkYkDul_37G0OA=s64",
      "userId": "08465960390418158788"
     },
     "user_tz": -60
    },
    "id": "qMZLqI8W8qUe",
    "outputId": "21f0e91f-e892-48f8-890f-6c17421bced6"
   },
   "outputs": [],
   "source": [
    "wd = WordCloud(stopwords=____)\n",
    "cloud = wd.generate(doc.text)\n",
    "plt.imshow(cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = WordCloud(stopwords=STOP_WORDS)\n",
    "cloud = wd.generate(doc.text)\n",
    "plt.imshow(cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z6JG4SSp_WFG"
   },
   "source": [
    "* Enfin nous aurions besoin d'avoir un corpus de mots lemmatis√©s, recr√©ez un corpus de mots lemmatis√©s et visualisez votre nouveau nuage de mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VpDEY7gk8rn-"
   },
   "outputs": [],
   "source": [
    "lemma = [token.___ for token in doc if token.___ not in STOP_WORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = [token.lemma_ for token in doc if token.lemma_ not in STOP_WORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4YEXGT9MHODh"
   },
   "outputs": [],
   "source": [
    "corpus_lemma = \" \".join(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2032,
     "status": "ok",
     "timestamp": 1582110215852,
     "user": {
      "displayName": "Antoine Krajnc",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mC4XzNDVGvURzl4T5duDbMr6bUdhkYkDul_37G0OA=s64",
      "userId": "08465960390418158788"
     },
     "user_tz": -60
    },
    "id": "ADnFJsRjG-4f",
    "outputId": "ad82bb24-2462-4fc3-99db-456e0b112384"
   },
   "outputs": [],
   "source": [
    "wd = WordCloud()\n",
    "cloud = wd.generate(corpus_lemma)\n",
    "plt.imshow(cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification de texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm\n",
    "#!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python -m spacy download fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "from spacy.lang.fr import French\n",
    "\n",
    "# Cr√©ation de la liste de signes de ponctuation\n",
    "punctuations = string.punctuation\n",
    "stop_words = spacy.lang.fr.stop_words.STOP_WORDS\n",
    "\n",
    "# Charger English tokenizer, tagger, parser, NER and word vectors\n",
    "parser = French()\n",
    "\n",
    "# Cr√©ation de la fonction de tokenization\n",
    "def spacy_tokenizer(sentence):\n",
    "    # Cr√©ation du token, utilis√© pour cr√©er des documents avec des annotations linguistiques\n",
    "    mytokens = parser(sentence)\n",
    "\n",
    "    # Lemmatisation de chaque token et conversion en caract√®res minuscules\n",
    "    mytokens = [word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "\n",
    "    # Suppression des stop words\n",
    "    mytokens = [word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "\n",
    "    # On retourne la liste des tokens pr√©par√©s\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer personnalis√© gr√¢ce √† spaCy\n",
    "class predictors(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        # Nettoyage du texte\n",
    "        return [clean_text(text) for text in X]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "\n",
    "# Fonction basique pour nettoyer le texte\n",
    "def clean_text(text):\n",
    "    # Suppression des espaces et conversion des caract√®res en minuscules\n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('restaurants.csv')\n",
    "\n",
    "X = df['texte'] # les donn√©es que l'on veut analyser\n",
    "ylabels = df['label_2'] # les labels, ou r√©ponses, pour lesquels on veut tester notre mod√®le\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "# Cr√©ation du pipeline en utilisant des Bag of Words\n",
    "pipe = Pipeline([('cleaner', predictors()),\n",
    "                 ('vectorizer', bow_vector),\n",
    "                 ('classifier', classifier)])\n",
    "\n",
    "# G√©n√©ration du mod√®le\n",
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "# Predicting avec un dataset de test\n",
    "predicted = pipe.predict(X_test)\n",
    "\n",
    "# Pr√©cision du mod√®le\n",
    "print(\"Confusion matrix:\\n\",metrics.confusion_matrix(y_test, predicted))\n",
    "print(\"\\nLogistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "# print(\"Logistic Regression Precision and Recall:\",metrics.precision_score(y_test, predicted, average = None))\n",
    "# print(\"Logistic Regression Precision and Recall:\",metrics.recall_score(y_test, predicted, average = None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarque**\n",
    "\n",
    "La qualit√© des donn√©es initiales a un r√¥le fondamental.\n",
    "\n",
    "*Exemple*\n",
    "- `' '.join(sentences)` --> 75% accuracy\n",
    "- `'. '.join(sentences)` --> 80% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-SNE sur TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['texte'] # les donn√©es que l'on veut analyser\n",
    "ylabels = df['label_3'] # les labels, ou r√©ponses, pour lesquels on veut tester notre mod√®le\n",
    "print(df.loc[:, ['texte','label_3']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On calcule le tf-idf pour chaque mot de chaque texte\n",
    "tfidf = tfidf_vector.fit_transform(X)\n",
    "print(tfidf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des r√©sultats pour le prmeier commentaire\n",
    "\n",
    "# Premier commentaire\n",
    "print(df.loc[0, 'texte'])\n",
    "\n",
    "# TF-IDF pour le premier commentaire\n",
    "df_tfidf = pd.DataFrame(tfidf[0].T.todense(), index = tfidf_vector.get_feature_names(), columns=[\"tf_idf\"])\n",
    "df_tfidf.sort_values(by=[\"tf_idf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation d'un dataframe regroupant les tf-idf et les labels\n",
    "df_tfidf = pd.DataFrame(tfidf.todense(), columns = tfidf_vector.get_feature_names())\n",
    "df_tfidf = pd.concat([df_tfidf, ylabels], axis=1)\n",
    "print(df_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE sur les donn√©es\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "tsne_results = tsne.fit_transform(df_tfidf.drop(['label_3'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On consid√®re les deux dimensions les plus importantes\n",
    "df_tfidf['Dim 1'] = tsne_results[:,0]\n",
    "df_tfidf['Dim 2'] = tsne_results[:,1]\n",
    "\n",
    "# On projette les donn√©es sur les deux axes, en colorant les points avec les labels\n",
    "plt.figure(figsize=(16,10))\n",
    "sns.scatterplot(\n",
    "    x = \"Dim 1\", y = \"Dim 2\",\n",
    "    hue = \"label_3\",\n",
    "    palette = sns.color_palette(\"hls\", 3),\n",
    "    data = df_tfidf,\n",
    "    legend = \"full\",\n",
    "    alpha = 1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m spacy download fr\n",
    "# python -m spacy download fr_core_news_md\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse factorielle (ACP, projection de texte)\n",
    "\n",
    "# Tokenization, Stemming, Lemmatization, POS-tagging, Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "# Stop words\n",
    "# Lemmatisation : regroupement des mots d une m√™me famille dans un texte, afin de r√©duire ces mots √† leur forme canonique (le lemme), comme petit, petite, petits, et petites.\n",
    "# Racinisation (stemming) : regroupement des mots ayant une racine commune et appartenant au m√™me champ lexical.\n",
    "# Reconnaissance d entit√©s nomm√©es : d√©termination dans un texte des noms propres, tels que des personnes ou des endroits, ainsi que les quantit√©s, valeurs, ou dates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ressources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quentin \n",
    "https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/\n",
    "\n",
    "Gabriel\n",
    "https://medium.com/@MSalnikov/text-clustering-with-k-means-and-tf-idf-f099bcf95183\n",
    "\n",
    "Compl√©ment\n",
    "https://www.actuia.com/contribution/victorbigand/tutoriel-tal-pour-les-debutants-classification-de-texte/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moins int√©ressantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuto cloudwords\n",
    "\n",
    "https://www.datacamp.com/community/tutorials/wordcloud-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/\n",
    "\n",
    "1. Basic feature extraction using text data\n",
    "    - Number of words\n",
    "    - Number of characters\n",
    "    - Average word length\n",
    "    - Number of stopwords\n",
    "    - Number of special characters\n",
    "    - Number of numerics\n",
    "    - Number of uppercase words\n",
    "2. Basic Text Pre-processing of text data\n",
    "    - Lower casing\n",
    "    - Punctuation removal\n",
    "    - Stopwords removal\n",
    "    - Frequent words removal\n",
    "    - Rare words removal\n",
    "    - Spelling correction\n",
    "    - Tokenization\n",
    "    - Stemming\n",
    "    - Lemmatization\n",
    "3. Advance Text Processing\n",
    "    - N-grams\n",
    "    - Term Frequency\n",
    "    - Inverse Document Frequency\n",
    "    - Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "    - Bag of Words\n",
    "    - Sentiment Analysis\n",
    "    - Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.actuia.com/contribution/victorbigand/tutoriel-tal-pour-les-debutants-classification-de-texte/\n",
    "\n",
    "Classification de texte (spams sur commentaires youtube)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/data-scientists-guide-to-summarization-fc0db952e363\n",
    "\n",
    "**NLTK Summarizer**\n",
    "\n",
    "We wanted to start our text summarization journey by trying something simple. So we turned to the popular NLP package in python ‚Äî NLTK. The idea here was to summarize by identifying ‚Äútop‚Äù sentences based on word frequency.\n",
    "\n",
    "1. tokenize words\n",
    "    - Pre-process the text by removing numbers, white spaces and ponctuation\n",
    "    - Remove stopwords\n",
    "    - Tokenize all words in the document\n",
    "2. Word frequency\n",
    "    - Calculate the frequency for every token in the document\n",
    "3. Sentence selection\n",
    "    - Sentence score is calculated for every sentence\n",
    "    - It is the sum of the word frequency of every word in the sentence\n",
    "    - Top 'n' sentences are selected based on highest sentence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('fr_core_news_md') # en_core_web_sm\n",
    "\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "STOP_WORDS.add(\"blabla\")\n",
    "stopwords = STOP_WORDS\n",
    "print(list(stopwords)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"Ceci est un exemple de texte destin√© √† tester les diff√©rentes librairies Python et √† pr√©parer une d√©monstration. Ce texte a √©t√© pr√©par√© par Gabriel pour LinkyStat.\"\n",
    "# text = 'European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices'\n",
    "# text = \"The engine coughs and shakes its head. The smoke, a plume of red and white, Waves madly in the face of night. And now the grave incurious stars Gleam on the groaning hurrying cars. Against the kind and awful reign Of darkness, this our angry train, A noisy little rebel, pouts Its brief defiance, flames and shouts‚Äî And passes on, and leaves no trace. For darkness holds its ancient place, Serene and absolute, the king Unchanged, of every living thing. The houses lie obscure and still In Rutherford and Carlton Hill. Our lamps intensify the dark Of slumbering Passaic Park. And quiet holds the weary feet That daily tramp through Prospect Street. What though we clang and clank and roar Through all Passaic's streets? No door Will open, not an eye will see Who this loud vagabond may be. Upon my crimson cushioned seat, In manufactured light and heat, I feel unnatural and mean. Outside the towns are cool and clean; Curtained awhile from sound and sight They take God's gracious gift of night. The stars are watchful over them. On Clifton as on Bethlehem The angels, leaning down the sky, Shed peace and gentle dreams. And I‚Äî I ride, I blasphemously ride Through all the silent countryside. The engine's shriek, the headlight s glare, Pollute the still nocturnal air. The cottages of Lake View sigh And sleeping, frown as we pass by. Why, even strident Paterson Rests quietly as any nun. Her foolish warring children keep The grateful armistice of sleep. For what tremendous errand's sake Are we so blatantly awake? What precious secret is our freight? What king must be abroad so late? Perhaps Death roams the hills to-night And we rush forth to give him fight. Or else, perhaps, we speed his way To some remote unthinking prey. Perhaps a woman writhes in pain And listens‚Äîlistens for the train! The train, that like an angel sings, The train, with healing on its wings. Now \\\"Hawthorne!\\\" the conductor cries. My neighbor starts and rubs his eyes. He hurries yawning through the car And steps out where the houses are. This is the reason of our quest! Not wantonly we break the rest Of town and village, nor do we Lightly profane night's sanctity. What Love commands the train fulfills, And beautiful upon the hills Are these our feet of burnished steel. Subtly and certainly I feel That Glen Rock welcomes us to her And silent Ridgewood seems to stir And smile, because she knows the train Has brought her children back again. We carry people home‚Äîand so God speeds us, wheresoe'er we go. Hohokus, Waldwick, Allendale Lift sleepy heads to give us hail. In Ramsey, Mahwah, Suffern stand Houses that wistfully demand A father‚Äîson‚Äîsome human thing That this, the midnight train, may bring. The trains that travel in the day They hurry folks to work or play. The midnight train is slow and old But of it let this thing be told, To its high honor be it said It carries people home to bed. My cottage lamp shines white and clear. God bless the train that brought me here.\"\n",
    "text = \"Ce jour-l√†, 25 mars dernier, P√©tersbourg fut le th√©√¢tre d‚Äôune aventure des plus √©tranges. Le barbier Ivan Yakovl√©vitch, domicili√© avenue de l‚ÄôAscension (son nom de famille est perdu et son enseigne ne porte que l‚Äôinscription : On pratique aussi les saign√©es, au-dessous d‚Äôun monsieur √† la joue barbouill√©e de savon), le barbier Ivan Yakovl√©vitch se r√©veilla d‚Äôassez bonne heure et per√ßut une odeur de pain chaud. S‚Äô√©tant mis sur son s√©ant, il vit que son √©pouse ‚Äì personne plut√¥t respectable et qui prisait fort le caf√© ‚Äì d√©fournait des pains tout frais cuits. ¬´ Aujourd‚Äôhui, Prascovie Ossipovna, je ne prendrai pas de caf√©, d√©clara Ivan Yakovl√©vitch ; je pr√©f√®re grignoter un bon pain chaud avec de la ciboule. ¬ª √Ä la v√©rit√©, Ivan Yakovl√©vitch aurait bien voulu et pain et caf√©, mais il jugeait impossible de demander les deux choses √† la fois, Prascovie Ossipovna ne tol√©rant pas de semblables caprices. ¬´ Tant mieux, se dit la respectable √©pouse en jetant un pain sur la table. Que mon nigaud s‚Äôempiffre de pain ! Il me restera davantage de caf√©. ¬ª Respectueux des convenances, Ivan Yakovl√©vitch passa son habit par-dessus sa chemise et se mit en devoir de d√©jeuner. Il posa devant lui une pinc√©e de sel, nettoya deux oignons, prit son couteau et, la mine grave, coupa son pain en deux. Il aper√ßut alors, √† sa grande surprise, un objet blanch√¢tre au beau milieu ; il le t√¢ta pr√©cautionneusement du couteau, le palpa du doigt‚Ä¶ ¬´ Qu‚Äôest-ce que cela peut bien √™tre ? ¬ª se dit-il en √©prouvant de la r√©sistance. Il fourra alors ses doigts dans le pain et en retira‚Ä¶ un nez ! Les bras lui en tomb√®rent.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "doc = nlp(text)\n",
    "tokens = [token for token in doc if not token.text in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "for token in tokens:\n",
    "    print('Original : %s, New: %s' % (token.text, token.lemma_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tags\n",
    "\n",
    "# NP: noun phrase\n",
    "# DT: determiner\n",
    "# JJ: adjective\n",
    "# JJS: adjective, superlative\n",
    "# NN: noun\n",
    "# NNP: proper noun, singular\n",
    "# NNS: noun, plural\n",
    "# IN: preposition or subordinating conjunction\n",
    "# VBD: verb, past tense\n",
    "# VBZ: verb, 3rd person singular present\n",
    "\n",
    "for token in tokens:\n",
    "    print('Word: %s, POS: %s' % (token.text, token.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization and POS tags\n",
    "[(token.orth_,token.pos_, token.lemma_) for token in\n",
    " [y for y in doc if not y.is_stop and y.pos_ != 'PUNCT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "for token in tokens:\n",
    "    print('Original : %s, Root form: %s' % (token.text, stemmer.stem(token.text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name-Entity Recognition (NER)\n",
    "\n",
    "for entity in doc.ents:\n",
    "    print(entity.text + ' - ' + entity.label_ + ' - ' + str(spacy.explain(entity.label_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting nouns\n",
    "\n",
    "for noun in doc.noun_chunks:\n",
    "    print(noun.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin: first token of a multi-token entity\n",
    "# In: inner token of a multi-token entity\n",
    "# Last: last token of a multi-token entity\n",
    "# Unit: single-token entity\n",
    "# Out: non-entity token\n",
    "\n",
    "print([(token, token.ent_iob_, token.ent_type_) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(nlp(str(text)), jupyter=True, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "displacy.render(nlp(str(text)), style='dep', jupyter = True, options = {'distance': 120})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus de spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarit√©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `spaCy` can compare two objects and predict similarity\n",
    "- `Doc.similarity()`, `Span.similarity()` and `Token.similarity()`\n",
    "- Take another object and return a similarity score (`0` to `1`)\n",
    "- Important: needs a model that has word vectors included, for example:\n",
    "    - ‚úÖ `en_core_web_md` (medium model)\n",
    "    - ‚úÖ `en_core_web_lg` (large model)\n",
    "    - üö´ NOT `en_core_web_sm` (small model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a larger model with vectors\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Compare two documents\n",
    "doc1 = nlp(\"I like fast food\")\n",
    "doc2 = nlp(\"I like pizza\")\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compare two tokens\n",
    "doc = nlp(\"I like pizza and pasta\")\n",
    "token1 = doc[2]\n",
    "token2 = doc[4]\n",
    "print(token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lists of dictionaries, one per token\n",
    "- Match exact token texts `[{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]`\n",
    "- Match lexical attributes `[{\"LOWER\": \"iphone\"}, {\"LOWER\": \"x\"}]`\n",
    "- Match any token attributes `[{\"LEMMA\": \"buy\"}, {\"POS\": \"NOUN\"}]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Import the Matcher\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Load a model and create the nlp object\n",
    "nlp = spacy.load(\"fr_core_news_md\")\n",
    "\n",
    "# Initialize the matcher with the shared vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "pattern = [{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]\n",
    "matcher.add(\"IPHONE_PATTERN\", None, pattern)\n",
    "\n",
    "# Process some text\n",
    "doc = nlp(\"Upcoming iPhone X release date leaked\")\n",
    "\n",
    "# Call the matcher on the doc\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [\n",
    "    {\"IS_DIGIT\": True},\n",
    "    {\"LOWER\": \"fifa\"},\n",
    "    {\"LOWER\": \"world\"},\n",
    "    {\"LOWER\": \"cup\"},\n",
    "    {\"IS_PUNCT\": True}\n",
    "]\n",
    "doc = nlp(\"2018 FIFA World Cup: France won!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pour aller plus loin avec spacy**\n",
    "\n",
    "N'h√©sitez pas √† consulter ce tuto qui est tr√®s bien fait -> https://course.spacy.io/en/chapter1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brouillon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gabriel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"Ceci est un exemple de texte destin√© √† tester les diff√©rentes librairies Python et √† pr√©parer une d√©monstration. Ce texte a √©t√© pr√©par√© par Gabriel pour LinkyStat.\"\n",
    "# text = 'European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices'\n",
    "# text = \"The engine coughs and shakes its head. The smoke, a plume of red and white, Waves madly in the face of night. And now the grave incurious stars Gleam on the groaning hurrying cars. Against the kind and awful reign Of darkness, this our angry train, A noisy little rebel, pouts Its brief defiance, flames and shouts‚Äî And passes on, and leaves no trace. For darkness holds its ancient place, Serene and absolute, the king Unchanged, of every living thing. The houses lie obscure and still In Rutherford and Carlton Hill. Our lamps intensify the dark Of slumbering Passaic Park. And quiet holds the weary feet That daily tramp through Prospect Street. What though we clang and clank and roar Through all Passaic's streets? No door Will open, not an eye will see Who this loud vagabond may be. Upon my crimson cushioned seat, In manufactured light and heat, I feel unnatural and mean. Outside the towns are cool and clean; Curtained awhile from sound and sight They take God's gracious gift of night. The stars are watchful over them. On Clifton as on Bethlehem The angels, leaning down the sky, Shed peace and gentle dreams. And I‚Äî I ride, I blasphemously ride Through all the silent countryside. The engine's shriek, the headlight s glare, Pollute the still nocturnal air. The cottages of Lake View sigh And sleeping, frown as we pass by. Why, even strident Paterson Rests quietly as any nun. Her foolish warring children keep The grateful armistice of sleep. For what tremendous errand's sake Are we so blatantly awake? What precious secret is our freight? What king must be abroad so late? Perhaps Death roams the hills to-night And we rush forth to give him fight. Or else, perhaps, we speed his way To some remote unthinking prey. Perhaps a woman writhes in pain And listens‚Äîlistens for the train! The train, that like an angel sings, The train, with healing on its wings. Now \\\"Hawthorne!\\\" the conductor cries. My neighbor starts and rubs his eyes. He hurries yawning through the car And steps out where the houses are. This is the reason of our quest! Not wantonly we break the rest Of town and village, nor do we Lightly profane night's sanctity. What Love commands the train fulfills, And beautiful upon the hills Are these our feet of burnished steel. Subtly and certainly I feel That Glen Rock welcomes us to her And silent Ridgewood seems to stir And smile, because she knows the train Has brought her children back again. We carry people home‚Äîand so God speeds us, wheresoe'er we go. Hohokus, Waldwick, Allendale Lift sleepy heads to give us hail. In Ramsey, Mahwah, Suffern stand Houses that wistfully demand A father‚Äîson‚Äîsome human thing That this, the midnight train, may bring. The trains that travel in the day They hurry folks to work or play. The midnight train is slow and old But of it let this thing be told, To its high honor be it said It carries people home to bed. My cottage lamp shines white and clear. God bless the train that brought me here.\"\n",
    "\n",
    "text = \"Ce jour-l√†, 25 mars dernier, P√©tersbourg fut le th√©√¢tre d‚Äôune aventure\"\n",
    "\"des plus √©tranges. Le barbier Ivan Yakovl√©vitch, domicili√© avenue de\"\n",
    "\"l‚ÄôAscension (son nom de famille est perdu et son enseigne ne porte que\"\n",
    "\"l‚Äôinscription : On pratique aussi les saign√©es, au-dessous d‚Äôun monsieur\"\n",
    "\"√† la joue barbouill√©e de savon), le barbier Ivan Yakovl√©vitch se r√©veilla\"\n",
    "\"d‚Äôassez bonne heure et per√ßut une odeur de pain chaud. S‚Äô√©tant mis sur\"\n",
    "\"son s√©ant, il vit que son √©pouse ‚Äì personne plut√¥t respectable et qui\"\n",
    "\"prisait fort le caf√© ‚Äì d√©fournait des pains tout frais cuits.\"\n",
    "\" ¬´ Aujourd‚Äôhui, Prascovie Ossipovna, je ne prendrai pas de caf√©, d√©clara\"\n",
    "\"Ivan Yakovl√©vitch ; je pr√©f√®re grignoter un bon pain chaud avec de la\"\n",
    "\"ciboule. ¬ª √Ä la v√©rit√©, Ivan Yakovl√©vitch aurait bien voulu et pain et\"\n",
    "\"caf√©, mais il jugeait impossible de demander les deux choses √† la fois,\"\n",
    "\"\"\" Prascovie Ossipovna ne tol√©rant pas de semblables caprices. ¬´ Tant\"\n",
    "\"mieux, se dit la respectable √©pouse en jetant un pain sur la table. Que\"\n",
    "\"mon nigaud s‚Äôempiffre de pain ! Il me restera davantage de caf√©. ¬ª\"\n",
    "\" Respectueux des convenances, Ivan Yakovl√©vitch passa son habit\"\n",
    "\"par-dessus sa chemise et se mit en devoir de d√©jeuner. Il posa devant\"\n",
    "\"lui une pinc√©e de sel, nettoya deux oignons, prit son couteau et, la\"\n",
    "\"mine grave, coupa son pain en deux. Il aper√ßut alors, √† sa grande\"\n",
    "\"surprise, un objet blanch√¢tre au beau milieu ; il le t√¢ta\"\n",
    "\"pr√©cautionneusement du couteau, le palpa du doigt‚Ä¶ ¬´ Qu‚Äôest-ce que cela\"\n",
    "\"peut bien √™tre ? ¬ª se dit-il en √©prouvant de la r√©sistance. Il fourra\"\n",
    "\"alors ses doigts dans le pain et en retira‚Ä¶ un nez ! Les bras lui en\"\n",
    "\"tomb√®rent.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('fr_core_news_md') # en_core_web_sm\n",
    "\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "STOP_WORDS.add(\"blabla\")\n",
    "stopwords = STOP_WORDS\n",
    "print(list(stopwords)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"Ceci est un exemple de texte destin√© √† tester les diff√©rentes librairies Python et √† pr√©parer une d√©monstration. Ce texte a √©t√© pr√©par√© par Gabriel pour LinkyStat.\"\n",
    "# text = 'European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices'\n",
    "# text = \"The engine coughs and shakes its head. The smoke, a plume of red and white, Waves madly in the face of night. And now the grave incurious stars Gleam on the groaning hurrying cars. Against the kind and awful reign Of darkness, this our angry train, A noisy little rebel, pouts Its brief defiance, flames and shouts‚Äî And passes on, and leaves no trace. For darkness holds its ancient place, Serene and absolute, the king Unchanged, of every living thing. The houses lie obscure and still In Rutherford and Carlton Hill. Our lamps intensify the dark Of slumbering Passaic Park. And quiet holds the weary feet That daily tramp through Prospect Street. What though we clang and clank and roar Through all Passaic's streets? No door Will open, not an eye will see Who this loud vagabond may be. Upon my crimson cushioned seat, In manufactured light and heat, I feel unnatural and mean. Outside the towns are cool and clean; Curtained awhile from sound and sight They take God's gracious gift of night. The stars are watchful over them. On Clifton as on Bethlehem The angels, leaning down the sky, Shed peace and gentle dreams. And I‚Äî I ride, I blasphemously ride Through all the silent countryside. The engine's shriek, the headlight s glare, Pollute the still nocturnal air. The cottages of Lake View sigh And sleeping, frown as we pass by. Why, even strident Paterson Rests quietly as any nun. Her foolish warring children keep The grateful armistice of sleep. For what tremendous errand's sake Are we so blatantly awake? What precious secret is our freight? What king must be abroad so late? Perhaps Death roams the hills to-night And we rush forth to give him fight. Or else, perhaps, we speed his way To some remote unthinking prey. Perhaps a woman writhes in pain And listens‚Äîlistens for the train! The train, that like an angel sings, The train, with healing on its wings. Now \\\"Hawthorne!\\\" the conductor cries. My neighbor starts and rubs his eyes. He hurries yawning through the car And steps out where the houses are. This is the reason of our quest! Not wantonly we break the rest Of town and village, nor do we Lightly profane night's sanctity. What Love commands the train fulfills, And beautiful upon the hills Are these our feet of burnished steel. Subtly and certainly I feel That Glen Rock welcomes us to her And silent Ridgewood seems to stir And smile, because she knows the train Has brought her children back again. We carry people home‚Äîand so God speeds us, wheresoe'er we go. Hohokus, Waldwick, Allendale Lift sleepy heads to give us hail. In Ramsey, Mahwah, Suffern stand Houses that wistfully demand A father‚Äîson‚Äîsome human thing That this, the midnight train, may bring. The trains that travel in the day They hurry folks to work or play. The midnight train is slow and old But of it let this thing be told, To its high honor be it said It carries people home to bed. My cottage lamp shines white and clear. God bless the train that brought me here.\"\n",
    "text = \"Ce jour-l√†, 25 mars dernier, P√©tersbourg fut le th√©√¢tre d‚Äôune aventure des plus √©tranges. Le barbier Ivan Yakovl√©vitch, domicili√© avenue de l‚ÄôAscension (son nom de famille est perdu et son enseigne ne porte que l‚Äôinscription : On pratique aussi les saign√©es, au-dessous d‚Äôun monsieur √† la joue barbouill√©e de savon), le barbier Ivan Yakovl√©vitch se r√©veilla d‚Äôassez bonne heure et per√ßut une odeur de pain chaud. S‚Äô√©tant mis sur son s√©ant, il vit que son √©pouse ‚Äì personne plut√¥t respectable et qui prisait fort le caf√© ‚Äì d√©fournait des pains tout frais cuits. ¬´ Aujourd‚Äôhui, Prascovie Ossipovna, je ne prendrai pas de caf√©, d√©clara Ivan Yakovl√©vitch ; je pr√©f√®re grignoter un bon pain chaud avec de la ciboule. ¬ª √Ä la v√©rit√©, Ivan Yakovl√©vitch aurait bien voulu et pain et caf√©, mais il jugeait impossible de demander les deux choses √† la fois, Prascovie Ossipovna ne tol√©rant pas de semblables caprices. ¬´ Tant mieux, se dit la respectable √©pouse en jetant un pain sur la table. Que mon nigaud s‚Äôempiffre de pain ! Il me restera davantage de caf√©. ¬ª Respectueux des convenances, Ivan Yakovl√©vitch passa son habit par-dessus sa chemise et se mit en devoir de d√©jeuner. Il posa devant lui une pinc√©e de sel, nettoya deux oignons, prit son couteau et, la mine grave, coupa son pain en deux. Il aper√ßut alors, √† sa grande surprise, un objet blanch√¢tre au beau milieu ; il le t√¢ta pr√©cautionneusement du couteau, le palpa du doigt‚Ä¶ ¬´ Qu‚Äôest-ce que cela peut bien √™tre ? ¬ª se dit-il en √©prouvant de la r√©sistance. Il fourra alors ses doigts dans le pain et en retira‚Ä¶ un nez ! Les bras lui en tomb√®rent.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "doc = nlp(text)\n",
    "tokens = [token for token in doc if not token.text in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "for token in tokens:\n",
    "    print('Original : %s, New: %s' % (token.text, token.lemma_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tags\n",
    "\n",
    "# NP: noun phrase\n",
    "# DT: determiner\n",
    "# JJ: adjective\n",
    "# JJS: adjective, superlative\n",
    "# NN: noun\n",
    "# NNP: proper noun, singular\n",
    "# NNS: noun, plural\n",
    "# IN: preposition or subordinating conjunction\n",
    "# VBD: verb, past tense\n",
    "# VBZ: verb, 3rd person singular present\n",
    "\n",
    "for token in tokens:\n",
    "    print('Word: %s, POS: %s' % (token.text, token.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization and POS tags\n",
    "[(token.orth_,token.pos_, token.lemma_) for token in\n",
    " [y for y in doc if not y.is_stop and y.pos_ != 'PUNCT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "for token in tokens:\n",
    "    print('Original : %s, Root form: %s' % (token.text, stemmer.stem(token.text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name-Entity Recognition (NER)\n",
    "\n",
    "for entity in doc.ents:\n",
    "    print(entity.text + ' - ' + entity.label_ + ' - ' + str(spacy.explain(entity.label_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting nouns\n",
    "\n",
    "for noun in doc.noun_chunks:\n",
    "    print(noun.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin: first token of a multi-token entity\n",
    "# In: inner token of a multi-token entity\n",
    "# Last: last token of a multi-token entity\n",
    "# Unit: single-token entity\n",
    "# Out: non-entity token\n",
    "\n",
    "print([(token, token.ent_iob_, token.ent_type_) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(nlp(str(text)), jupyter=True, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "displacy.render(nlp(str(text)), style='dep', jupyter = True, options = {'distance': 120})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classif en anglais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ls datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon = pd.read_csv (\"datasets/amazon_alexa.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon.feedback.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Create our list of punctuation marks\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Create our list of stopwords\n",
    "nlp = spacy.load('en')\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "parser = English()\n",
    "\n",
    "# Creating our tokenizer function\n",
    "def spacy_tokenizer(sentence):\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    mytokens = parser(sentence)\n",
    "\n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "\n",
    "    # Removing stop words\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer using spaCy\n",
    "class predictors(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        # Cleaning Text\n",
    "        return [clean_text(text) for text in X]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "\n",
    "# Basic function to clean the text\n",
    "def clean_text(text):\n",
    "    # Removing spaces and converting text into lowercase\n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_amazon['verified_reviews'] # the features we want to analyze\n",
    "ylabels = df_amazon['feedback'] # the labels, or answers, we want to test against\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "# Create pipeline using Bag of Words\n",
    "pipe = Pipeline([(\"cleaner\", predictors()),\n",
    "                 ('vectorizer', bow_vector),\n",
    "                 ('classifier', classifier)])\n",
    "\n",
    "# model generation\n",
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "# Predicting with a test dataset\n",
    "predicted = pipe.predict(X_test)\n",
    "\n",
    "# Model Accuracy\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
