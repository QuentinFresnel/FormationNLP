{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install spacy\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "3oRtjzFQYbMv"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from spacy.lang.fr import French\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "vWzzZG9JYiwB"
   },
   "outputs": [],
   "source": [
    "texteNLP = \"\"\"Le traitement automatique du langage naturel (abr. TALN), ou traitement automatique de la langue naturelle, \n",
    " ou encore traitement automatique des langues (abr. TAL) est un domaine multidisciplinaire impliquant la linguistique,\n",
    " l'informatique et l'intelligence artificielle, qui vise à créer des outils de traitement de la langue naturelle pour diverses applications.\n",
    " Il ne doit pas être confondu avec la linguistique informatique, qui vise à comprendre les langues au moyen d'outils informatiques.\n",
    " Le TALN est sorti des laboratoires de recherche pour être progressivement mis en œuvre dans des applications informatiques nécessitant\n",
    " l'intégration du langage humain à la machine. Aussi le TALN est-il parfois appelé ingénierie linguistique.\n",
    " En France, le traitement automatique de la langue naturelle a sa revue, Traitement automatique des langues,\n",
    " publiée par l’Association pour le traitement automatique des langues (ATALA).\"\"\"\n",
    "\n",
    "texteDataScience = \"\"\"En termes généraux, la science des données est l'extraction de connaissance d'ensembles de données.\n",
    " Elle emploie des techniques et des théories tirées de plusieurs autres domaines plus larges des mathématiques, analyse,\n",
    " optimisation et statistique principalement, la théorie de l'information et la technologie de l'information, notamment le traitement de signal,\n",
    " des modèles probabilistes, l'apprentissage automatique, l'apprentissage statistique, la programmation informatique, l'ingénierie de données,\n",
    " la reconnaissance de formes et l'apprentissage, la visualisation, l'analytique prophétique, la modélisation d'incertitude, le stockage de données,\n",
    " la géo-visualisation, la compression de données et le calcul à haute performance.\n",
    " Les méthodes qui s'adaptent aux données de masse sont particulièrement intéressantes dans la science des données,\n",
    " bien que la discipline ne soit généralement pas considérée comme limitée à ces données.\n",
    "\n",
    "La science des données (en anglais data science) est une discipline qui s'appuie sur des outils mathématiques, de statistiques,\n",
    " d'informatique (cette science est principalement une « science des données numériques ») et de visualisation des données.\n",
    " Elle est en plein développement, dans le monde universitaire ainsi que dans le secteur privé et le secteur public.\n",
    " Moore en 1991 a défini la statistique comme la science des données6 (définition reprise par d'autres dont James T. McClave et al. en 1997)\n",
    " et U. Beck en 2001 oppose la science des données à la science de l'expérience, voyant une dissociation croissante entre ces deux types de science,\n",
    " que tendrait selon lui à encourager une société de la gestion du risque au sein d'une « civilisation du danger ».\"\"\"\n",
    "\n",
    "texteEconometrie = \"\"\"L'économétrie est une branche de la science économique qui a pour objectif d'estimer et de tester les modèles économiques.\n",
    " L'économétrie en tant que discipline naît dans les années 1930 avec la création de la société d'économétrie par Irving Fisher et Ragnar Frisch (1930)\n",
    " et la création de la revue Econometrica (1933). Depuis lors, l'économétrie n'a cessé de se développer et de prendre une importance croissante\n",
    " au sein de la science économique.\n",
    "\n",
    "L'économétrie théorique se focalise essentiellement sur deux questions, l'identification et l'estimation statistique. L'économétrie appliquée\n",
    " utilise les méthodes économétriques pour comprendre des domaines de l'économie comme l'analyse du marché du travail,\n",
    " l'économie de l'éducation ou encore tester la pertinence empirique des modèles de croissance.\n",
    "\n",
    "L'économétrie appliquée utilise aussi bien des données issues d'un protocole expérimental, que ce soit une expérience de laboratoire\n",
    " ou une expérience de terrain, que des données issues directement de l'observation du réel sans manipulation du chercheur.\n",
    " Lorsque l'économètre utilise des données issues directement de l'observation du réel, il est fréquent d'identifier des expériences naturelles\n",
    " pour retrouver une situation quasi-expérimentale. On parle parfois de révolution de crédibilité, terme controversé, pour désigner l'essor fulgurant\n",
    " de ces méthodes de recherche dans la discipline, et en économie en général.\"\"\"\n",
    "\n",
    "texteHistoire = \"\"\"L’histoire, souvent écrit avec la première lettre majuscule,\n",
    " est à la fois l’étude et l'écriture des faits et des événements passés quelles que soient leur variété et leur complexité.\n",
    " L'histoire est également une science humaine et sociale. On désigne aussi couramment sous le terme d’histoire (par synecdoque) le passé lui-même,\n",
    " comme dans les leçons de l'histoire. L'histoire est un récit écrit par lequel des hommes et des femmes (les historiens et historiennes)\n",
    " s'efforcent de faire connaître les temps révolus. Ces tentatives ne sont jamais entièrement indépendantes de conditionnements étrangers au domaine\n",
    " telle que la vision du monde de leur auteur ou de sa culture, mais elles sont censées être élaborées à partir de sources plutôt que guidées\n",
    " par la spéculation ou l'idéologie.\n",
    "\n",
    "Au cours des siècles, les historiens ont façonné leurs méthodes ainsi que les champs d'intervention, tout en réévaluant leurs sources,\n",
    " leur origine et leur exploitation. La discipline universitaire d'étude et écriture de l'histoire, y comprise la critique des méthodes,\n",
    " est l'historiographie. Elle s'appuie sur diverses sciences auxiliaires complétant selon les travaux menés la compétence générale de l'historien.\n",
    " Elle reste malgré tout une construction humaine, inévitablement inscrite dans son époque, susceptible d'être utilisée en dehors de son domaine,\n",
    " notamment à des fins d'ordre politique. \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "UH2LKmshYkDM"
   },
   "outputs": [],
   "source": [
    "nlp = French()\n",
    "\n",
    "documentNLP = nlp(texteNLP)\n",
    "documentDataScience = nlp(texteDataScience)\n",
    "documentEconometrie = nlp(texteEconometrie)\n",
    "documentHistoire = nlp(texteHistoire)\n",
    "\n",
    "bagOfWordsNLP = []\n",
    "bagOfWordsDataScience = []\n",
    "bagOfWordsEconometrie = []\n",
    "bagOfWordsHistoire = []\n",
    "\n",
    "for token in documentNLP:\n",
    "    bagOfWordsNLP.append(token.text)\n",
    "\n",
    "for token in documentDataScience:\n",
    "    bagOfWordsDataScience.append(token.text)\n",
    "\n",
    "for token in documentEconometrie:\n",
    "    bagOfWordsEconometrie.append(token.text)\n",
    "\n",
    "for token in documentHistoire:\n",
    "    bagOfWordsHistoire.append(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "DUmZrE2lYk42"
   },
   "outputs": [],
   "source": [
    "uniqueWords = set(bagOfWordsNLP).union(set(bagOfWordsDataScience)).union(set(bagOfWordsEconometrie)).union(set(bagOfWordsHistoire))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "U2k-oq2MYlFy"
   },
   "outputs": [],
   "source": [
    "numOfWordsNLP = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsNLP:\n",
    "    numOfWordsNLP[word] += 1\n",
    "\n",
    "numOfWordsDataScience = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsDataScience:\n",
    "    numOfWordsDataScience[word] += 1\n",
    "\n",
    "numOfWordsEconometrie = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsEconometrie:\n",
    "    numOfWordsEconometrie[word] += 1\n",
    "\n",
    "numOfWordsHistoire = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsHistoire:\n",
    "    numOfWordsHistoire[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "U4UZsBqBYlac"
   },
   "outputs": [],
   "source": [
    "def computeTF(wordDict, bagOfWords):\n",
    "    tfDict = {}\n",
    "    bagOfWordsCount = len(bagOfWords)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count / float(bagOfWordsCount)\n",
    "    return tfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "1eGebIApYlje"
   },
   "outputs": [],
   "source": [
    "tfNLP = computeTF(numOfWordsNLP, bagOfWordsNLP)\n",
    "tfDataScience = computeTF(numOfWordsDataScience, bagOfWordsDataScience)\n",
    "tfEconometrie = computeTF(numOfWordsEconometrie, bagOfWordsEconometrie)\n",
    "tfHistoire = computeTF(numOfWordsHistoire, bagOfWordsHistoire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "yw240CWgYlsr"
   },
   "outputs": [],
   "source": [
    "def computeIDF(documents):\n",
    "    import math\n",
    "    N = len(documents)\n",
    "    \n",
    "    idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
    "    for document in documents:\n",
    "        for word, val in document.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log(N / float(val))\n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "C7_ejnuOZWez"
   },
   "outputs": [],
   "source": [
    "idfs = computeIDF([numOfWordsNLP, numOfWordsDataScience, numOfWordsEconometrie, numOfWordsHistoire])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "vW2xwamlZWmq"
   },
   "outputs": [],
   "source": [
    "def computeTFIDF(tfBagOfWords, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBagOfWords.items():\n",
    "        tfidf[word] = val * idfs[word]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "TB2O-vKAZWtm"
   },
   "outputs": [],
   "source": [
    "tfidfNLP = computeTFIDF(tfNLP, idfs)\n",
    "tfidfDataScience = computeTFIDF(tfDataScience, idfs)\n",
    "tfidfEconometrie = computeTFIDF(tfEconometrie, idfs)\n",
    "tfidfHistoire = computeTFIDF(tfHistoire, idfs)\n",
    "df = pd.DataFrame([tfidfNLP, tfidfDataScience, tfidfEconometrie, tfidfHistoire])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "hidden": true,
    "id": "4KiDyXbubBF1",
    "outputId": "fd50a624-43ce-4992-eea3-1a4413fefdf7"
   },
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "oLiRFXtzZWzE"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform([texteNLP, texteDataScience, texteEconometrie, texteHistoire])\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "df = pd.DataFrame(denselist, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 146
    },
    "colab_type": "code",
    "hidden": true,
    "id": "w6LHAKsYZW37",
    "outputId": "ecc29b42-06ea-417d-c150-54220c6c53b8"
   },
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "colab_type": "code",
    "hidden": true,
    "id": "rmYJ_oGYZW8Y",
    "outputId": "80fef33b-2d6e-4720-dd5a-599b8fe4b739"
   },
   "outputs": [],
   "source": [
    "df.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "3o4KAtR6ZXAs"
   },
   "outputs": [],
   "source": [
    "dist_NLP_DataScience = np.linalg.norm(df.loc[0] - df.loc[1])\n",
    "dist_NLP_Econometrie = np.linalg.norm(df.loc[0] - df.loc[2])\n",
    "dist_NLP_Histoire = np.linalg.norm(df.loc[0] - df.loc[3])\n",
    "dist_DataScience_Econometrie = np.linalg.norm(df.loc[1] - df.loc[2])\n",
    "dist_DataScience_Histoire = np.linalg.norm(df.loc[1] - df.loc[3])\n",
    "dist_Econometrie_Histoire = np.linalg.norm(df.loc[2] - df.loc[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "hidden": true,
    "id": "qVQzDI2EZXE3",
    "outputId": "1afd2505-42f4-406f-f164-1ab7ed9d154c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(dist_NLP_DataScience)\n",
    "print(dist_NLP_Econometrie)\n",
    "print(dist_NLP_Histoire)\n",
    "print(dist_DataScience_Econometrie)\n",
    "print(dist_DataScience_Histoire)\n",
    "print(dist_Econometrie_Histoire)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Embeddings de mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "WMGgjIn2k1Ig"
   },
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "hidden": true,
    "id": "dww0XB9-ZXXm",
    "outputId": "874b8ab1-f33b-4106-868d-e19000746fea"
   },
   "outputs": [],
   "source": [
    "import gensim \n",
    "from gensim.models import Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "xYaB2OBaZXbG"
   },
   "outputs": [],
   "source": [
    "texte1 = \"\"\"L’exploration de données, connue aussi sous l'expression de fouille de données, forage de données, prospection de données, data mining,\n",
    " ou encore extraction de connaissances à partir de données, a pour objet l’extraction d'un savoir ou d'une connaissance à partir de grandes quantités\n",
    " de données, par des méthodes automatiques ou semi-automatiques.\n",
    "Elle se propose d'utiliser un ensemble d'algorithmes issus de disciplines scientifiques diverses telles que les statistiques,\n",
    " l'intelligence artificielle ou l'informatique, pour construire des modèles à partir des données,\n",
    " c'est-à-dire trouver des structures intéressantes ou des motifs selon des critères fixés au préalable,\n",
    " et d'en extraire un maximum de connaissances.\n",
    "L'utilisation industrielle ou opérationnelle de ce savoir dans le monde professionnel permet de résoudre des problèmes très divers,\n",
    " allant de la gestion de la relation client à la maintenance préventive, en passant par la détection de fraudes ou encore l'optimisation de sites web.\n",
    "C'est aussi le mode de travail du journalisme de données.\n",
    "L'exploration de données fait suite, dans l'escalade de l'exploitation des données de l'entreprise, à l'informatique décisionnelle.\n",
    "Celle-ci permet de constater un fait, tel que le chiffre d'affaires, et de l'expliquer comme le chiffre d'affaires décliné par produits,\n",
    " tandis que l'exploration de données permet de classer les faits et de les prévoir dans une certaine mesure ou encore de les éclairer en révélant\n",
    " par exemple les variables ou paramètres qui pourraient faire comprendre pourquoi le chiffre d'affaires de tel point de vente est supérieur\n",
    " à celui de tel autre. \"\"\"\n",
    "\n",
    "texte2 = \"\"\"En statistique, les analyses multivariées ont pour caractéristique de s'intéresser à des lois de probabilité à plusieurs variables.\n",
    "Les analyses bivariées sont des cas particuliers à deux variables.\n",
    "Les analyses multivariées sont très diverses selon l'objectif recherché, la nature des variables et la mise en œuvre formelle.\n",
    "On peut identifier deux grandes familles : celle des méthodes descriptives (visant à structurer et résumer l'information)\n",
    " et celle des méthodes explicatives visant à expliquer une ou des variables dites « dépendantes » (variables à expliquer) par un ensemble de variables\n",
    " dites « indépendantes » (variables explicatives).\n",
    "Les méthodes appelées en français analyse des données en sont un sous-ensemble. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "texte = nlp(texte1 + texte2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "Xd-JukqaZXeg"
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "# iterate through each sentence in the file \n",
    "for sent in texte: \n",
    "    temp = []\n",
    "      \n",
    "    # tokenize the sentence into words \n",
    "    for token in texte: \n",
    "        temp.append(token.text.lower()) \n",
    "  \n",
    "    data.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "hidden": true,
    "id": "hH25xm_jZXh7",
    "outputId": "0390a0fc-3444-4145-d253-d3ed89a12722"
   },
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "3QFXtLWm6wch"
   },
   "source": [
    "### Continuous bag of words (CBOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "b4tlfQB561cS"
   },
   "source": [
    "Le modèle CBOW prédit le mot courant étant donné les mots de contexte dans une fenêtre autour du mot courant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "hidden": true,
    "id": "A5jZxGw0ZXlm",
    "outputId": "b7263297-242f-4f46-d27f-44ef1ee0cb4c"
   },
   "outputs": [],
   "source": [
    "# Create CBOW model \n",
    "model1 = gensim.models.Word2Vec(data, min_count = 1,  \n",
    "                              size = 100, window = 5,\n",
    "                              sg = 0)\n",
    "\n",
    "# Print results\n",
    "print(\"Cosine similarity between 'données' \" + \n",
    "               \"and 'connaissance' - CBOW : \", \n",
    "    model1.wv.similarity('données', 'connaissance'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "NSBSotcP7gna"
   },
   "source": [
    "### Skip gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "OM3PLzyv7hQb"
   },
   "source": [
    "La méthode \"skip gram\" fait le contraire de ce que fait la méthode \"cbow\" : elle prédit les mots de contexte d'un mot donné."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "hidden": true,
    "id": "6A0xo1WJZXpV",
    "outputId": "bceb6790-3665-45e7-d0dc-aad4d8b55068"
   },
   "outputs": [],
   "source": [
    "# Create Skip Gram model \n",
    "model2 = gensim.models.Word2Vec(data, min_count = 1,\n",
    "                                size = 100, window = 5,\n",
    "                                sg = 1) \n",
    "\n",
    "# Print results \n",
    "print(\"Cosine similarity between 'données' \" +\n",
    "          \"and 'connaissance' - Skip Gram : \", \n",
    "    model2.wv.similarity('données', 'connaissance')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "SAEF7KdfZXwJ"
   },
   "outputs": [],
   "source": [
    "word_vectors = model1.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "hidden": true,
    "id": "J740GCysZXtC",
    "outputId": "e21bf589-7663-4dc0-b47f-ed097c5b40fe"
   },
   "outputs": [],
   "source": [
    "print(word_vectors['données'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "hidden": true,
    "id": "VgLh1oxr-0vv",
    "outputId": "73b027c3-e5e2-40db-8502-81aa10b0c0dc"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# api.info(\"glove-wiki-gigaword-100\")\n",
    "word_vectors = api.load(\"glove-wiki-gigaword-100\")  # load pre-trained word-vectors from gensim-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "hidden": true,
    "id": "j_FlOX5e-19a",
    "outputId": "da64440e-831a-461a-b5fd-4751cbf48be5"
   },
   "outputs": [],
   "source": [
    "result = word_vectors.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "print(\"{}: {:.4f}\".format(*result[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "hidden": true,
    "id": "sszMfjlFjVnp",
    "outputId": "f806f3a7-deb1-45e0-fa94-f0c8f6a8829c"
   },
   "outputs": [],
   "source": [
    "result = word_vectors.most_similar(positive=['female', 'lion'], negative=['male'])\n",
    "print(\"{}: {:.4f}\".format(*result[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "colab_type": "code",
    "hidden": true,
    "id": "08wJCNXF-2Xy",
    "outputId": "b1a9652a-bc0f-45fb-9175-e90d5c3c8fed"
   },
   "outputs": [],
   "source": [
    "print(word_vectors.doesnt_match(\"breakfast cereal dinner lunch\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "hidden": true,
    "id": "8uWeTuU2-2g0",
    "outputId": "d43065a1-b9b8-4688-9d72-f18e0232d0ef"
   },
   "outputs": [],
   "source": [
    "result = word_vectors.similar_by_word(\"cat\")\n",
    "print(\"{}: {:.4f}\".format(*result[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "z0DVkHDjsS4z"
   },
   "source": [
    "### FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "SMRAVtne-2ny"
   },
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "# from gensim.test.utils import common_texts\n",
    "# print(common_texts[0])\n",
    "# ['human', 'interface', 'computer']\n",
    "\n",
    "model = FastText(data, size=100, window=5, min_count=5, workers=4, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257
    },
    "colab_type": "code",
    "hidden": true,
    "id": "oPPHel-8-2ts",
    "outputId": "d113941f-df5e-4442-bb72-e4119241c012"
   },
   "outputs": [],
   "source": [
    "model.wv.most_similar(\"données\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "hidden": true,
    "id": "LHBYQpJJ-2zC",
    "outputId": "73f0aac8-e60d-46cc-a3ae-c545a21ca827"
   },
   "outputs": [],
   "source": [
    "import fasttext.util\n",
    "\n",
    "fasttext.util.download_model('en', if_exists='ignore')\n",
    "ft = fasttext.load_model('cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "hidden": true,
    "id": "PFt0bOct-27l",
    "outputId": "919e8706-b640-4a92-9d40-3388275c6b5d"
   },
   "outputs": [],
   "source": [
    "ft.get_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "6RRvkNjD-2_n"
   },
   "outputs": [],
   "source": [
    "# fasttext.util.reduce_model(ft, 100)\n",
    "# ft.get_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "hidden": true,
    "id": "FtvT4wQV-3DT",
    "outputId": "24ed2574-33ce-4ac9-d432-48ef5a909965"
   },
   "outputs": [],
   "source": [
    "ft.get_word_vector('hello').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "colab_type": "code",
    "hidden": true,
    "id": "YFUESWkK-3HX",
    "outputId": "63a8eeb5-6365-40bb-899a-30860838ef93"
   },
   "outputs": [],
   "source": [
    "ft.get_nearest_neighbors('hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Embeddings keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import spacy\n",
    "from keras.preprocessing.text import one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Création d'un corpus d'exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sample_text_1=\"bitty bought a bit of butter\"\n",
    "sample_text_2=\"but the bit of butter was a bit bitter\"\n",
    "sample_text_3=\"so she bought some better butter to make the bitter butter better\"\n",
    "\n",
    "corp=[sample_text_1,sample_text_2,sample_text_3]\n",
    "no_docs=len(corp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Encodage du corpus en one-hot à l'aide de la fonction keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vocab_size=50 \n",
    "encod_corp=[]\n",
    "for i,doc in enumerate(corp):\n",
    "    # taille de vocab 50 pour être sur que chaque mot est encodé sur un entier unique.\n",
    "    encod_corp.append(one_hot(doc,50))\n",
    "    print(\"The encoding for document\",i+1,\" is : \",one_hot(doc,50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Padding des documents : la couche d'embedding de keras nécessite des entrées de la même longueur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#!python -m spacy download fr_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# nlp = spacy.load('fr_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# length of maximum document. will be nedded whenever create embeddings for the words\n",
    "maxlen = -1\n",
    "for doc in corp:\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    if(maxlen < len(tokens)):\n",
    "        maxlen = len(tokens)\n",
    "print(\"The maximum number of words in any document is : \",maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# now to create embeddings all of our docs need to be of same length. hence we can pad the docs with zeros.\n",
    "pad_corp=pad_sequences(encod_corp,maxlen=maxlen,padding='post',value=0.0)\n",
    "print(\"No of padded documents: \",len(pad_corp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i,doc in enumerate(pad_corp):\n",
    "     print(\"The padded encoding for document\",i+1,\" is : \",doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import CamembertTokenizer, CamembertConfig\n",
    "from transformers import CamembertForTokenClassification\n",
    "from tqdm import tqdm, trange\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentences, labels = [], []\n",
    "with open(\"frwikinews-20130110-pages-articles.txt.tok.stanford-pos\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f.readlines():\n",
    "        sentence = []\n",
    "        sent_tag = []\n",
    "        tokens = line.replace(\"\\n\", \"\").split(\" \")\n",
    "        for token in tokens:\n",
    "            splits = token.split(\"_\")\n",
    "            if len(splits) != 2: continue\n",
    "            word, tag = splits\n",
    "            sentence.append(word)\n",
    "            sent_tag.append(tag)\n",
    "        sentences.append(\" \".join(sentence))\n",
    "        labels.append(sent_tag)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tags_val = list(set().union(*labels))\n",
    "tag2idx = {t:i for i,t in enumerate(tags_val)}\n",
    "# tag2idx[\"<PAD>\"] = len(tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lens = np.array(list(map(len, sentences)))\n",
    "lens.min(), lens.max(), lens.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 150\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tokenizer = CamembertTokenizer.from_pretrained('camembert-base/', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "print(tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
    "                     maxlen=MAX_LEN, value=tag2idx[\"PONCT\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "attention_masks = [[float(i>0) for i in ii] for ii in input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags, \n",
    "                                                            random_state=2018, test_size=0.1)\n",
    "tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2018, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tr_inputs = torch.tensor(tr_inputs)\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "tr_tags = torch.tensor(tr_tags)\n",
    "val_tags = torch.tensor(val_tags)\n",
    "tr_masks = torch.tensor(tr_masks)\n",
    "val_masks = torch.tensor(val_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = CamembertForTokenClassification.from_pretrained(\"camembert-base/\", num_labels=len(tag2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "FULL_FINETUNING = True #True\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters()) \n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "max_grad_norm = 1.0\n",
    "total_t0 = time.time()\n",
    "\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    t0 = time.time()\n",
    "    # TRAIN loop\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        \n",
    "        if step % 10 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "            \n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # forward pass\n",
    "        loss, something_else = model(b_input_ids, token_type_ids=None,\n",
    "                     attention_mask=b_input_mask, labels=b_labels)\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # track train loss\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "    # print train loss per epoch\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    # VALIDATION on validation set\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions , true_labels = [], []\n",
    "    for batch in valid_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n",
    "                                  attention_mask=b_input_mask, labels=b_labels)[0]\n",
    "            logits = model(b_input_ids, token_type_ids=None,\n",
    "                           attention_mask=b_input_mask)[0]\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        true_labels.append(label_ids)\n",
    "        \n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        \n",
    "        nb_eval_examples += b_input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "    eval_loss = eval_loss/nb_eval_steps\n",
    "    print(\"Validation loss: {}\".format(eval_loss))\n",
    "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "    pred_tags = [tags_val[p_i] for p in predictions for p_i in p]\n",
    "    valid_tags = [tags_val[l_ii] for l in true_labels for l_i in l for l_ii in l_i]\n",
    "    print(\"F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "for batch in valid_dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n",
    "                              attention_mask=b_input_mask, labels=b_labels)[0]\n",
    "        logits = model(b_input_ids, token_type_ids=None,\n",
    "                       attention_mask=b_input_mask)[0]\n",
    "        \n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    true_labels.append(label_ids)\n",
    "#     tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "\n",
    "    eval_loss += tmp_eval_loss.mean().item()\n",
    "#     eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    nb_eval_examples += b_input_ids.size(0)\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "pred_tags = [[tags_val[p_i] for p_i in p] for p in predictions]\n",
    "valid_tags = [[tags_val[l_ii] for l_ii in l_i] for l in true_labels for l_i in l ]\n",
    "print(\"Validation loss: {}\".format(eval_loss/nb_eval_steps))\n",
    "# print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "print(\"Validation F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model, open('CamemBERT_POS', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings de doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Introduces Gensim's Doc2Vec model and demonstrates its use on the Lee Corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "CRKcnSLzLKs5"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "hidden": true,
    "id": "0izPSUqCLKtE",
    "outputId": "ac4abd5e-891f-421b-9186-1e3186a70aa7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "# Set file names for train and test data\n",
    "test_data_dir = os.path.join(gensim.__path__[0], 'test', 'test_data')\n",
    "lee_train_file = os.path.join(test_data_dir, 'lee_background.cor')\n",
    "lee_test_file = os.path.join(test_data_dir, 'lee.cor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "Ydji5yVtLKtM"
   },
   "outputs": [],
   "source": [
    "import smart_open\n",
    "\n",
    "def read_corpus(fname, tokens_only=False):\n",
    "    with smart_open.open(fname, encoding=\"iso-8859-1\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            tokens = gensim.utils.simple_preprocess(line)\n",
    "            if tokens_only:\n",
    "                yield tokens\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n",
    "\n",
    "train_corpus = list(read_corpus(lee_train_file))\n",
    "test_corpus = list(read_corpus(lee_test_file, tokens_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "Z8gIsgByLKtT"
   },
   "source": [
    "Let's take a look at the training corpus\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "hidden": true,
    "id": "6hvPFckPLKtU",
    "outputId": "6307261c-7080-4ba4-a9e1-9358102895ca"
   },
   "outputs": [],
   "source": [
    "print(train_corpus[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "mJweA-XfLKtb"
   },
   "source": [
    "And the testing corpus looks like this:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "hidden": true,
    "id": "oMrYDqBhLKtd",
    "outputId": "6feb1a68-a753-4a76-f216-bf15f1b0f32e"
   },
   "outputs": [],
   "source": [
    "print(test_corpus[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "IRTuQP9ZLKtj"
   },
   "source": [
    "Notice that the testing corpus is just a list of lists and does not contain\n",
    "any tags.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "0fFw9-GNLKtk"
   },
   "source": [
    "Training the Model\n",
    "------------------\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "hidden": true,
    "id": "1hMFvbhJLKtl",
    "outputId": "3de00c95-d248-483b-af2f-3a45714c3ba8"
   },
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "AawhBdwXLKtp"
   },
   "source": [
    "Build a vocabulary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "hidden": true,
    "id": "IsYsdsARLKtr",
    "outputId": "a9243ce4-7461-47c4-a9a4-e88f005464a3"
   },
   "outputs": [],
   "source": [
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "Aw7X_oekLKty"
   },
   "source": [
    "Essentially, the vocabulary is a dictionary (accessible via\n",
    "``model.wv.vocab``\\ ) of all of the unique words extracted from the training\n",
    "corpus along with the count (e.g., ``model.wv.vocab['penalty'].count`` for\n",
    "counts for the word ``penalty``\\ ).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "hidden": true,
    "id": "NwQWJNdGLKt0",
    "outputId": "5127f105-409e-4bb9-9c7e-0ff24a7f2481"
   },
   "outputs": [],
   "source": [
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "yZRrhB2nLKt5"
   },
   "source": [
    "Now, we can use the trained model to infer a vector for any piece of text\n",
    "by passing a list of words to the ``model.infer_vector`` function. This\n",
    "vector can then be compared with other vectors via cosine similarity.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 182
    },
    "hidden": true,
    "id": "TO9U7nxCLKt7",
    "outputId": "fb553030-b856-4320-8681-19b71699456b"
   },
   "outputs": [],
   "source": [
    "vector = model.infer_vector(['only', 'you', 'can', 'prevent', 'forest', 'fires'])\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "g424ZE1WLKuC"
   },
   "source": [
    "Assessing the Model\n",
    "-------------------\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "hidden": true,
    "id": "y4tC54xDLKuE",
    "outputId": "63f978f7-ddd9-4c2b-e27a-f07c37331bf4"
   },
   "outputs": [],
   "source": [
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(train_corpus)):\n",
    "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "    second_ranks.append(sims[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "AOiGfzgYLKuI"
   },
   "source": [
    "Let's count how each document ranks with respect to the training corpus\n",
    "\n",
    "NB. Results vary between runs due to random seeding and very small corpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "hidden": true,
    "id": "Y-v8Njt1LKuJ",
    "outputId": "7b8c5521-36e6-45c8-cb22-1f9c32ca3238"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "counter = collections.Counter(ranks)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "XxBrgrAPLKuN"
   },
   "source": [
    "Basically, greater than 95% of the inferred documents are found to be most\n",
    "similar to itself and about 5% of the time it is mistakenly most similar to\n",
    "another document. Checking the inferred-vector against a\n",
    "training-vector is a sort of 'sanity check' as to whether the model is\n",
    "behaving in a usefully consistent manner, though not a real 'accuracy' value.\n",
    "\n",
    "This is great and not entirely surprising. We can take a look at an example:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257
    },
    "hidden": true,
    "id": "4uI9ofS6LKuO",
    "outputId": "1584f662-61f5-47ae-9678-ad2102b8d5a5"
   },
   "outputs": [],
   "source": [
    "print('Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "nywYZwvkLKuT"
   },
   "source": [
    "Notice above that the most similar document (usually the same text) is has a\n",
    "similarity score approaching 1.0. However, the similarity score for the\n",
    "second-ranked documents should be significantly lower (assuming the documents\n",
    "are in fact different) and the reasoning becomes obvious when we examine the\n",
    "text itself.\n",
    "\n",
    "We can run the next cell repeatedly to see a sampling other target-document\n",
    "comparisons.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "hidden": true,
    "id": "uV5KV7T7LKuT",
    "outputId": "3aa940e7-9618-412b-fa26-67121e6cc27c"
   },
   "outputs": [],
   "source": [
    "# Pick a random document from the corpus and infer a vector from the model\n",
    "import random\n",
    "doc_id = random.randint(0, len(train_corpus) - 1)\n",
    "\n",
    "# Compare and print the second-most-similar document\n",
    "print('Train Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "sim_id = second_ranks[doc_id]\n",
    "print('Similar Document {}: «{}»\\n'.format(sim_id, ' '.join(train_corpus[sim_id[0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "i9Sppy4SLKuW"
   },
   "source": [
    "Testing the Model\n",
    "-----------------\n",
    "\n",
    "Using the same approach above, we'll infer the vector for a randomly chosen\n",
    "test document, and compare the document to our model by eye.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257
    },
    "hidden": true,
    "id": "wrnlmeNwLKuX",
    "outputId": "1506524e-ab78-4252-b751-b4de3483b941"
   },
   "outputs": [],
   "source": [
    "# Pick a random document from the test corpus and infer a vector from the model\n",
    "doc_id = random.randint(0, len(test_corpus) - 1)\n",
    "inferred_vector = model.infer_vector(test_corpus[doc_id])\n",
    "sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "\n",
    "# Compare and print the most/median/least similar documents from the train corpus\n",
    "print('Test Document ({}): «{}»\\n'.format(doc_id, ' '.join(test_corpus[doc_id])))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "TGp0qeXvtFeW"
   },
   "source": [
    "## LDA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Introduces Gensim's LDA model and demonstrates its use on the NIPS corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "w2Hcagg7tFeY"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "eHfA8Ss9tFel"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os.path\n",
    "import re\n",
    "import tarfile\n",
    "import nltk\n",
    "\n",
    "import smart_open\n",
    "\n",
    "def extract_documents(url='https://cs.nyu.edu/~roweis/data/nips12raw_str602.tgz'):\n",
    "    fname = url.split('/')[-1]\n",
    "    \n",
    "    # Download the file to local storage first.\n",
    "    # We can't read it on the fly because of \n",
    "    # https://github.com/RaRe-Technologies/smart_open/issues/331\n",
    "    if not os.path.isfile(fname):\n",
    "        with smart_open.open(url, \"rb\") as fin:\n",
    "            with smart_open.open(fname, 'wb') as fout:\n",
    "                while True:\n",
    "                    buf = fin.read(io.DEFAULT_BUFFER_SIZE)\n",
    "                    if not buf:\n",
    "                        break\n",
    "                    fout.write(buf)\n",
    "                         \n",
    "    with tarfile.open(fname, mode='r:gz') as tar:\n",
    "        # Ignore directory entries, as well as files like README, etc.\n",
    "        files = [\n",
    "            m for m in tar.getmembers()\n",
    "            if m.isfile() and re.search(r'nipstxt/nips\\d+/\\d+\\.txt', m.name)\n",
    "        ]\n",
    "        for member in sorted(files, key=lambda x: x.name):\n",
    "            member_bytes = tar.extractfile(member).read()\n",
    "            yield member_bytes.decode('utf-8', errors='replace')\n",
    "\n",
    "docs = list(extract_documents())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "jr7W3VG2tFes"
   },
   "source": [
    "So we have a list of 1740 documents, where each document is a Unicode string. \n",
    "If you're thinking about using your own corpus, then you need to make sure\n",
    "that it's in the same format (list of Unicode strings) before proceeding\n",
    "with the rest of this tutorial.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "hidden": true,
    "id": "1wUTd2bctFe1",
    "outputId": "9d12f933-5e16-41fd-dca2-8331cfcf5614"
   },
   "outputs": [],
   "source": [
    "print(len(docs))\n",
    "print(docs[0][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "Lr2-gpuHtFe8"
   },
   "outputs": [],
   "source": [
    "# Tokenize the documents.\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Split the documents into tokens.\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for idx in range(len(docs)):\n",
    "    docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "    docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "# Remove numbers, but not words that contain numbers.\n",
    "docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "\n",
    "# Remove words that are only one character.\n",
    "docs = [[token for token in doc if len(token) > 1] for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "S1oN4BL4tFfD"
   },
   "source": [
    "We use the WordNet lemmatizer from NLTK. A lemmatizer is preferred over a\n",
    "stemmer in this case because it produces more readable words. Output that is\n",
    "easy to read is very desirable in topic modelling.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "hidden": true,
    "id": "p2D2GTM-tFfE",
    "outputId": "047e8101-67c9-4282-a52c-434dfcedf1b6"
   },
   "outputs": [],
   "source": [
    "# Lemmatize the documents.\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "wjyVTiretFfK"
   },
   "source": [
    "We find bigrams in the documents. Bigrams are sets of two adjacent words.\n",
    "Using bigrams we can get phrases like \"machine_learning\" in our output\n",
    "(spaces are replaced with underscores); without bigrams we would only get\n",
    "\"machine\" and \"learning\".\n",
    "\n",
    "Note that in the code below, we find bigrams and then add them to the\n",
    "original data, because we would like to keep the words \"machine\" and\n",
    "\"learning\" as well as the bigram \"machine_learning\".\n",
    "\n",
    ".. Important::\n",
    "    Computing n-grams of large dataset can be very computationally\n",
    "    and memory intensive.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 146
    },
    "hidden": true,
    "id": "_bwLiLmptFfL",
    "outputId": "67258211-2afd-4e79-e485-7f5b1a90d361"
   },
   "outputs": [],
   "source": [
    "# Compute bigrams.\n",
    "from gensim.models import Phrases\n",
    "\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "213Kpo2ztFfP"
   },
   "source": [
    "We remove rare words and common words based on their *document frequency*.\n",
    "Below we remove words that appear in less than 20 documents or in more than\n",
    "50% of the documents. Consider trying to remove words only based on their\n",
    "frequency, or maybe combining that with this approach.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109
    },
    "hidden": true,
    "id": "CRNGpD0EtFfP",
    "outputId": "7b53cf12-5c4f-4c97-da58-6a1e860d97a9"
   },
   "outputs": [],
   "source": [
    "# Remove rare and common tokens.\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "Do9JhUYhtFfV"
   },
   "source": [
    "Finally, we transform the documents to a vectorized form. We simply compute\n",
    "the frequency of each word, including the bigrams.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "n_hZjiu3tFfV"
   },
   "outputs": [],
   "source": [
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "hRVkC8v1tFfb"
   },
   "source": [
    "Let's see how many tokens and documents we have to train on.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "hidden": true,
    "id": "mJf9QhAatFfc",
    "outputId": "cceddb80-a33d-4fed-d819-5eaa58a3c9ba"
   },
   "outputs": [],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "hidden": true,
    "id": "qKqG2x8ytFfj",
    "outputId": "443c8794-6b03-4d6a-bc53-682a1a656d70",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train LDA model.\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 10\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "hidden": true,
    "id": "AZr6b66itFfr",
    "outputId": "b4a2468b-f331-451c-d053-7873c0a63e77",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_topics = model.top_topics(corpus) #, num_words=20)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "VOgn7Wl9swsB"
   },
   "source": [
    "## Word Movers' Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Demonstrates using Gensim's implemenation of the WMD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "RTN0tDhpswsI"
   },
   "outputs": [],
   "source": [
    "# Initialize logging.\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "sentence_obama = 'Obama speaks to the media in Illinois'\n",
    "sentence_president = 'The president greets the press in Chicago'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "TjIOnZ1CswsP"
   },
   "source": [
    "These sentences have very similar content, and as such the WMD should be low.\n",
    "Before we compute the WMD, we want to remove stopwords (\"the\", \"to\", etc.),\n",
    "as these do not contribute a lot to the information in the sentences.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "hidden": true,
    "id": "DPyuX3VmswsR",
    "outputId": "032c7107-227e-42fb-e35f-0f024fce22fc"
   },
   "outputs": [],
   "source": [
    "# Import and download stopwords from NLTK.\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "download('stopwords')  # Download stopwords list.\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def preprocess(sentence):\n",
    "    return [w for w in sentence.lower().split() if w not in stop_words]\n",
    "\n",
    "sentence_obama = preprocess(sentence_obama)\n",
    "sentence_president = preprocess(sentence_president)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "dDg7-4KkswsX"
   },
   "source": [
    "Now, as mentioned earlier, we will be using some downloaded pre-trained\n",
    "embeddings. We load these into a Gensim Word2Vec model class.\n",
    "\n",
    ".. Important::\n",
    "  The embeddings we have chosen here require a lot of memory.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "hidden": true,
    "id": "ZW-VlCMHswsZ",
    "outputId": "28af71b8-1fd9-4e04-a682-3823fb63a3c2"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "eSeoEAz3swsf"
   },
   "source": [
    "So let's compute WMD using the ``wmdistance`` method.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "hidden": true,
    "id": "QQpDAHYhswsh",
    "outputId": "b9f5f061-2cff-4880-c18d-9aa1380ee365"
   },
   "outputs": [],
   "source": [
    "distance = model.wmdistance(sentence_obama, sentence_president)\n",
    "print('distance = %.4f' % distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "QqVqTgTXswsn"
   },
   "source": [
    "Let's try the same thing with two completely unrelated sentences. Notice that the distance is larger.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "hidden": true,
    "id": "LVe5tkUkswso",
    "outputId": "2b915ab9-79eb-430c-ab27-54e2eea7396c"
   },
   "outputs": [],
   "source": [
    "sentence_orange = preprocess('Oranges are my favorite fruit')\n",
    "distance = model.wmdistance(sentence_obama, sentence_orange)\n",
    "print('distance = %.4f' % distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 146
    },
    "hidden": true,
    "id": "8Fm3r_8rswsu",
    "outputId": "6d814d42-a9b6-4317-fdab-f02f7a953099"
   },
   "outputs": [],
   "source": [
    "model.init_sims(replace=True)  # Normalizes the vectors in the word2vec class.\n",
    "\n",
    "distance = model.wmdistance(sentence_obama, sentence_president)  # Compute WMD as normal.\n",
    "print('distance: %r' % distance)\n",
    "\n",
    "distance = model.wmdistance(sentence_obama, sentence_orange)\n",
    "print('distance = %.4f' % distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "izA3-6kffbdT"
   },
   "source": [
    "## DistillBERT sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "hidden": true,
    "id": "To9ENLU90WGl",
    "outputId": "c337b24e-1e24-4fd2-e06e-cb760348fea9"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "fvFvBLJV0Dkv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import torch\n",
    "import transformers as ppb\n",
    "import urllib.request\n",
    "import os\n",
    "import zipfile\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "hidden": true,
    "id": "KDW7swH8-Py5",
    "outputId": "3baea8e8-55b8-41b9-bbdc-04c327921953"
   },
   "outputs": [],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "PkHh_CVP-BAE"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', delimiter='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "gTM3hOHW4hUY"
   },
   "outputs": [],
   "source": [
    "batch_1 = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "6t-svuwwGo3K"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "hidden": true,
    "id": "RUHzX36pSg3v",
    "outputId": "da372666-b09e-4003-b232-415ae91f8f07"
   },
   "outputs": [],
   "source": [
    "batch_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "hidden": true,
    "id": "jGvcfcCP5xpZ",
    "outputId": "0665636b-10f5-4dc6-a779-2d345fc7e3cf"
   },
   "outputs": [],
   "source": [
    "batch_1[1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "q1InADgf5xm2"
   },
   "outputs": [],
   "source": [
    "# DistilBERT:\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "\n",
    "## Bert\n",
    "#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "\n",
    "# chargement modèle et tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "Dg82ndBA5xlN"
   },
   "outputs": [],
   "source": [
    "# Tokenization de nos phrases pour distillBERT\n",
    "tokenized = batch_1[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "URn-DWJt5xhP"
   },
   "outputs": [],
   "source": [
    "# Padding afin que chaque phrase fasse la même taille\n",
    "\n",
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "hidden": true,
    "id": "jdi7uXo95xeq",
    "outputId": "71667bf4-c66d-4b12-a505-db89f0a1d1a2"
   },
   "outputs": [],
   "source": [
    "np.array(padded).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "hidden": true,
    "id": "4K_iGRNa_Ozc",
    "outputId": "64c3547c-a0cb-444a-ef7a-ae10956537fa"
   },
   "outputs": [],
   "source": [
    "# On cache ce padding\n",
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "39UVjAV56PJz"
   },
   "outputs": [],
   "source": [
    "# On applique le modèle sur nos token avec le masque\n",
    "model.eval()\n",
    "input_ids = torch.tensor(padded)  \n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "hidden": true,
    "id": "su7oNNFXce0f",
    "outputId": "49347c75-d30c-4493-e3dd-7986bb084ce5"
   },
   "outputs": [],
   "source": [
    "np.shape(last_hidden_states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "C9t60At16PVs"
   },
   "outputs": [],
   "source": [
    "# Chaque token obtient un vecteur, ici, seul le token special 'cls' en position 1 nous intéresse\n",
    "features = last_hidden_states[0][:,0,:].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "JD3fX2yh6PTx"
   },
   "outputs": [],
   "source": [
    "labels = batch_1[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "iaoEvM2evRx1"
   },
   "source": [
    "### Suite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Entrainnement d'une reg log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "ddAqbkoU6PP9"
   },
   "outputs": [],
   "source": [
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "hidden": true,
    "id": "gG-EVWx4CzBc",
    "outputId": "d0539784-43b4-470f-dfe8-ffe6133dc11d"
   },
   "outputs": [],
   "source": [
    "lr_clf = LogisticRegression(C=1)\n",
    "lr_clf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "hidden": true,
    "id": "iCoyxRJ7ECTA",
    "outputId": "862b9f89-ae20-4850-bbc8-4a041483f67c"
   },
   "outputs": [],
   "source": [
    "lr_clf.score(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Comparaison des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
