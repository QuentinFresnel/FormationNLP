{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "3oRtjzFQYbMv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "vWzzZG9JYiwB"
   },
   "outputs": [],
   "source": [
    "documentNLP = \"\"\"Le traitement automatique du langage naturel (abr. TALN), ou traitement automatique de la langue naturelle, \n",
    " ou encore traitement automatique des langues (abr. TAL) est un domaine multidisciplinaire impliquant la linguistique,\n",
    " l'informatique et l'intelligence artificielle, qui vise à créer des outils de traitement de la langue naturelle pour diverses applications.\n",
    " Il ne doit pas être confondu avec la linguistique informatique, qui vise à comprendre les langues au moyen d'outils informatiques.\n",
    " Le TALN est sorti des laboratoires de recherche pour être progressivement mis en œuvre dans des applications informatiques nécessitant\n",
    " l'intégration du langage humain à la machine. Aussi le TALN est-il parfois appelé ingénierie linguistique.\n",
    " En France, le traitement automatique de la langue naturelle a sa revue, Traitement automatique des langues,\n",
    " publiée par l’Association pour le traitement automatique des langues (ATALA).\"\"\"\n",
    "\n",
    "documentDataScience = \"\"\"En termes généraux, la science des données est l'extraction de connaissance d'ensembles de données.\n",
    " Elle emploie des techniques et des théories tirées de plusieurs autres domaines plus larges des mathématiques, analyse,\n",
    " optimisation et statistique principalement, la théorie de l'information et la technologie de l'information, notamment le traitement de signal,\n",
    " des modèles probabilistes, l'apprentissage automatique, l'apprentissage statistique, la programmation informatique, l'ingénierie de données,\n",
    " la reconnaissance de formes et l'apprentissage, la visualisation, l'analytique prophétique, la modélisation d'incertitude, le stockage de données,\n",
    " la géo-visualisation, la compression de données et le calcul à haute performance.\n",
    " Les méthodes qui s'adaptent aux données de masse sont particulièrement intéressantes dans la science des données,\n",
    " bien que la discipline ne soit généralement pas considérée comme limitée à ces données.\n",
    "\n",
    "La science des données (en anglais data science) est une discipline qui s'appuie sur des outils mathématiques, de statistiques,\n",
    " d'informatique (cette science est principalement une « science des données numériques ») et de visualisation des données.\n",
    " Elle est en plein développement, dans le monde universitaire ainsi que dans le secteur privé et le secteur public.\n",
    " Moore en 1991 a défini la statistique comme la science des données6 (définition reprise par d'autres dont James T. McClave et al. en 1997)\n",
    " et U. Beck en 2001 oppose la science des données à la science de l'expérience, voyant une dissociation croissante entre ces deux types de science,\n",
    " que tendrait selon lui à encourager une société de la gestion du risque au sein d'une « civilisation du danger ».\"\"\"\n",
    "\n",
    "documentEconometrie = \"\"\"L'économétrie est une branche de la science économique qui a pour objectif d'estimer et de tester les modèles économiques.\n",
    " L'économétrie en tant que discipline naît dans les années 1930 avec la création de la société d'économétrie par Irving Fisher et Ragnar Frisch (1930)\n",
    " et la création de la revue Econometrica (1933). Depuis lors, l'économétrie n'a cessé de se développer et de prendre une importance croissante\n",
    " au sein de la science économique.\n",
    "\n",
    "L'économétrie théorique se focalise essentiellement sur deux questions, l'identification et l'estimation statistique. L'économétrie appliquée\n",
    " utilise les méthodes économétriques pour comprendre des domaines de l'économie comme l'analyse du marché du travail,\n",
    " l'économie de l'éducation ou encore tester la pertinence empirique des modèles de croissance.\n",
    "\n",
    "L'économétrie appliquée utilise aussi bien des données issues d'un protocole expérimental, que ce soit une expérience de laboratoire\n",
    " ou une expérience de terrain, que des données issues directement de l'observation du réel sans manipulation du chercheur.\n",
    " Lorsque l'économètre utilise des données issues directement de l'observation du réel, il est fréquent d'identifier des expériences naturelles\n",
    " pour retrouver une situation quasi-expérimentale. On parle parfois de révolution de crédibilité, terme controversé, pour désigner l'essor fulgurant\n",
    " de ces méthodes de recherche dans la discipline, et en économie en général.\"\"\"\n",
    "\n",
    "documentHistoire = \"\"\"L’histoire, souvent écrit avec la première lettre majuscule,\n",
    " est à la fois l’étude et l'écriture des faits et des événements passés quelles que soient leur variété et leur complexité.\n",
    " L'histoire est également une science humaine et sociale. On désigne aussi couramment sous le terme d’histoire (par synecdoque) le passé lui-même,\n",
    " comme dans les leçons de l'histoire. L'histoire est un récit écrit par lequel des hommes et des femmes (les historiens et historiennes)\n",
    " s'efforcent de faire connaître les temps révolus. Ces tentatives ne sont jamais entièrement indépendantes de conditionnements étrangers au domaine\n",
    " telle que la vision du monde de leur auteur ou de sa culture, mais elles sont censées être élaborées à partir de sources plutôt que guidées\n",
    " par la spéculation ou l'idéologie.\n",
    "\n",
    "Au cours des siècles, les historiens ont façonné leurs méthodes ainsi que les champs d'intervention, tout en réévaluant leurs sources,\n",
    " leur origine et leur exploitation. La discipline universitaire d'étude et écriture de l'histoire, y comprise la critique des méthodes,\n",
    " est l'historiographie. Elle s'appuie sur diverses sciences auxiliaires complétant selon les travaux menés la compétence générale de l'historien.\n",
    " Elle reste malgré tout une construction humaine, inévitablement inscrite dans son époque, susceptible d'être utilisée en dehors de son domaine,\n",
    " notamment à des fins d'ordre politique. \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "UH2LKmshYkDM"
   },
   "outputs": [],
   "source": [
    "bagOfWordsNLP = documentNLP.split(' ')\n",
    "bagOfWordsDataScience = documentDataScience.split(' ')\n",
    "bagOfWordsEconometrie = documentEconometrie.split(' ')\n",
    "bagOfWordsHistoire = documentHistoire.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "DUmZrE2lYk42"
   },
   "outputs": [],
   "source": [
    "uniqueWords = set(bagOfWordsNLP).union(set(bagOfWordsDataScience)).union(set(bagOfWordsEconometrie)).union(set(bagOfWordsHistoire))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "U2k-oq2MYlFy"
   },
   "outputs": [],
   "source": [
    "numOfWordsNLP = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsNLP:\n",
    "    numOfWordsNLP[word] += 1\n",
    "\n",
    "numOfWordsDataScience = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsDataScience:\n",
    "    numOfWordsDataScience[word] += 1\n",
    "\n",
    "numOfWordsEconometrie = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsEconometrie:\n",
    "    numOfWordsEconometrie[word] += 1\n",
    "\n",
    "numOfWordsHistoire = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsHistoire:\n",
    "    numOfWordsHistoire[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "hidden": true,
    "id": "6xYX613WYlQz",
    "outputId": "58f8359f-a627-4a3a-c11b-3b2fd8e8759d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['au',\n",
       " 'aux',\n",
       " 'avec',\n",
       " 'ce',\n",
       " 'ces',\n",
       " 'dans',\n",
       " 'de',\n",
       " 'des',\n",
       " 'du',\n",
       " 'elle',\n",
       " 'en',\n",
       " 'et',\n",
       " 'eux',\n",
       " 'il',\n",
       " 'ils',\n",
       " 'je',\n",
       " 'la',\n",
       " 'le',\n",
       " 'les',\n",
       " 'leur',\n",
       " 'lui',\n",
       " 'ma',\n",
       " 'mais',\n",
       " 'me',\n",
       " 'même',\n",
       " 'mes',\n",
       " 'moi',\n",
       " 'mon',\n",
       " 'ne',\n",
       " 'nos',\n",
       " 'notre',\n",
       " 'nous',\n",
       " 'on',\n",
       " 'ou',\n",
       " 'par',\n",
       " 'pas',\n",
       " 'pour',\n",
       " 'qu',\n",
       " 'que',\n",
       " 'qui',\n",
       " 'sa',\n",
       " 'se',\n",
       " 'ses',\n",
       " 'son',\n",
       " 'sur',\n",
       " 'ta',\n",
       " 'te',\n",
       " 'tes',\n",
       " 'toi',\n",
       " 'ton',\n",
       " 'tu',\n",
       " 'un',\n",
       " 'une',\n",
       " 'vos',\n",
       " 'votre',\n",
       " 'vous',\n",
       " 'c',\n",
       " 'd',\n",
       " 'j',\n",
       " 'l',\n",
       " 'à',\n",
       " 'm',\n",
       " 'n',\n",
       " 's',\n",
       " 't',\n",
       " 'y',\n",
       " 'été',\n",
       " 'étée',\n",
       " 'étées',\n",
       " 'étés',\n",
       " 'étant',\n",
       " 'étante',\n",
       " 'étants',\n",
       " 'étantes',\n",
       " 'suis',\n",
       " 'es',\n",
       " 'est',\n",
       " 'sommes',\n",
       " 'êtes',\n",
       " 'sont',\n",
       " 'serai',\n",
       " 'seras',\n",
       " 'sera',\n",
       " 'serons',\n",
       " 'serez',\n",
       " 'seront',\n",
       " 'serais',\n",
       " 'serait',\n",
       " 'serions',\n",
       " 'seriez',\n",
       " 'seraient',\n",
       " 'étais',\n",
       " 'était',\n",
       " 'étions',\n",
       " 'étiez',\n",
       " 'étaient',\n",
       " 'fus',\n",
       " 'fut',\n",
       " 'fûmes',\n",
       " 'fûtes',\n",
       " 'furent',\n",
       " 'sois',\n",
       " 'soit',\n",
       " 'soyons',\n",
       " 'soyez',\n",
       " 'soient',\n",
       " 'fusse',\n",
       " 'fusses',\n",
       " 'fût',\n",
       " 'fussions',\n",
       " 'fussiez',\n",
       " 'fussent',\n",
       " 'ayant',\n",
       " 'ayante',\n",
       " 'ayantes',\n",
       " 'ayants',\n",
       " 'eu',\n",
       " 'eue',\n",
       " 'eues',\n",
       " 'eus',\n",
       " 'ai',\n",
       " 'as',\n",
       " 'avons',\n",
       " 'avez',\n",
       " 'ont',\n",
       " 'aurai',\n",
       " 'auras',\n",
       " 'aura',\n",
       " 'aurons',\n",
       " 'aurez',\n",
       " 'auront',\n",
       " 'aurais',\n",
       " 'aurait',\n",
       " 'aurions',\n",
       " 'auriez',\n",
       " 'auraient',\n",
       " 'avais',\n",
       " 'avait',\n",
       " 'avions',\n",
       " 'aviez',\n",
       " 'avaient',\n",
       " 'eut',\n",
       " 'eûmes',\n",
       " 'eûtes',\n",
       " 'eurent',\n",
       " 'aie',\n",
       " 'aies',\n",
       " 'ait',\n",
       " 'ayons',\n",
       " 'ayez',\n",
       " 'aient',\n",
       " 'eusse',\n",
       " 'eusses',\n",
       " 'eût',\n",
       " 'eussions',\n",
       " 'eussiez',\n",
       " 'eussent']"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "stopwords.words('french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "U4UZsBqBYlac"
   },
   "outputs": [],
   "source": [
    "def computeTF(wordDict, bagOfWords):\n",
    "    tfDict = {}\n",
    "    bagOfWordsCount = len(bagOfWords)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count / float(bagOfWordsCount)\n",
    "    return tfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "1eGebIApYlje"
   },
   "outputs": [],
   "source": [
    "tfNLP = computeTF(numOfWordsNLP, bagOfWordsNLP)\n",
    "tfDataScience = computeTF(numOfWordsDataScience, bagOfWordsDataScience)\n",
    "tfEconometrie = computeTF(numOfWordsEconometrie, bagOfWordsEconometrie)\n",
    "tfHistoire = computeTF(numOfWordsHistoire, bagOfWordsHistoire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "yw240CWgYlsr"
   },
   "outputs": [],
   "source": [
    "def computeIDF(documents):\n",
    "    import math\n",
    "    N = len(documents)\n",
    "    \n",
    "    idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
    "    for document in documents:\n",
    "        for word, val in document.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log(N / float(val))\n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "C7_ejnuOZWez"
   },
   "outputs": [],
   "source": [
    "idfs = computeIDF([numOfWordsNLP, numOfWordsDataScience, numOfWordsEconometrie, numOfWordsHistoire])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "vW2xwamlZWmq"
   },
   "outputs": [],
   "source": [
    "def computeTFIDF(tfBagOfWords, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBagOfWords.items():\n",
    "        tfidf[word] = val * idfs[word]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "TB2O-vKAZWtm"
   },
   "outputs": [],
   "source": [
    "tfidfNLP = computeTFIDF(tfNLP, idfs)\n",
    "tfidfDataScience = computeTFIDF(tfDataScience, idfs)\n",
    "tfidfEconometrie = computeTFIDF(tfEconometrie, idfs)\n",
    "tfidfHistoire = computeTFIDF(tfHistoire, idfs)\n",
    "df = pd.DataFrame([tfidfNLP, tfidfDataScience, tfidfEconometrie])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "hidden": true,
    "id": "4KiDyXbubBF1",
    "outputId": "fd50a624-43ce-4992-eea3-1a4413fefdf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   (définition   l'essor  traitement  ...  femmes  l'expérience,       les\n",
      "0     0.000000  0.000000    0.031991  ...     0.0       0.000000  0.002213\n",
      "1     0.005658  0.000000    0.002829  ...     0.0       0.005658  0.000000\n",
      "2     0.000000  0.007037    0.000000  ...     0.0       0.000000  0.004381\n",
      "\n",
      "[3 rows x 413 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "oLiRFXtzZWzE"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform([documentNLP, documentDataScience, documentEconometrie, documentHistoire])\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "df = pd.DataFrame(denselist, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 146
    },
    "colab_type": "code",
    "hidden": true,
    "id": "w6LHAKsYZW37",
    "outputId": "ecc29b42-06ea-417d-c150-54220c6c53b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       1930      1933      1991  ...  événements      être     œuvre\n",
      "0  0.000000  0.000000  0.000000  ...     0.00000  0.106778  0.067717\n",
      "1  0.000000  0.000000  0.043496  ...     0.00000  0.000000  0.000000\n",
      "2  0.104828  0.052414  0.000000  ...     0.00000  0.000000  0.000000\n",
      "3  0.000000  0.000000  0.000000  ...     0.05785  0.091219  0.000000\n",
      "\n",
      "[4 rows x 342 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "colab_type": "code",
    "hidden": true,
    "id": "rmYJ_oGYZW8Y",
    "outputId": "80fef33b-2d6e-4720-dd5a-599b8fe4b739"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1930          0.000000\n",
       "1933          0.000000\n",
       "1991          0.000000\n",
       "1997          0.000000\n",
       "2001          0.000000\n",
       "                ...   \n",
       "étrangers     0.000000\n",
       "étude         0.000000\n",
       "événements    0.000000\n",
       "être          0.106778\n",
       "œuvre         0.067717\n",
       "Name: 0, Length: 342, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "3o4KAtR6ZXAs"
   },
   "outputs": [],
   "source": [
    "dist_NLP_DataScience = np.linalg.norm(df.loc[0] - df.loc[1])\n",
    "dist_NLP_Econometrie = np.linalg.norm(df.loc[0] - df.loc[2])\n",
    "dist_NLP_Histoire = np.linalg.norm(df.loc[0] - df.loc[3])\n",
    "dist_DataScience_Econometrie = np.linalg.norm(df.loc[1] - df.loc[2])\n",
    "dist_DataScience_Histoire = np.linalg.norm(df.loc[1] - df.loc[3])\n",
    "dist_Econometrie_Histoire = np.linalg.norm(df.loc[2] - df.loc[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "hidden": true,
    "id": "qVQzDI2EZXE3",
    "outputId": "1afd2505-42f4-406f-f164-1ab7ed9d154c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1673312267944298\n",
      "1.2190626386442045\n",
      "1.2430371781718232\n",
      "0.9948497561633074\n",
      "1.1000016429145862\n",
      "1.144129897552517\n"
     ]
    }
   ],
   "source": [
    "print(dist_NLP_DataScience)\n",
    "print(dist_NLP_Econometrie)\n",
    "print(dist_NLP_Histoire)\n",
    "print(dist_DataScience_Econometrie)\n",
    "print(dist_DataScience_Histoire)\n",
    "print(dist_Econometrie_Histoire)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Embeddings de mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "WMGgjIn2k1Ig"
   },
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "hidden": true,
    "id": "dww0XB9-ZXXm",
    "outputId": "874b8ab1-f33b-4106-868d-e19000746fea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "import gensim \n",
    "from gensim.models import Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "xYaB2OBaZXbG"
   },
   "outputs": [],
   "source": [
    "texte1 = \"\"\"L’exploration de données, connue aussi sous l'expression de fouille de données, forage de données, prospection de données, data mining,\n",
    " ou encore extraction de connaissances à partir de données, a pour objet l’extraction d'un savoir ou d'une connaissance à partir de grandes quantités\n",
    " de données, par des méthodes automatiques ou semi-automatiques.\n",
    "Elle se propose d'utiliser un ensemble d'algorithmes issus de disciplines scientifiques diverses telles que les statistiques,\n",
    " l'intelligence artificielle ou l'informatique, pour construire des modèles à partir des données,\n",
    " c'est-à-dire trouver des structures intéressantes ou des motifs selon des critères fixés au préalable,\n",
    " et d'en extraire un maximum de connaissances.\n",
    "L'utilisation industrielle ou opérationnelle de ce savoir dans le monde professionnel permet de résoudre des problèmes très divers,\n",
    " allant de la gestion de la relation client à la maintenance préventive, en passant par la détection de fraudes ou encore l'optimisation de sites web.\n",
    "C'est aussi le mode de travail du journalisme de données.\n",
    "L'exploration de données fait suite, dans l'escalade de l'exploitation des données de l'entreprise, à l'informatique décisionnelle.\n",
    "Celle-ci permet de constater un fait, tel que le chiffre d'affaires, et de l'expliquer comme le chiffre d'affaires décliné par produits,\n",
    " tandis que l'exploration de données permet de classer les faits et de les prévoir dans une certaine mesure ou encore de les éclairer en révélant\n",
    " par exemple les variables ou paramètres qui pourraient faire comprendre pourquoi le chiffre d'affaires de tel point de vente est supérieur\n",
    " à celui de tel autre. \"\"\"\n",
    "\n",
    "texte2 = \"\"\"En statistique, les analyses multivariées ont pour caractéristique de s'intéresser à des lois de probabilité à plusieurs variables.\n",
    "Les analyses bivariées sont des cas particuliers à deux variables.\n",
    "Les analyses multivariées sont très diverses selon l'objectif recherché, la nature des variables et la mise en œuvre formelle.\n",
    "On peut identifier deux grandes familles : celle des méthodes descriptives (visant à structurer et résumer l'information)\n",
    " et celle des méthodes explicatives visant à expliquer une ou des variables dites « dépendantes » (variables à expliquer) par un ensemble de variables\n",
    " dites « indépendantes » (variables explicatives).\n",
    "Les méthodes appelées en français analyse des données en sont un sous-ensemble. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "Xd-JukqaZXeg"
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "# iterate through each sentence in the file \n",
    "for i in sent_tokenize(texte1 + texte2): \n",
    "    temp = [] \n",
    "      \n",
    "    # tokenize the sentence into words \n",
    "    for j in word_tokenize(i): \n",
    "        temp.append(j.lower()) \n",
    "  \n",
    "    data.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "hidden": true,
    "id": "hH25xm_jZXh7",
    "outputId": "0390a0fc-3444-4145-d253-d3ed89a12722"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['l', '’', 'exploration', 'de', 'données', ',', 'connue', 'aussi', 'sous', \"l'expression\", 'de', 'fouille', 'de', 'données', ',', 'forage', 'de', 'données', ',', 'prospection', 'de', 'données', ',', 'data', 'mining', ',', 'ou', 'encore', 'extraction', 'de', 'connaissances', 'à', 'partir', 'de', 'données', ',', 'a', 'pour', 'objet', 'l', '’', 'extraction', \"d'un\", 'savoir', 'ou', \"d'une\", 'connaissance', 'à', 'partir', 'de', 'grandes', 'quantités', 'de', 'données', ',', 'par', 'des', 'méthodes', 'automatiques', 'ou', 'semi-automatiques', '.'], ['elle', 'se', 'propose', \"d'utiliser\", 'un', 'ensemble', \"d'algorithmes\", 'issus', 'de', 'disciplines', 'scientifiques', 'diverses', 'telles', 'que', 'les', 'statistiques', ',', \"l'intelligence\", 'artificielle', 'ou', \"l'informatique\", ',', 'pour', 'construire', 'des', 'modèles', 'à', 'partir', 'des', 'données', ',', \"c'est-à-dire\", 'trouver', 'des', 'structures', 'intéressantes', 'ou', 'des', 'motifs', 'selon', 'des', 'critères', 'fixés', 'au', 'préalable', ',', 'et', \"d'en\", 'extraire', 'un', 'maximum', 'de', 'connaissances', '.'], [\"l'utilisation\", 'industrielle', 'ou', 'opérationnelle', 'de', 'ce', 'savoir', 'dans', 'le', 'monde', 'professionnel', 'permet', 'de', 'résoudre', 'des', 'problèmes', 'très', 'divers', ',', 'allant', 'de', 'la', 'gestion', 'de', 'la', 'relation', 'client', 'à', 'la', 'maintenance', 'préventive', ',', 'en', 'passant', 'par', 'la', 'détection', 'de', 'fraudes', 'ou', 'encore', \"l'optimisation\", 'de', 'sites', 'web', '.'], [\"c'est\", 'aussi', 'le', 'mode', 'de', 'travail', 'du', 'journalisme', 'de', 'données', '.'], [\"l'exploration\", 'de', 'données', 'fait', 'suite', ',', 'dans', \"l'escalade\", 'de', \"l'exploitation\", 'des', 'données', 'de', \"l'entreprise\", ',', 'à', \"l'informatique\", 'décisionnelle', '.'], ['celle-ci', 'permet', 'de', 'constater', 'un', 'fait', ',', 'tel', 'que', 'le', 'chiffre', \"d'affaires\", ',', 'et', 'de', \"l'expliquer\", 'comme', 'le', 'chiffre', \"d'affaires\", 'décliné', 'par', 'produits', ',', 'tandis', 'que', \"l'exploration\", 'de', 'données', 'permet', 'de', 'classer', 'les', 'faits', 'et', 'de', 'les', 'prévoir', 'dans', 'une', 'certaine', 'mesure', 'ou', 'encore', 'de', 'les', 'éclairer', 'en', 'révélant', 'par', 'exemple', 'les', 'variables', 'ou', 'paramètres', 'qui', 'pourraient', 'faire', 'comprendre', 'pourquoi', 'le', 'chiffre', \"d'affaires\", 'de', 'tel', 'point', 'de', 'vente', 'est', 'supérieur', 'à', 'celui', 'de', 'tel', 'autre', '.'], ['en', 'statistique', ',', 'les', 'analyses', 'multivariées', 'ont', 'pour', 'caractéristique', 'de', \"s'intéresser\", 'à', 'des', 'lois', 'de', 'probabilité', 'à', 'plusieurs', 'variables', '.'], ['les', 'analyses', 'bivariées', 'sont', 'des', 'cas', 'particuliers', 'à', 'deux', 'variables', '.'], ['les', 'analyses', 'multivariées', 'sont', 'très', 'diverses', 'selon', \"l'objectif\", 'recherché', ',', 'la', 'nature', 'des', 'variables', 'et', 'la', 'mise', 'en', 'œuvre', 'formelle', '.'], ['on', 'peut', 'identifier', 'deux', 'grandes', 'familles', ':', 'celle', 'des', 'méthodes', 'descriptives', '(', 'visant', 'à', 'structurer', 'et', 'résumer', \"l'information\", ')', 'et', 'celle', 'des', 'méthodes', 'explicatives', 'visant', 'à', 'expliquer', 'une', 'ou', 'des', 'variables', 'dites', '«', 'dépendantes', '»', '(', 'variables', 'à', 'expliquer', ')', 'par', 'un', 'ensemble', 'de', 'variables', 'dites', '«', 'indépendantes', '»', '(', 'variables', 'explicatives', ')', '.'], ['les', 'méthodes', 'appelées', 'en', 'français', 'analyse', 'des', 'données', 'en', 'sont', 'un', 'sous-ensemble', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "3QFXtLWm6wch"
   },
   "source": [
    "### Continuous bag of words (CBOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "b4tlfQB561cS"
   },
   "source": [
    "Le modèle CBOW prédit le mot courant étant donné les mots de contexte dans une fenêtre autour du mot courant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "hidden": true,
    "id": "A5jZxGw0ZXlm",
    "outputId": "b7263297-242f-4f46-d27f-44ef1ee0cb4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'données' and 'connaissance' - CBOW :  0.03347106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "# Create CBOW model \n",
    "model1 = gensim.models.Word2Vec(data, min_count = 1,  \n",
    "                              size = 100, window = 5,\n",
    "                              sg = 0)\n",
    "\n",
    "# Print results\n",
    "print(\"Cosine similarity between 'données' \" + \n",
    "               \"and 'connaissance' - CBOW : \", \n",
    "    model1.wv.similarity('données', 'connaissance'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "NSBSotcP7gna"
   },
   "source": [
    "### Skip gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "OM3PLzyv7hQb"
   },
   "source": [
    "La méthode \"skip gram\" fait le contraire de ce que fait la méthode \"cbow\" : elle prédit les mots de contexte d'un mot donné."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "hidden": true,
    "id": "6A0xo1WJZXpV",
    "outputId": "bceb6790-3665-45e7-d0dc-aad4d8b55068"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'données' and 'connaissance' - Skip Gram :  0.083709136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "# Create Skip Gram model \n",
    "model2 = gensim.models.Word2Vec(data, min_count = 1,\n",
    "                                size = 100, window = 5,\n",
    "                                sg = 1) \n",
    "\n",
    "# Print results \n",
    "print(\"Cosine similarity between 'données' \" +\n",
    "          \"and 'connaissance' - Skip Gram : \", \n",
    "    model2.wv.similarity('données', 'connaissance')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "SAEF7KdfZXwJ"
   },
   "outputs": [],
   "source": [
    "word_vectors = model1.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "hidden": true,
    "id": "J740GCysZXtC",
    "outputId": "e21bf589-7663-4dc0-b47f-ed097c5b40fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.33718786e-03  1.75855705e-03  9.17808735e-04  2.81643821e-03\n",
      "  1.99199794e-03 -1.61679613e-03 -3.80598963e-03  3.38992360e-03\n",
      " -2.49176705e-03 -2.87178392e-03  1.97654660e-03  3.97887453e-03\n",
      " -1.64196594e-03  2.37140525e-03  1.08697882e-03 -1.63304631e-03\n",
      " -2.12068786e-03 -4.05492214e-03  3.76474462e-03 -4.62627644e-03\n",
      "  4.94506164e-03  4.41957824e-03  1.76363857e-03 -4.63162456e-03\n",
      "  4.29786835e-03  6.06227317e-04  1.44860230e-03 -5.17087989e-03\n",
      "  4.10126522e-04  6.55350625e-04  2.99918070e-03 -1.30605232e-03\n",
      " -2.55716965e-04  8.03350064e-04  3.29441065e-03 -4.73395037e-03\n",
      "  3.17598484e-03 -1.62105437e-03 -2.05177651e-03 -3.10599175e-03\n",
      "  3.82131664e-03 -3.72754061e-04  4.59108036e-03 -3.37163615e-03\n",
      " -1.83012278e-03 -2.08565313e-03  1.97788142e-03 -2.61708372e-03\n",
      " -3.93491471e-03 -3.84520786e-03  7.93123210e-04  1.77513775e-05\n",
      "  2.78521422e-03 -1.52845529e-03  4.06482536e-03  8.67334544e-04\n",
      "  3.24610039e-03 -2.18762521e-04 -1.92727684e-03 -1.06676962e-04\n",
      " -2.79278448e-03 -6.25226879e-04 -1.10585665e-04 -1.74578698e-03\n",
      "  3.56695917e-03 -2.37241783e-03  1.40168914e-03  4.44820756e-03\n",
      "  2.24166480e-03  1.98822143e-03 -3.00399191e-03  2.26636929e-03\n",
      " -1.03494804e-03 -3.55230272e-03 -3.90480272e-03 -3.63350403e-03\n",
      " -2.16393243e-03 -2.61072791e-03 -2.26090429e-03 -5.57457446e-04\n",
      " -1.08598091e-04  4.40519303e-04 -1.97696988e-03  7.47780781e-04\n",
      "  1.89734099e-03 -3.42906197e-03  3.20256944e-03  3.80075118e-03\n",
      "  1.10279152e-03  1.71939877e-03  2.54608080e-04 -4.90187062e-03\n",
      "  1.79714710e-03 -3.79632018e-03  7.61395320e-04  2.75061489e-03\n",
      "  5.58287778e-04  6.24556735e-04 -2.19215150e-03  1.30283704e-03]\n"
     ]
    }
   ],
   "source": [
    "print(word_vectors['données'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "hidden": true,
    "id": "VgLh1oxr-0vv",
    "outputId": "73b027c3-e5e2-40db-8502-81aa10b0c0dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[================================================--] 96.0% 123.0/128.1MB downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# api.info(\"glove-wiki-gigaword-100\")\n",
    "word_vectors = api.load(\"glove-wiki-gigaword-100\")  # load pre-trained word-vectors from gensim-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "hidden": true,
    "id": "j_FlOX5e-19a",
    "outputId": "da64440e-831a-461a-b5fd-4751cbf48be5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen: 0.7699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "result = word_vectors.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "print(\"{}: {:.4f}\".format(*result[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "hidden": true,
    "id": "sszMfjlFjVnp",
    "outputId": "f806f3a7-deb1-45e0-fa94-f0c8f6a8829c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dragon: 0.6694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "result = word_vectors.most_similar(positive=['female', 'lion'], negative=['male'])\n",
    "print(\"{}: {:.4f}\".format(*result[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "colab_type": "code",
    "hidden": true,
    "id": "08wJCNXF-2Xy",
    "outputId": "b1a9652a-bc0f-45fb-9175-e90d5c3c8fed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cereal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "print(word_vectors.doesnt_match(\"breakfast cereal dinner lunch\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "hidden": true,
    "id": "8uWeTuU2-2g0",
    "outputId": "d43065a1-b9b8-4688-9d72-f18e0232d0ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog: 0.8798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "result = word_vectors.similar_by_word(\"cat\")\n",
    "print(\"{}: {:.4f}\".format(*result[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "z0DVkHDjsS4z"
   },
   "source": [
    "### FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "SMRAVtne-2ny"
   },
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "# from gensim.test.utils import common_texts\n",
    "# print(common_texts[0])\n",
    "# ['human', 'interface', 'computer']\n",
    "\n",
    "model = FastText(data, size=100, window=5, min_count=5, workers=4, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257
    },
    "colab_type": "code",
    "hidden": true,
    "id": "oPPHel-8-2ts",
    "outputId": "d113941f-df5e-4442-bb72-e4119241c012"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('des', 0.18751689791679382),\n",
       " (',', 0.09106265753507614),\n",
       " ('la', 0.03603788837790489),\n",
       " ('les', 0.031714729964733124),\n",
       " ('un', 0.020188312977552414),\n",
       " ('en', 0.013541869819164276),\n",
       " ('le', -0.006070766597986221),\n",
       " ('variables', -0.019463203847408295),\n",
       " ('par', -0.029420850798487663),\n",
       " ('et', -0.040456950664520264)]"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"données\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "hidden": true,
    "id": "LHBYQpJJ-2zC",
    "outputId": "73f0aac8-e60d-46cc-a3ae-c545a21ca827"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import fasttext.util\n",
    "\n",
    "fasttext.util.download_model('en', if_exists='ignore')\n",
    "ft = fasttext.load_model('cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "hidden": true,
    "id": "PFt0bOct-27l",
    "outputId": "919e8706-b640-4a92-9d40-3388275c6b5d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.get_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "6RRvkNjD-2_n"
   },
   "outputs": [],
   "source": [
    "# fasttext.util.reduce_model(ft, 100)\n",
    "# ft.get_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "hidden": true,
    "id": "FtvT4wQV-3DT",
    "outputId": "24ed2574-33ce-4ac9-d432-48ef5a909965"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.get_word_vector('hello').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "colab_type": "code",
    "hidden": true,
    "id": "YFUESWkK-3HX",
    "outputId": "63a8eeb5-6365-40bb-899a-30860838ef93"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.6911550760269165, 'hello.'),\n",
       " (0.6733187437057495, 'hellow'),\n",
       " (0.6578026413917542, 'hi'),\n",
       " (0.6480079293251038, 'hello-'),\n",
       " (0.6307998895645142, 'hello.I'),\n",
       " (0.6276512145996094, 'hullo'),\n",
       " (0.6193587779998779, 'hallo'),\n",
       " (0.6185808777809143, 'howdy'),\n",
       " (0.600105881690979, 'hellooooo'),\n",
       " (0.5991216897964478, 'hellos')]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.get_nearest_neighbors('hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Embeddings keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import spacy\n",
    "from keras.preprocessing.text import one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Création d'un corpus d'exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sample_text_1=\"bitty bought a bit of butter\"\n",
    "sample_text_2=\"but the bit of butter was a bit bitter\"\n",
    "sample_text_3=\"so she bought some better butter to make the bitter butter better\"\n",
    "\n",
    "corp=[sample_text_1,sample_text_2,sample_text_3]\n",
    "no_docs=len(corp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Encodage du corpus en one-hot à l'aide de la fonction keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The encoding for document 1  is :  [2, 40, 38, 1, 22, 28]\n",
      "The encoding for document 2  is :  [33, 26, 1, 22, 28, 15, 38, 1, 8]\n",
      "The encoding for document 3  is :  [2, 41, 40, 46, 10, 28, 32, 31, 26, 8, 28, 10]\n"
     ]
    }
   ],
   "source": [
    "vocab_size=50 \n",
    "encod_corp=[]\n",
    "for i,doc in enumerate(corp):\n",
    "    # taille de vocab 50 pour être sur que chaque mot est encodé sur un entier unique.\n",
    "    encod_corp.append(one_hot(doc,50))\n",
    "    print(\"The encoding for document\",i+1,\" is : \",one_hot(doc,50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Padding des documents : la couche d'embedding de keras nécessite des entrées de la même longueur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#!python -m spacy download fr_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# nlp = spacy.load('fr_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\qf721b1n\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (3.4)\n",
      "Requirement already satisfied: six in c:\\users\\qf721b1n\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from nltk) (1.12.0)\n",
      "Requirement already satisfied: singledispatch in c:\\users\\qf721b1n\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from nltk) (3.4.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [WinError 10060] Une\n",
      "[nltk_data]     tentative de connexion a échoué car le parti connecté\n",
      "[nltk_data]     n’a pas répondu convenablement au-delà d’une certaine\n",
      "[nltk_data]     durée ou une connexion établie a échoué car l’hôte de\n",
      "[nltk_data]     connexion n’a pas répondu>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\qf721b1n/nltk_data'\n    - 'C:\\\\Users\\\\qf721b1n\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\qf721b1n\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\qf721b1n\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\qf721b1n\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-d626e88c83f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmaxlen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaxlen\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mmaxlen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m     \"\"\"\n\u001b[1;32m--> 143\u001b[1;33m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m     return [\n\u001b[0;32m    145\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m     \"\"\"\n\u001b[1;32m--> 104\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    866\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 868\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raw'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'nltk'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 993\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    994\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'file'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    995\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    697\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\qf721b1n/nltk_data'\n    - 'C:\\\\Users\\\\qf721b1n\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\qf721b1n\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\qf721b1n\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\qf721b1n\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# length of maximum document. will be nedded whenever create embeddings for the words\n",
    "maxlen = -1\n",
    "for doc in corp:\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    if(maxlen < len(tokens)):\n",
    "        maxlen = len(tokens)\n",
    "print(\"The maximum number of words in any document is : \",maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# now to create embeddings all of our docs need to be of same length. hence we can pad the docs with zeros.\n",
    "pad_corp=pad_sequences(encod_corp,maxlen=maxlen,padding='post',value=0.0)\n",
    "print(\"No of padded documents: \",len(pad_corp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i,doc in enumerate(pad_corp):\n",
    "     print(\"The padded encoding for document\",i+1,\" is : \",doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import CamembertTokenizer, CamembertConfig\n",
    "from transformers import CamembertForTokenClassification\n",
    "from tqdm import tqdm, trange\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentences, labels = [], []\n",
    "with open(\"frwikinews-20130110-pages-articles.txt.tok.stanford-pos\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f.readlines():\n",
    "        sentence = []\n",
    "        sent_tag = []\n",
    "        tokens = line.replace(\"\\n\", \"\").split(\" \")\n",
    "        for token in tokens:\n",
    "            splits = token.split(\"_\")\n",
    "            if len(splits) != 2: continue\n",
    "            word, tag = splits\n",
    "            sentence.append(word)\n",
    "            sent_tag.append(tag)\n",
    "        sentences.append(\" \".join(sentence))\n",
    "        labels.append(sent_tag)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tags_val = list(set().union(*labels))\n",
    "tag2idx = {t:i for i,t in enumerate(tags_val)}\n",
    "# tag2idx[\"<PAD>\"] = len(tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 1733, 156.07599958838796)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = np.array(list(map(len, sentences)))\n",
    "lens.min(), lens.max(), lens.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 150\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tokenizer = CamembertTokenizer.from_pretrained('camembert-base/', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁à', '▁la', '▁suite', '▁de', '▁la', '▁parution', '▁le', '▁matin', '▁même', '▁d', \"'\", '▁un', '▁article', '▁2', '▁=', '▁le', '▁concernant', '▁dans', '▁le', '▁quotidien', '▁libération', '▁', ',', '▁christ', 'oph', 'e', '▁h', 'onde', 'la', 'tte', '▁décide', '▁de', '▁ne', '▁pas', '▁présenter', '▁le', '▁journal', '▁de', '▁13', '▁h', '▁00', '▁de', '▁france', '▁2', '▁', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "print(tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
    "                     maxlen=MAX_LEN, value=tag2idx[\"PONCT\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "attention_masks = [[float(i>0) for i in ii] for ii in input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags, \n",
    "                                                            random_state=2018, test_size=0.1)\n",
    "tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2018, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tr_inputs = torch.tensor(tr_inputs)\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "tr_tags = torch.tensor(tr_tags)\n",
    "val_tags = torch.tensor(val_tags)\n",
    "tr_masks = torch.tensor(tr_masks)\n",
    "val_masks = torch.tensor(val_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base/ were not used when initializing CamembertForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing CamembertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForTokenClassification were not initialized from the model checkpoint at camembert-base/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = CamembertForTokenClassification.from_pretrained(\"camembert-base/\", num_labels=len(tag2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "FULL_FINETUNING = True #True\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters()) \n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    10  of  1,230.    Elapsed: 0:02:31.\n",
      "  Batch    20  of  1,230.    Elapsed: 0:04:46.\n",
      "  Batch    30  of  1,230.    Elapsed: 0:07:00.\n",
      "  Batch    50  of  1,230.    Elapsed: 0:11:32.\n",
      "  Batch    60  of  1,230.    Elapsed: 0:13:45.\n",
      "  Batch    70  of  1,230.    Elapsed: 0:15:57.\n",
      "  Batch    80  of  1,230.    Elapsed: 0:18:15.\n",
      "  Batch    90  of  1,230.    Elapsed: 0:20:32.\n",
      "  Batch   100  of  1,230.    Elapsed: 0:22:43.\n",
      "  Batch   110  of  1,230.    Elapsed: 0:24:55.\n",
      "  Batch   120  of  1,230.    Elapsed: 0:27:06.\n",
      "  Batch   130  of  1,230.    Elapsed: 0:29:17.\n",
      "  Batch   140  of  1,230.    Elapsed: 0:31:28.\n",
      "  Batch   150  of  1,230.    Elapsed: 0:33:39.\n",
      "  Batch   160  of  1,230.    Elapsed: 0:35:50.\n",
      "  Batch   170  of  1,230.    Elapsed: 0:38:02.\n",
      "  Batch   180  of  1,230.    Elapsed: 0:40:14.\n",
      "  Batch   190  of  1,230.    Elapsed: 0:42:25.\n",
      "  Batch   200  of  1,230.    Elapsed: 0:44:36.\n",
      "  Batch   210  of  1,230.    Elapsed: 0:46:47.\n",
      "  Batch   220  of  1,230.    Elapsed: 0:48:59.\n",
      "  Batch   230  of  1,230.    Elapsed: 0:51:09.\n",
      "  Batch   240  of  1,230.    Elapsed: 0:53:20.\n",
      "  Batch   250  of  1,230.    Elapsed: 0:55:30.\n",
      "  Batch   260  of  1,230.    Elapsed: 0:57:42.\n",
      "  Batch   270  of  1,230.    Elapsed: 0:59:54.\n",
      "  Batch   280  of  1,230.    Elapsed: 1:02:06.\n",
      "  Batch   290  of  1,230.    Elapsed: 1:04:18.\n",
      "  Batch   300  of  1,230.    Elapsed: 1:06:29.\n",
      "  Batch   310  of  1,230.    Elapsed: 1:08:40.\n",
      "  Batch   320  of  1,230.    Elapsed: 1:10:51.\n",
      "  Batch   330  of  1,230.    Elapsed: 1:13:02.\n",
      "  Batch   340  of  1,230.    Elapsed: 1:15:13.\n",
      "  Batch   350  of  1,230.    Elapsed: 1:17:24.\n",
      "  Batch   360  of  1,230.    Elapsed: 1:19:37.\n",
      "  Batch   370  of  1,230.    Elapsed: 1:21:48.\n",
      "  Batch   380  of  1,230.    Elapsed: 1:23:59.\n",
      "  Batch   390  of  1,230.    Elapsed: 1:26:09.\n",
      "  Batch   400  of  1,230.    Elapsed: 1:28:19.\n",
      "  Batch   410  of  1,230.    Elapsed: 1:30:30.\n",
      "  Batch   420  of  1,230.    Elapsed: 1:32:41.\n",
      "  Batch   430  of  1,230.    Elapsed: 1:34:52.\n",
      "  Batch   440  of  1,230.    Elapsed: 1:37:04.\n",
      "  Batch   450  of  1,230.    Elapsed: 1:39:16.\n",
      "  Batch   460  of  1,230.    Elapsed: 1:41:28.\n",
      "  Batch   470  of  1,230.    Elapsed: 1:43:39.\n",
      "  Batch   480  of  1,230.    Elapsed: 1:45:52.\n",
      "  Batch   490  of  1,230.    Elapsed: 1:48:04.\n",
      "  Batch   500  of  1,230.    Elapsed: 1:50:16.\n",
      "  Batch   510  of  1,230.    Elapsed: 1:52:28.\n",
      "  Batch   520  of  1,230.    Elapsed: 1:54:42.\n",
      "  Batch   530  of  1,230.    Elapsed: 1:56:54.\n",
      "  Batch   540  of  1,230.    Elapsed: 1:59:06.\n",
      "  Batch   550  of  1,230.    Elapsed: 2:01:18.\n",
      "  Batch   560  of  1,230.    Elapsed: 2:03:30.\n",
      "  Batch   570  of  1,230.    Elapsed: 2:05:42.\n",
      "  Batch   580  of  1,230.    Elapsed: 2:07:55.\n",
      "  Batch   590  of  1,230.    Elapsed: 2:10:08.\n",
      "  Batch   600  of  1,230.    Elapsed: 2:12:21.\n",
      "  Batch   610  of  1,230.    Elapsed: 2:14:34.\n",
      "  Batch   620  of  1,230.    Elapsed: 2:16:46.\n",
      "  Batch   630  of  1,230.    Elapsed: 2:18:58.\n",
      "  Batch   640  of  1,230.    Elapsed: 2:21:10.\n",
      "  Batch   650  of  1,230.    Elapsed: 2:23:22.\n",
      "  Batch   660  of  1,230.    Elapsed: 2:25:34.\n",
      "  Batch   670  of  1,230.    Elapsed: 2:27:46.\n",
      "  Batch   680  of  1,230.    Elapsed: 2:29:58.\n",
      "  Batch   690  of  1,230.    Elapsed: 2:32:10.\n",
      "  Batch   700  of  1,230.    Elapsed: 2:34:22.\n",
      "  Batch   710  of  1,230.    Elapsed: 2:36:34.\n",
      "  Batch   720  of  1,230.    Elapsed: 2:38:44.\n",
      "  Batch   730  of  1,230.    Elapsed: 2:40:56.\n",
      "  Batch   740  of  1,230.    Elapsed: 2:43:09.\n",
      "  Batch   750  of  1,230.    Elapsed: 2:45:21.\n",
      "  Batch   760  of  1,230.    Elapsed: 2:47:32.\n",
      "  Batch   770  of  1,230.    Elapsed: 2:49:44.\n",
      "  Batch   780  of  1,230.    Elapsed: 2:51:57.\n",
      "  Batch   790  of  1,230.    Elapsed: 2:54:09.\n",
      "  Batch   800  of  1,230.    Elapsed: 2:56:21.\n",
      "  Batch   810  of  1,230.    Elapsed: 2:58:33.\n",
      "  Batch   820  of  1,230.    Elapsed: 3:00:45.\n",
      "  Batch   830  of  1,230.    Elapsed: 3:02:57.\n",
      "  Batch   840  of  1,230.    Elapsed: 3:05:09.\n",
      "  Batch   850  of  1,230.    Elapsed: 3:07:22.\n",
      "  Batch   860  of  1,230.    Elapsed: 3:09:34.\n",
      "  Batch   870  of  1,230.    Elapsed: 3:11:46.\n",
      "  Batch   880  of  1,230.    Elapsed: 3:13:59.\n",
      "  Batch   890  of  1,230.    Elapsed: 3:16:11.\n",
      "  Batch   900  of  1,230.    Elapsed: 3:18:22.\n",
      "  Batch   910  of  1,230.    Elapsed: 3:20:34.\n",
      "  Batch   920  of  1,230.    Elapsed: 3:22:46.\n",
      "  Batch   930  of  1,230.    Elapsed: 3:24:58.\n",
      "  Batch   940  of  1,230.    Elapsed: 3:27:10.\n",
      "  Batch   950  of  1,230.    Elapsed: 3:29:22.\n",
      "  Batch   960  of  1,230.    Elapsed: 3:31:34.\n",
      "  Batch   970  of  1,230.    Elapsed: 3:33:46.\n",
      "  Batch   980  of  1,230.    Elapsed: 3:35:58.\n",
      "  Batch   990  of  1,230.    Elapsed: 3:38:10.\n",
      "  Batch 1,000  of  1,230.    Elapsed: 3:40:23.\n",
      "  Batch 1,010  of  1,230.    Elapsed: 3:42:35.\n",
      "  Batch 1,020  of  1,230.    Elapsed: 3:44:47.\n",
      "  Batch 1,030  of  1,230.    Elapsed: 3:47:00.\n",
      "  Batch 1,040  of  1,230.    Elapsed: 3:49:12.\n",
      "  Batch 1,050  of  1,230.    Elapsed: 3:51:24.\n",
      "  Batch 1,060  of  1,230.    Elapsed: 3:53:36.\n",
      "  Batch 1,070  of  1,230.    Elapsed: 3:55:49.\n",
      "  Batch 1,080  of  1,230.    Elapsed: 3:58:01.\n",
      "  Batch 1,090  of  1,230.    Elapsed: 4:00:13.\n",
      "  Batch 1,100  of  1,230.    Elapsed: 4:02:26.\n",
      "  Batch 1,110  of  1,230.    Elapsed: 4:04:39.\n",
      "  Batch 1,120  of  1,230.    Elapsed: 4:06:52.\n",
      "  Batch 1,130  of  1,230.    Elapsed: 4:09:04.\n",
      "  Batch 1,140  of  1,230.    Elapsed: 4:11:17.\n",
      "  Batch 1,150  of  1,230.    Elapsed: 4:13:29.\n",
      "  Batch 1,160  of  1,230.    Elapsed: 4:15:44.\n",
      "  Batch 1,170  of  1,230.    Elapsed: 4:17:57.\n",
      "  Batch 1,180  of  1,230.    Elapsed: 4:20:09.\n",
      "  Batch 1,190  of  1,230.    Elapsed: 4:22:22.\n",
      "  Batch 1,200  of  1,230.    Elapsed: 4:24:34.\n",
      "  Batch 1,210  of  1,230.    Elapsed: 4:26:47.\n",
      "  Batch 1,220  of  1,230.    Elapsed: 4:28:59.\n",
      "Train loss: 1.5070774912834168\n",
      "Validation loss: 0.7504648927354465\n",
      "Validation Accuracy: 0.20958368698013924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  10%|█         | 1/10 [4:46:16<42:56:31, 17176.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.6678528399311532\n",
      "  Batch    10  of  1,230.    Elapsed: 0:02:13.\n",
      "  Batch    20  of  1,230.    Elapsed: 0:04:26.\n",
      "  Batch    30  of  1,230.    Elapsed: 0:06:39.\n",
      "  Batch    40  of  1,230.    Elapsed: 0:08:51.\n",
      "  Batch    50  of  1,230.    Elapsed: 0:11:03.\n",
      "  Batch    60  of  1,230.    Elapsed: 0:13:16.\n",
      "  Batch    70  of  1,230.    Elapsed: 0:15:26.\n",
      "  Batch    80  of  1,230.    Elapsed: 0:17:39.\n",
      "  Batch    90  of  1,230.    Elapsed: 0:19:50.\n",
      "  Batch   100  of  1,230.    Elapsed: 0:22:02.\n",
      "  Batch   110  of  1,230.    Elapsed: 0:24:14.\n",
      "  Batch   120  of  1,230.    Elapsed: 0:26:26.\n",
      "  Batch   130  of  1,230.    Elapsed: 0:28:38.\n",
      "  Batch   140  of  1,230.    Elapsed: 0:30:50.\n",
      "  Batch   150  of  1,230.    Elapsed: 0:33:01.\n",
      "  Batch   160  of  1,230.    Elapsed: 0:35:13.\n",
      "  Batch   170  of  1,230.    Elapsed: 0:37:25.\n",
      "  Batch   180  of  1,230.    Elapsed: 0:39:37.\n",
      "  Batch   190  of  1,230.    Elapsed: 0:41:50.\n",
      "  Batch   200  of  1,230.    Elapsed: 0:44:02.\n",
      "  Batch   210  of  1,230.    Elapsed: 0:46:15.\n",
      "  Batch   220  of  1,230.    Elapsed: 0:48:26.\n",
      "  Batch   230  of  1,230.    Elapsed: 0:50:38.\n",
      "  Batch   240  of  1,230.    Elapsed: 0:52:50.\n",
      "  Batch   250  of  1,230.    Elapsed: 0:55:02.\n",
      "  Batch   260  of  1,230.    Elapsed: 0:57:15.\n",
      "  Batch   270  of  1,230.    Elapsed: 0:59:27.\n",
      "  Batch   280  of  1,230.    Elapsed: 1:01:40.\n",
      "  Batch   290  of  1,230.    Elapsed: 1:03:51.\n",
      "  Batch   300  of  1,230.    Elapsed: 1:06:02.\n",
      "  Batch   310  of  1,230.    Elapsed: 1:08:14.\n",
      "  Batch   320  of  1,230.    Elapsed: 1:10:26.\n",
      "  Batch   330  of  1,230.    Elapsed: 1:12:38.\n",
      "  Batch   340  of  1,230.    Elapsed: 1:14:49.\n",
      "  Batch   350  of  1,230.    Elapsed: 1:17:02.\n",
      "  Batch   360  of  1,230.    Elapsed: 1:19:14.\n",
      "  Batch   370  of  1,230.    Elapsed: 1:21:26.\n",
      "  Batch   380  of  1,230.    Elapsed: 1:23:38.\n",
      "  Batch   390  of  1,230.    Elapsed: 1:25:50.\n",
      "  Batch   400  of  1,230.    Elapsed: 1:28:02.\n",
      "  Batch   410  of  1,230.    Elapsed: 1:30:14.\n",
      "  Batch   420  of  1,230.    Elapsed: 1:32:26.\n",
      "  Batch   430  of  1,230.    Elapsed: 1:34:38.\n",
      "  Batch   440  of  1,230.    Elapsed: 1:36:50.\n",
      "  Batch   450  of  1,230.    Elapsed: 1:39:02.\n",
      "  Batch   460  of  1,230.    Elapsed: 1:41:13.\n",
      "  Batch   470  of  1,230.    Elapsed: 1:43:25.\n",
      "  Batch   480  of  1,230.    Elapsed: 1:45:36.\n",
      "  Batch   490  of  1,230.    Elapsed: 1:47:52.\n",
      "  Batch   500  of  1,230.    Elapsed: 1:50:03.\n",
      "  Batch   510  of  1,230.    Elapsed: 1:52:14.\n",
      "  Batch   520  of  1,230.    Elapsed: 1:54:25.\n",
      "  Batch   530  of  1,230.    Elapsed: 1:56:37.\n",
      "  Batch   540  of  1,230.    Elapsed: 1:58:49.\n",
      "  Batch   550  of  1,230.    Elapsed: 2:01:00.\n",
      "  Batch   560  of  1,230.    Elapsed: 2:03:12.\n",
      "  Batch   570  of  1,230.    Elapsed: 2:05:25.\n",
      "  Batch   580  of  1,230.    Elapsed: 2:07:36.\n",
      "  Batch   590  of  1,230.    Elapsed: 2:09:49.\n",
      "  Batch   600  of  1,230.    Elapsed: 2:12:00.\n",
      "  Batch   610  of  1,230.    Elapsed: 2:14:13.\n",
      "  Batch   620  of  1,230.    Elapsed: 2:16:23.\n",
      "  Batch   630  of  1,230.    Elapsed: 2:18:35.\n",
      "  Batch   640  of  1,230.    Elapsed: 2:20:48.\n",
      "  Batch   650  of  1,230.    Elapsed: 2:23:00.\n",
      "  Batch   660  of  1,230.    Elapsed: 2:25:12.\n",
      "  Batch   670  of  1,230.    Elapsed: 2:27:23.\n",
      "  Batch   680  of  1,230.    Elapsed: 2:29:36.\n",
      "  Batch   690  of  1,230.    Elapsed: 2:31:49.\n",
      "  Batch   700  of  1,230.    Elapsed: 2:34:01.\n",
      "  Batch   710  of  1,230.    Elapsed: 2:36:11.\n",
      "  Batch   720  of  1,230.    Elapsed: 2:38:23.\n",
      "  Batch   730  of  1,230.    Elapsed: 2:40:35.\n",
      "  Batch   740  of  1,230.    Elapsed: 2:42:46.\n",
      "  Batch   750  of  1,230.    Elapsed: 2:44:56.\n",
      "  Batch   760  of  1,230.    Elapsed: 2:47:07.\n",
      "  Batch   770  of  1,230.    Elapsed: 2:49:19.\n",
      "  Batch   780  of  1,230.    Elapsed: 2:51:30.\n",
      "  Batch   790  of  1,230.    Elapsed: 2:53:41.\n",
      "  Batch   800  of  1,230.    Elapsed: 2:55:53.\n",
      "  Batch   810  of  1,230.    Elapsed: 2:58:04.\n",
      "  Batch   820  of  1,230.    Elapsed: 3:00:16.\n",
      "  Batch   830  of  1,230.    Elapsed: 3:02:28.\n",
      "  Batch   840  of  1,230.    Elapsed: 3:04:39.\n",
      "  Batch   850  of  1,230.    Elapsed: 3:06:51.\n",
      "  Batch   860  of  1,230.    Elapsed: 3:09:04.\n",
      "  Batch   870  of  1,230.    Elapsed: 3:11:15.\n",
      "  Batch   880  of  1,230.    Elapsed: 3:13:25.\n",
      "  Batch   890  of  1,230.    Elapsed: 3:15:37.\n",
      "  Batch   900  of  1,230.    Elapsed: 3:17:48.\n",
      "  Batch   910  of  1,230.    Elapsed: 3:19:59.\n",
      "  Batch   920  of  1,230.    Elapsed: 3:22:11.\n",
      "  Batch   930  of  1,230.    Elapsed: 3:24:23.\n",
      "  Batch   940  of  1,230.    Elapsed: 3:26:34.\n",
      "  Batch   950  of  1,230.    Elapsed: 3:28:44.\n",
      "  Batch   960  of  1,230.    Elapsed: 3:30:56.\n",
      "  Batch   970  of  1,230.    Elapsed: 3:33:08.\n",
      "  Batch   980  of  1,230.    Elapsed: 3:35:19.\n",
      "  Batch   990  of  1,230.    Elapsed: 3:37:31.\n",
      "  Batch 1,000  of  1,230.    Elapsed: 3:39:43.\n",
      "  Batch 1,010  of  1,230.    Elapsed: 3:41:55.\n",
      "  Batch 1,020  of  1,230.    Elapsed: 3:44:06.\n",
      "  Batch 1,030  of  1,230.    Elapsed: 3:46:18.\n",
      "  Batch 1,040  of  1,230.    Elapsed: 3:48:29.\n",
      "  Batch 1,050  of  1,230.    Elapsed: 3:50:40.\n",
      "  Batch 1,060  of  1,230.    Elapsed: 3:52:50.\n",
      "  Batch 1,070  of  1,230.    Elapsed: 3:55:01.\n",
      "  Batch 1,080  of  1,230.    Elapsed: 3:57:13.\n",
      "  Batch 1,090  of  1,230.    Elapsed: 3:59:26.\n",
      "  Batch 1,100  of  1,230.    Elapsed: 4:01:38.\n",
      "  Batch 1,110  of  1,230.    Elapsed: 4:03:51.\n",
      "  Batch 1,120  of  1,230.    Elapsed: 4:06:03.\n",
      "  Batch 1,130  of  1,230.    Elapsed: 4:08:16.\n",
      "  Batch 1,140  of  1,230.    Elapsed: 4:10:30.\n",
      "  Batch 1,150  of  1,230.    Elapsed: 4:12:44.\n",
      "  Batch 1,160  of  1,230.    Elapsed: 4:14:57.\n",
      "  Batch 1,170  of  1,230.    Elapsed: 4:17:10.\n",
      "  Batch 1,180  of  1,230.    Elapsed: 4:19:23.\n",
      "  Batch 1,190  of  1,230.    Elapsed: 4:21:35.\n",
      "  Batch 1,200  of  1,230.    Elapsed: 4:23:49.\n",
      "  Batch 1,210  of  1,230.    Elapsed: 4:26:02.\n",
      "  Batch 1,220  of  1,230.    Elapsed: 4:28:15.\n",
      "Train loss: 0.6372126102689805\n",
      "Validation loss: 0.7680652433068212\n",
      "Validation Accuracy: 0.19712785746619133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  20%|██        | 2/10 [9:32:01<38:08:58, 17167.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.6456441531748572\n",
      "  Batch    10  of  1,230.    Elapsed: 0:02:14.\n",
      "  Batch    20  of  1,230.    Elapsed: 0:04:27.\n",
      "  Batch    30  of  1,230.    Elapsed: 0:06:40.\n",
      "  Batch    40  of  1,230.    Elapsed: 0:08:54.\n",
      "  Batch    50  of  1,230.    Elapsed: 0:11:07.\n",
      "  Batch    60  of  1,230.    Elapsed: 0:13:22.\n",
      "  Batch    70  of  1,230.    Elapsed: 0:15:37.\n",
      "  Batch    80  of  1,230.    Elapsed: 0:17:51.\n",
      "  Batch    90  of  1,230.    Elapsed: 0:20:06.\n",
      "  Batch   100  of  1,230.    Elapsed: 0:22:21.\n",
      "  Batch   110  of  1,230.    Elapsed: 0:24:37.\n",
      "  Batch   120  of  1,230.    Elapsed: 0:26:50.\n",
      "  Batch   130  of  1,230.    Elapsed: 0:29:03.\n",
      "  Batch   140  of  1,230.    Elapsed: 0:31:15.\n",
      "  Batch   150  of  1,230.    Elapsed: 0:33:28.\n",
      "  Batch   160  of  1,230.    Elapsed: 0:35:41.\n",
      "  Batch   170  of  1,230.    Elapsed: 0:37:53.\n",
      "  Batch   180  of  1,230.    Elapsed: 0:40:07.\n",
      "  Batch   190  of  1,230.    Elapsed: 0:42:21.\n",
      "  Batch   200  of  1,230.    Elapsed: 0:44:35.\n",
      "  Batch   210  of  1,230.    Elapsed: 0:46:50.\n",
      "  Batch   220  of  1,230.    Elapsed: 0:49:04.\n",
      "  Batch   230  of  1,230.    Elapsed: 0:51:18.\n",
      "  Batch   240  of  1,230.    Elapsed: 0:53:31.\n",
      "  Batch   250  of  1,230.    Elapsed: 0:55:44.\n",
      "  Batch   260  of  1,230.    Elapsed: 0:57:58.\n",
      "  Batch   270  of  1,230.    Elapsed: 1:00:12.\n",
      "  Batch   280  of  1,230.    Elapsed: 1:02:25.\n",
      "  Batch   290  of  1,230.    Elapsed: 1:04:39.\n",
      "  Batch   300  of  1,230.    Elapsed: 1:06:53.\n",
      "  Batch   310  of  1,230.    Elapsed: 1:09:07.\n",
      "  Batch   320  of  1,230.    Elapsed: 1:11:21.\n",
      "  Batch   330  of  1,230.    Elapsed: 1:13:35.\n",
      "  Batch   340  of  1,230.    Elapsed: 1:15:49.\n",
      "  Batch   350  of  1,230.    Elapsed: 1:18:03.\n",
      "  Batch   360  of  1,230.    Elapsed: 1:20:17.\n",
      "  Batch   370  of  1,230.    Elapsed: 1:22:31.\n",
      "  Batch   380  of  1,230.    Elapsed: 1:24:45.\n",
      "  Batch   390  of  1,230.    Elapsed: 1:26:59.\n",
      "  Batch   400  of  1,230.    Elapsed: 1:29:13.\n",
      "  Batch   410  of  1,230.    Elapsed: 1:31:28.\n",
      "  Batch   420  of  1,230.    Elapsed: 1:33:43.\n",
      "  Batch   430  of  1,230.    Elapsed: 1:35:56.\n",
      "  Batch   440  of  1,230.    Elapsed: 1:38:11.\n",
      "  Batch   450  of  1,230.    Elapsed: 1:40:25.\n",
      "  Batch   460  of  1,230.    Elapsed: 1:42:40.\n",
      "  Batch   470  of  1,230.    Elapsed: 1:44:52.\n",
      "  Batch   480  of  1,230.    Elapsed: 1:47:07.\n",
      "  Batch   490  of  1,230.    Elapsed: 1:49:21.\n",
      "  Batch   500  of  1,230.    Elapsed: 1:51:34.\n",
      "  Batch   510  of  1,230.    Elapsed: 1:53:50.\n",
      "  Batch   520  of  1,230.    Elapsed: 1:56:04.\n",
      "  Batch   530  of  1,230.    Elapsed: 1:58:17.\n",
      "  Batch   540  of  1,230.    Elapsed: 2:00:31.\n",
      "  Batch   550  of  1,230.    Elapsed: 2:02:45.\n",
      "  Batch   560  of  1,230.    Elapsed: 2:04:58.\n",
      "  Batch   570  of  1,230.    Elapsed: 2:07:13.\n",
      "  Batch   580  of  1,230.    Elapsed: 2:09:29.\n",
      "  Batch   590  of  1,230.    Elapsed: 2:11:43.\n",
      "  Batch   600  of  1,230.    Elapsed: 2:13:57.\n",
      "  Batch   610  of  1,230.    Elapsed: 2:16:12.\n",
      "  Batch   620  of  1,230.    Elapsed: 2:18:26.\n",
      "  Batch   630  of  1,230.    Elapsed: 2:20:39.\n",
      "  Batch   640  of  1,230.    Elapsed: 2:22:54.\n",
      "  Batch   650  of  1,230.    Elapsed: 2:25:07.\n",
      "  Batch   660  of  1,230.    Elapsed: 2:27:21.\n",
      "  Batch   670  of  1,230.    Elapsed: 2:29:35.\n",
      "  Batch   680  of  1,230.    Elapsed: 2:31:48.\n",
      "  Batch   690  of  1,230.    Elapsed: 2:34:04.\n",
      "  Batch   700  of  1,230.    Elapsed: 2:36:19.\n",
      "  Batch   710  of  1,230.    Elapsed: 2:38:31.\n",
      "  Batch   720  of  1,230.    Elapsed: 2:40:45.\n",
      "  Batch   730  of  1,230.    Elapsed: 2:42:58.\n",
      "  Batch   740  of  1,230.    Elapsed: 2:45:12.\n",
      "  Batch   750  of  1,230.    Elapsed: 2:47:26.\n",
      "  Batch   760  of  1,230.    Elapsed: 2:49:42.\n",
      "  Batch   770  of  1,230.    Elapsed: 2:51:56.\n",
      "  Batch   780  of  1,230.    Elapsed: 2:54:09.\n",
      "  Batch   790  of  1,230.    Elapsed: 2:56:23.\n",
      "  Batch   800  of  1,230.    Elapsed: 2:58:36.\n",
      "  Batch   810  of  1,230.    Elapsed: 3:00:50.\n",
      "  Batch   820  of  1,230.    Elapsed: 3:03:04.\n",
      "  Batch   830  of  1,230.    Elapsed: 3:05:17.\n",
      "  Batch   840  of  1,230.    Elapsed: 3:07:31.\n",
      "  Batch   850  of  1,230.    Elapsed: 3:09:44.\n",
      "  Batch   860  of  1,230.    Elapsed: 3:11:59.\n",
      "  Batch   870  of  1,230.    Elapsed: 3:14:13.\n",
      "  Batch   880  of  1,230.    Elapsed: 3:16:27.\n",
      "  Batch   890  of  1,230.    Elapsed: 3:18:42.\n",
      "  Batch   900  of  1,230.    Elapsed: 3:20:56.\n",
      "  Batch   910  of  1,230.    Elapsed: 3:23:11.\n",
      "  Batch   920  of  1,230.    Elapsed: 3:25:25.\n",
      "  Batch   930  of  1,230.    Elapsed: 3:27:38.\n",
      "  Batch   940  of  1,230.    Elapsed: 3:29:52.\n",
      "  Batch   950  of  1,230.    Elapsed: 3:32:05.\n",
      "  Batch   960  of  1,230.    Elapsed: 3:34:18.\n",
      "  Batch   970  of  1,230.    Elapsed: 3:36:31.\n",
      "  Batch   980  of  1,230.    Elapsed: 3:38:45.\n",
      "  Batch   990  of  1,230.    Elapsed: 3:41:00.\n",
      "  Batch 1,000  of  1,230.    Elapsed: 3:43:14.\n",
      "  Batch 1,010  of  1,230.    Elapsed: 3:45:27.\n",
      "  Batch 1,020  of  1,230.    Elapsed: 3:47:41.\n",
      "  Batch 1,030  of  1,230.    Elapsed: 3:49:54.\n",
      "  Batch 1,040  of  1,230.    Elapsed: 3:52:09.\n",
      "  Batch 1,050  of  1,230.    Elapsed: 3:54:25.\n",
      "  Batch 1,060  of  1,230.    Elapsed: 3:56:41.\n",
      "  Batch 1,070  of  1,230.    Elapsed: 3:58:57.\n",
      "  Batch 1,080  of  1,230.    Elapsed: 4:01:12.\n",
      "  Batch 1,090  of  1,230.    Elapsed: 4:03:27.\n",
      "  Batch 1,100  of  1,230.    Elapsed: 4:05:41.\n",
      "  Batch 1,110  of  1,230.    Elapsed: 4:07:56.\n",
      "  Batch 1,120  of  1,230.    Elapsed: 4:10:12.\n",
      "  Batch 1,130  of  1,230.    Elapsed: 4:12:25.\n",
      "  Batch 1,140  of  1,230.    Elapsed: 4:14:41.\n",
      "  Batch 1,150  of  1,230.    Elapsed: 4:16:54.\n",
      "  Batch 1,160  of  1,230.    Elapsed: 4:19:10.\n",
      "  Batch 1,170  of  1,230.    Elapsed: 4:21:25.\n",
      "  Batch 1,180  of  1,230.    Elapsed: 4:23:40.\n",
      "  Batch 1,190  of  1,230.    Elapsed: 4:25:53.\n",
      "  Batch 1,200  of  1,230.    Elapsed: 4:28:08.\n",
      "  Batch 1,210  of  1,230.    Elapsed: 4:30:23.\n",
      "  Batch 1,220  of  1,230.    Elapsed: 4:32:37.\n",
      "Train loss: 0.3921777993077185\n",
      "Validation loss: 0.24186624412554025\n",
      "Validation Accuracy: 0.2336084882306343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  30%|███       | 3/10 [14:22:30<33:32:00, 17245.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.8512631076728391\n",
      "  Batch    10  of  1,230.    Elapsed: 0:02:13.\n",
      "  Batch    20  of  1,230.    Elapsed: 0:04:25.\n",
      "  Batch    30  of  1,230.    Elapsed: 0:06:39.\n",
      "  Batch    40  of  1,230.    Elapsed: 0:08:53.\n",
      "  Batch    50  of  1,230.    Elapsed: 0:11:06.\n",
      "  Batch    60  of  1,230.    Elapsed: 0:13:20.\n",
      "  Batch    70  of  1,230.    Elapsed: 0:15:36.\n",
      "  Batch    80  of  1,230.    Elapsed: 0:17:50.\n",
      "  Batch    90  of  1,230.    Elapsed: 0:20:04.\n",
      "  Batch   100  of  1,230.    Elapsed: 0:22:18.\n",
      "  Batch   110  of  1,230.    Elapsed: 0:24:32.\n",
      "  Batch   120  of  1,230.    Elapsed: 0:26:46.\n",
      "  Batch   130  of  1,230.    Elapsed: 0:29:02.\n",
      "  Batch   140  of  1,230.    Elapsed: 0:31:15.\n",
      "  Batch   150  of  1,230.    Elapsed: 0:33:31.\n",
      "  Batch   160  of  1,230.    Elapsed: 0:35:45.\n",
      "  Batch   170  of  1,230.    Elapsed: 0:37:58.\n",
      "  Batch   180  of  1,230.    Elapsed: 0:40:12.\n",
      "  Batch   190  of  1,230.    Elapsed: 0:42:26.\n",
      "  Batch   200  of  1,230.    Elapsed: 0:44:39.\n",
      "  Batch   210  of  1,230.    Elapsed: 0:46:53.\n",
      "  Batch   220  of  1,230.    Elapsed: 0:49:07.\n",
      "  Batch   230  of  1,230.    Elapsed: 0:51:22.\n",
      "  Batch   240  of  1,230.    Elapsed: 0:53:36.\n",
      "  Batch   250  of  1,230.    Elapsed: 0:55:50.\n",
      "  Batch   260  of  1,230.    Elapsed: 0:58:06.\n",
      "  Batch   270  of  1,230.    Elapsed: 1:00:20.\n",
      "  Batch   280  of  1,230.    Elapsed: 1:02:34.\n",
      "  Batch   290  of  1,230.    Elapsed: 1:04:48.\n",
      "  Batch   300  of  1,230.    Elapsed: 1:07:02.\n",
      "  Batch   310  of  1,230.    Elapsed: 1:09:16.\n",
      "  Batch   320  of  1,230.    Elapsed: 1:11:30.\n",
      "  Batch   330  of  1,230.    Elapsed: 1:13:45.\n",
      "  Batch   340  of  1,230.    Elapsed: 1:15:57.\n",
      "  Batch   350  of  1,230.    Elapsed: 1:18:12.\n",
      "  Batch   360  of  1,230.    Elapsed: 1:20:27.\n",
      "  Batch   370  of  1,230.    Elapsed: 1:22:43.\n",
      "  Batch   380  of  1,230.    Elapsed: 1:24:58.\n",
      "  Batch   390  of  1,230.    Elapsed: 1:27:13.\n",
      "  Batch   400  of  1,230.    Elapsed: 1:29:29.\n",
      "  Batch   410  of  1,230.    Elapsed: 1:31:44.\n",
      "  Batch   420  of  1,230.    Elapsed: 1:33:59.\n",
      "  Batch   430  of  1,230.    Elapsed: 1:36:12.\n",
      "  Batch   440  of  1,230.    Elapsed: 1:38:28.\n",
      "  Batch   450  of  1,230.    Elapsed: 1:40:43.\n",
      "  Batch   460  of  1,230.    Elapsed: 1:42:57.\n",
      "  Batch   470  of  1,230.    Elapsed: 1:45:12.\n",
      "  Batch   480  of  1,230.    Elapsed: 1:47:26.\n",
      "  Batch   490  of  1,230.    Elapsed: 1:49:40.\n",
      "  Batch   500  of  1,230.    Elapsed: 1:51:54.\n",
      "  Batch   510  of  1,230.    Elapsed: 1:54:08.\n",
      "  Batch   520  of  1,230.    Elapsed: 1:56:22.\n",
      "  Batch   530  of  1,230.    Elapsed: 1:58:37.\n",
      "  Batch   540  of  1,230.    Elapsed: 2:00:51.\n",
      "  Batch   550  of  1,230.    Elapsed: 2:03:06.\n",
      "  Batch   560  of  1,230.    Elapsed: 2:05:20.\n",
      "  Batch   570  of  1,230.    Elapsed: 2:07:35.\n",
      "  Batch   580  of  1,230.    Elapsed: 2:09:50.\n",
      "  Batch   590  of  1,230.    Elapsed: 2:12:04.\n",
      "  Batch   600  of  1,230.    Elapsed: 2:14:19.\n",
      "  Batch   610  of  1,230.    Elapsed: 2:16:32.\n",
      "  Batch   620  of  1,230.    Elapsed: 2:18:46.\n",
      "  Batch   630  of  1,230.    Elapsed: 2:20:59.\n",
      "  Batch   640  of  1,230.    Elapsed: 2:23:13.\n",
      "  Batch   650  of  1,230.    Elapsed: 2:25:27.\n",
      "  Batch   660  of  1,230.    Elapsed: 2:27:42.\n",
      "  Batch   670  of  1,230.    Elapsed: 2:29:57.\n",
      "  Batch   680  of  1,230.    Elapsed: 2:32:12.\n",
      "  Batch   690  of  1,230.    Elapsed: 2:34:26.\n",
      "  Batch   700  of  1,230.    Elapsed: 2:36:41.\n",
      "  Batch   710  of  1,230.    Elapsed: 2:38:55.\n",
      "  Batch   720  of  1,230.    Elapsed: 2:41:10.\n",
      "  Batch   730  of  1,230.    Elapsed: 2:43:23.\n",
      "  Batch   740  of  1,230.    Elapsed: 2:45:38.\n",
      "  Batch   750  of  1,230.    Elapsed: 2:47:53.\n",
      "  Batch   760  of  1,230.    Elapsed: 2:50:07.\n",
      "  Batch   770  of  1,230.    Elapsed: 2:52:21.\n",
      "  Batch   780  of  1,230.    Elapsed: 2:54:36.\n",
      "  Batch   790  of  1,230.    Elapsed: 2:56:50.\n",
      "  Batch   800  of  1,230.    Elapsed: 2:59:04.\n",
      "  Batch   810  of  1,230.    Elapsed: 3:01:18.\n",
      "  Batch   820  of  1,230.    Elapsed: 3:03:31.\n",
      "  Batch   830  of  1,230.    Elapsed: 3:05:44.\n",
      "  Batch   840  of  1,230.    Elapsed: 3:07:58.\n",
      "  Batch   850  of  1,230.    Elapsed: 3:10:11.\n",
      "  Batch   860  of  1,230.    Elapsed: 3:12:25.\n",
      "  Batch   870  of  1,230.    Elapsed: 3:14:38.\n",
      "  Batch   880  of  1,230.    Elapsed: 3:16:52.\n",
      "  Batch   890  of  1,230.    Elapsed: 3:19:07.\n",
      "  Batch   900  of  1,230.    Elapsed: 3:21:20.\n",
      "  Batch   910  of  1,230.    Elapsed: 3:23:35.\n",
      "  Batch   920  of  1,230.    Elapsed: 3:25:49.\n",
      "  Batch   930  of  1,230.    Elapsed: 3:28:04.\n",
      "  Batch   940  of  1,230.    Elapsed: 3:30:18.\n",
      "  Batch   950  of  1,230.    Elapsed: 3:32:32.\n",
      "  Batch   960  of  1,230.    Elapsed: 3:34:45.\n",
      "  Batch   970  of  1,230.    Elapsed: 3:36:59.\n",
      "  Batch   980  of  1,230.    Elapsed: 3:39:13.\n",
      "  Batch   990  of  1,230.    Elapsed: 3:41:28.\n",
      "  Batch 1,000  of  1,230.    Elapsed: 3:43:43.\n",
      "  Batch 1,010  of  1,230.    Elapsed: 3:45:57.\n",
      "  Batch 1,020  of  1,230.    Elapsed: 3:48:10.\n",
      "  Batch 1,030  of  1,230.    Elapsed: 3:50:25.\n",
      "  Batch 1,040  of  1,230.    Elapsed: 3:52:39.\n",
      "  Batch 1,050  of  1,230.    Elapsed: 3:54:53.\n",
      "  Batch 1,060  of  1,230.    Elapsed: 3:57:07.\n",
      "  Batch 1,070  of  1,230.    Elapsed: 3:59:21.\n",
      "  Batch 1,080  of  1,230.    Elapsed: 4:01:35.\n",
      "  Batch 1,090  of  1,230.    Elapsed: 4:03:49.\n",
      "  Batch 1,100  of  1,230.    Elapsed: 4:06:03.\n",
      "  Batch 1,110  of  1,230.    Elapsed: 4:08:16.\n",
      "  Batch 1,120  of  1,230.    Elapsed: 4:10:30.\n",
      "  Batch 1,130  of  1,230.    Elapsed: 4:12:45.\n",
      "  Batch 1,140  of  1,230.    Elapsed: 4:14:59.\n",
      "  Batch 1,150  of  1,230.    Elapsed: 4:17:13.\n",
      "  Batch 1,160  of  1,230.    Elapsed: 4:19:27.\n",
      "  Batch 1,170  of  1,230.    Elapsed: 4:21:41.\n",
      "  Batch 1,180  of  1,230.    Elapsed: 4:23:56.\n",
      "  Batch 1,190  of  1,230.    Elapsed: 4:26:09.\n",
      "  Batch 1,200  of  1,230.    Elapsed: 4:28:23.\n",
      "  Batch 1,210  of  1,230.    Elapsed: 4:30:38.\n",
      "  Batch 1,220  of  1,230.    Elapsed: 4:32:52.\n",
      "Train loss: 0.2821777369191007\n",
      "Validation loss: 0.21323060010471484\n",
      "Validation Accuracy: 0.23461974127199678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  40%|████      | 4/10 [19:13:06<28:50:16, 17302.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.8555478903830289\n",
      "  Batch    10  of  1,230.    Elapsed: 0:02:14.\n",
      "  Batch    20  of  1,230.    Elapsed: 0:04:29.\n",
      "  Batch    30  of  1,230.    Elapsed: 0:06:44.\n",
      "  Batch    40  of  1,230.    Elapsed: 0:08:58.\n",
      "  Batch    50  of  1,230.    Elapsed: 0:11:13.\n",
      "  Batch    60  of  1,230.    Elapsed: 0:13:25.\n",
      "  Batch    70  of  1,230.    Elapsed: 0:15:39.\n",
      "  Batch    80  of  1,230.    Elapsed: 0:17:55.\n",
      "  Batch    90  of  1,230.    Elapsed: 0:20:09.\n",
      "  Batch   100  of  1,230.    Elapsed: 0:22:24.\n",
      "  Batch   110  of  1,230.    Elapsed: 0:24:39.\n",
      "  Batch   120  of  1,230.    Elapsed: 0:26:55.\n",
      "  Batch   130  of  1,230.    Elapsed: 0:29:09.\n",
      "  Batch   140  of  1,230.    Elapsed: 0:31:23.\n",
      "  Batch   150  of  1,230.    Elapsed: 0:33:36.\n",
      "  Batch   160  of  1,230.    Elapsed: 0:35:50.\n",
      "  Batch   170  of  1,230.    Elapsed: 0:38:05.\n",
      "  Batch   180  of  1,230.    Elapsed: 0:40:20.\n",
      "  Batch   190  of  1,230.    Elapsed: 0:42:34.\n",
      "  Batch   200  of  1,230.    Elapsed: 0:44:48.\n",
      "  Batch   210  of  1,230.    Elapsed: 0:47:02.\n",
      "  Batch   220  of  1,230.    Elapsed: 0:49:17.\n",
      "  Batch   230  of  1,230.    Elapsed: 0:51:32.\n",
      "  Batch   240  of  1,230.    Elapsed: 0:53:45.\n",
      "  Batch   250  of  1,230.    Elapsed: 0:55:59.\n",
      "  Batch   260  of  1,230.    Elapsed: 0:58:14.\n",
      "  Batch   270  of  1,230.    Elapsed: 1:00:29.\n",
      "  Batch   280  of  1,230.    Elapsed: 1:02:42.\n",
      "  Batch   290  of  1,230.    Elapsed: 1:04:56.\n",
      "  Batch   300  of  1,230.    Elapsed: 1:07:11.\n",
      "  Batch   310  of  1,230.    Elapsed: 1:09:26.\n",
      "  Batch   320  of  1,230.    Elapsed: 1:11:41.\n",
      "  Batch   330  of  1,230.    Elapsed: 1:13:54.\n",
      "  Batch   340  of  1,230.    Elapsed: 1:16:08.\n",
      "  Batch   350  of  1,230.    Elapsed: 1:18:23.\n",
      "  Batch   360  of  1,230.    Elapsed: 1:20:37.\n",
      "  Batch   370  of  1,230.    Elapsed: 1:22:53.\n",
      "  Batch   380  of  1,230.    Elapsed: 1:25:08.\n",
      "  Batch   390  of  1,230.    Elapsed: 1:27:22.\n",
      "  Batch   400  of  1,230.    Elapsed: 1:29:37.\n",
      "  Batch   410  of  1,230.    Elapsed: 1:31:52.\n",
      "  Batch   420  of  1,230.    Elapsed: 1:34:06.\n",
      "  Batch   430  of  1,230.    Elapsed: 1:36:20.\n",
      "  Batch   440  of  1,230.    Elapsed: 1:38:35.\n",
      "  Batch   450  of  1,230.    Elapsed: 1:40:49.\n",
      "  Batch   460  of  1,230.    Elapsed: 1:43:04.\n",
      "  Batch   470  of  1,230.    Elapsed: 1:45:19.\n",
      "  Batch   480  of  1,230.    Elapsed: 1:47:32.\n",
      "  Batch   490  of  1,230.    Elapsed: 1:49:46.\n",
      "  Batch   500  of  1,230.    Elapsed: 1:51:59.\n",
      "  Batch   510  of  1,230.    Elapsed: 1:54:14.\n",
      "  Batch   520  of  1,230.    Elapsed: 1:56:28.\n",
      "  Batch   530  of  1,230.    Elapsed: 1:58:43.\n",
      "  Batch   540  of  1,230.    Elapsed: 2:00:58.\n",
      "  Batch   550  of  1,230.    Elapsed: 2:03:12.\n",
      "  Batch   560  of  1,230.    Elapsed: 2:05:27.\n",
      "  Batch   570  of  1,230.    Elapsed: 2:07:41.\n",
      "  Batch   580  of  1,230.    Elapsed: 2:09:55.\n",
      "  Batch   590  of  1,230.    Elapsed: 2:12:10.\n",
      "  Batch   600  of  1,230.    Elapsed: 2:14:24.\n",
      "  Batch   610  of  1,230.    Elapsed: 2:16:38.\n",
      "  Batch   620  of  1,230.    Elapsed: 2:18:52.\n",
      "  Batch   630  of  1,230.    Elapsed: 2:21:07.\n",
      "  Batch   640  of  1,230.    Elapsed: 2:23:18.\n",
      "  Batch   650  of  1,230.    Elapsed: 2:25:32.\n",
      "  Batch   660  of  1,230.    Elapsed: 2:27:45.\n",
      "  Batch   670  of  1,230.    Elapsed: 2:29:58.\n",
      "  Batch   680  of  1,230.    Elapsed: 2:32:13.\n",
      "  Batch   690  of  1,230.    Elapsed: 2:34:27.\n",
      "  Batch   700  of  1,230.    Elapsed: 2:36:41.\n",
      "  Batch   710  of  1,230.    Elapsed: 2:38:55.\n",
      "  Batch   720  of  1,230.    Elapsed: 2:41:10.\n",
      "  Batch   730  of  1,230.    Elapsed: 2:43:24.\n",
      "  Batch   740  of  1,230.    Elapsed: 2:45:38.\n",
      "  Batch   750  of  1,230.    Elapsed: 2:47:52.\n",
      "  Batch   760  of  1,230.    Elapsed: 2:50:06.\n",
      "  Batch   770  of  1,230.    Elapsed: 2:52:20.\n",
      "  Batch   780  of  1,230.    Elapsed: 2:54:35.\n",
      "  Batch   790  of  1,230.    Elapsed: 2:56:48.\n",
      "  Batch   800  of  1,230.    Elapsed: 2:59:02.\n",
      "  Batch   810  of  1,230.    Elapsed: 3:01:16.\n",
      "  Batch   820  of  1,230.    Elapsed: 3:03:31.\n",
      "  Batch   830  of  1,230.    Elapsed: 3:05:45.\n",
      "  Batch   840  of  1,230.    Elapsed: 3:07:58.\n",
      "  Batch   850  of  1,230.    Elapsed: 3:10:12.\n",
      "  Batch   860  of  1,230.    Elapsed: 3:12:26.\n",
      "  Batch   870  of  1,230.    Elapsed: 3:14:39.\n",
      "  Batch   880  of  1,230.    Elapsed: 3:16:53.\n",
      "  Batch   890  of  1,230.    Elapsed: 3:19:09.\n",
      "  Batch   900  of  1,230.    Elapsed: 3:21:23.\n",
      "  Batch   910  of  1,230.    Elapsed: 3:23:38.\n",
      "  Batch   920  of  1,230.    Elapsed: 3:25:51.\n",
      "  Batch   930  of  1,230.    Elapsed: 3:28:06.\n",
      "  Batch   940  of  1,230.    Elapsed: 3:30:21.\n",
      "  Batch   950  of  1,230.    Elapsed: 3:32:36.\n",
      "  Batch   960  of  1,230.    Elapsed: 3:34:51.\n",
      "  Batch   970  of  1,230.    Elapsed: 3:37:07.\n",
      "  Batch   980  of  1,230.    Elapsed: 3:39:21.\n",
      "  Batch   990  of  1,230.    Elapsed: 3:41:34.\n",
      "  Batch 1,000  of  1,230.    Elapsed: 3:43:49.\n",
      "  Batch 1,010  of  1,230.    Elapsed: 3:46:03.\n",
      "  Batch 1,020  of  1,230.    Elapsed: 3:48:18.\n",
      "  Batch 1,030  of  1,230.    Elapsed: 3:50:34.\n",
      "  Batch 1,040  of  1,230.    Elapsed: 3:52:49.\n",
      "  Batch 1,050  of  1,230.    Elapsed: 3:55:05.\n",
      "  Batch 1,060  of  1,230.    Elapsed: 3:57:20.\n",
      "  Batch 1,070  of  1,230.    Elapsed: 3:59:35.\n",
      "  Batch 1,080  of  1,230.    Elapsed: 4:01:50.\n",
      "  Batch 1,090  of  1,230.    Elapsed: 4:04:05.\n",
      "  Batch 1,100  of  1,230.    Elapsed: 4:06:21.\n",
      "  Batch 1,110  of  1,230.    Elapsed: 4:08:37.\n",
      "  Batch 1,120  of  1,230.    Elapsed: 4:10:51.\n",
      "  Batch 1,130  of  1,230.    Elapsed: 4:13:04.\n",
      "  Batch 1,140  of  1,230.    Elapsed: 4:15:18.\n",
      "  Batch 1,150  of  1,230.    Elapsed: 4:17:32.\n",
      "  Batch 1,160  of  1,230.    Elapsed: 4:19:46.\n",
      "  Batch 1,170  of  1,230.    Elapsed: 4:22:01.\n",
      "  Batch 1,180  of  1,230.    Elapsed: 4:24:16.\n",
      "  Batch 1,190  of  1,230.    Elapsed: 4:26:32.\n",
      "  Batch 1,200  of  1,230.    Elapsed: 4:28:46.\n",
      "  Batch 1,210  of  1,230.    Elapsed: 4:31:00.\n",
      "  Batch 1,220  of  1,230.    Elapsed: 4:33:14.\n",
      "Train loss: 0.21357444705880754\n",
      "Validation loss: 0.19522546144732594\n",
      "Validation Accuracy: 0.23548037613874262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 5/10 [24:04:10<24:05:56, 17351.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.8583599313546852\n",
      "  Batch    10  of  1,230.    Elapsed: 0:02:15.\n",
      "  Batch    20  of  1,230.    Elapsed: 0:04:28.\n",
      "  Batch    30  of  1,230.    Elapsed: 0:06:41.\n",
      "  Batch    40  of  1,230.    Elapsed: 0:08:54.\n",
      "  Batch    50  of  1,230.    Elapsed: 0:11:08.\n",
      "  Batch    60  of  1,230.    Elapsed: 0:13:21.\n",
      "  Batch    70  of  1,230.    Elapsed: 0:15:35.\n",
      "  Batch    80  of  1,230.    Elapsed: 0:17:51.\n",
      "  Batch    90  of  1,230.    Elapsed: 0:20:10.\n",
      "  Batch   100  of  1,230.    Elapsed: 0:22:26.\n",
      "  Batch   110  of  1,230.    Elapsed: 0:24:42.\n",
      "  Batch   120  of  1,230.    Elapsed: 0:26:58.\n",
      "  Batch   130  of  1,230.    Elapsed: 0:29:13.\n",
      "  Batch   140  of  1,230.    Elapsed: 0:31:28.\n",
      "  Batch   150  of  1,230.    Elapsed: 0:33:45.\n",
      "  Batch   160  of  1,230.    Elapsed: 0:36:00.\n",
      "  Batch   170  of  1,230.    Elapsed: 0:38:16.\n",
      "  Batch   180  of  1,230.    Elapsed: 0:40:31.\n",
      "  Batch   190  of  1,230.    Elapsed: 0:42:46.\n",
      "  Batch   200  of  1,230.    Elapsed: 0:45:02.\n",
      "  Batch   210  of  1,230.    Elapsed: 0:47:18.\n",
      "  Batch   220  of  1,230.    Elapsed: 0:49:34.\n",
      "  Batch   230  of  1,230.    Elapsed: 0:51:49.\n",
      "  Batch   240  of  1,230.    Elapsed: 0:54:04.\n",
      "  Batch   250  of  1,230.    Elapsed: 0:56:18.\n",
      "  Batch   260  of  1,230.    Elapsed: 0:58:32.\n",
      "  Batch   270  of  1,230.    Elapsed: 1:00:47.\n",
      "  Batch   280  of  1,230.    Elapsed: 1:03:03.\n",
      "  Batch   290  of  1,230.    Elapsed: 1:05:18.\n",
      "  Batch   300  of  1,230.    Elapsed: 1:07:34.\n",
      "  Batch   310  of  1,230.    Elapsed: 1:09:48.\n",
      "  Batch   320  of  1,230.    Elapsed: 1:12:01.\n",
      "  Batch   330  of  1,230.    Elapsed: 1:14:15.\n",
      "  Batch   340  of  1,230.    Elapsed: 1:16:30.\n",
      "  Batch   350  of  1,230.    Elapsed: 1:18:45.\n",
      "  Batch   360  of  1,230.    Elapsed: 1:20:59.\n",
      "  Batch   370  of  1,230.    Elapsed: 1:23:14.\n",
      "  Batch   380  of  1,230.    Elapsed: 1:25:29.\n",
      "  Batch   390  of  1,230.    Elapsed: 1:27:45.\n",
      "  Batch   400  of  1,230.    Elapsed: 1:29:59.\n",
      "  Batch   410  of  1,230.    Elapsed: 1:32:13.\n",
      "  Batch   420  of  1,230.    Elapsed: 1:34:27.\n",
      "  Batch   430  of  1,230.    Elapsed: 1:36:40.\n",
      "  Batch   440  of  1,230.    Elapsed: 1:38:53.\n",
      "  Batch   450  of  1,230.    Elapsed: 1:41:08.\n",
      "  Batch   460  of  1,230.    Elapsed: 1:43:23.\n",
      "  Batch   470  of  1,230.    Elapsed: 1:45:36.\n",
      "  Batch   480  of  1,230.    Elapsed: 1:47:51.\n",
      "  Batch   490  of  1,230.    Elapsed: 1:50:05.\n",
      "  Batch   500  of  1,230.    Elapsed: 1:52:19.\n",
      "  Batch   510  of  1,230.    Elapsed: 1:54:34.\n",
      "  Batch   520  of  1,230.    Elapsed: 1:56:48.\n",
      "  Batch   530  of  1,230.    Elapsed: 1:59:02.\n",
      "  Batch   540  of  1,230.    Elapsed: 2:01:16.\n",
      "  Batch   550  of  1,230.    Elapsed: 2:03:30.\n",
      "  Batch   560  of  1,230.    Elapsed: 2:05:45.\n",
      "  Batch   570  of  1,230.    Elapsed: 2:07:59.\n",
      "  Batch   580  of  1,230.    Elapsed: 2:10:15.\n",
      "  Batch   590  of  1,230.    Elapsed: 2:12:29.\n",
      "  Batch   600  of  1,230.    Elapsed: 2:14:43.\n",
      "  Batch   610  of  1,230.    Elapsed: 2:16:57.\n",
      "  Batch   620  of  1,230.    Elapsed: 2:19:11.\n",
      "  Batch   630  of  1,230.    Elapsed: 2:21:25.\n",
      "  Batch   640  of  1,230.    Elapsed: 2:23:39.\n",
      "  Batch   650  of  1,230.    Elapsed: 2:25:54.\n",
      "  Batch   660  of  1,230.    Elapsed: 2:28:10.\n",
      "  Batch   670  of  1,230.    Elapsed: 2:30:25.\n",
      "  Batch   680  of  1,230.    Elapsed: 2:32:41.\n",
      "  Batch   690  of  1,230.    Elapsed: 2:34:56.\n",
      "  Batch   700  of  1,230.    Elapsed: 2:37:09.\n",
      "  Batch   710  of  1,230.    Elapsed: 2:39:24.\n",
      "  Batch   720  of  1,230.    Elapsed: 2:41:39.\n",
      "  Batch   730  of  1,230.    Elapsed: 2:43:52.\n",
      "  Batch   740  of  1,230.    Elapsed: 2:46:06.\n",
      "  Batch   750  of  1,230.    Elapsed: 2:48:21.\n",
      "  Batch   760  of  1,230.    Elapsed: 2:50:35.\n",
      "  Batch   770  of  1,230.    Elapsed: 2:52:50.\n",
      "  Batch   780  of  1,230.    Elapsed: 2:55:04.\n",
      "  Batch   790  of  1,230.    Elapsed: 2:57:17.\n",
      "  Batch   800  of  1,230.    Elapsed: 2:59:31.\n",
      "  Batch   810  of  1,230.    Elapsed: 3:01:44.\n",
      "  Batch   820  of  1,230.    Elapsed: 3:03:59.\n",
      "  Batch   830  of  1,230.    Elapsed: 3:06:14.\n",
      "  Batch   840  of  1,230.    Elapsed: 3:08:28.\n",
      "  Batch   850  of  1,230.    Elapsed: 3:10:43.\n",
      "  Batch   860  of  1,230.    Elapsed: 3:12:57.\n",
      "  Batch   870  of  1,230.    Elapsed: 3:15:11.\n",
      "  Batch   880  of  1,230.    Elapsed: 3:17:26.\n",
      "  Batch   890  of  1,230.    Elapsed: 3:19:42.\n",
      "  Batch   900  of  1,230.    Elapsed: 3:21:57.\n",
      "  Batch   910  of  1,230.    Elapsed: 3:24:11.\n",
      "  Batch   920  of  1,230.    Elapsed: 3:26:26.\n",
      "  Batch   930  of  1,230.    Elapsed: 3:28:40.\n",
      "  Batch   940  of  1,230.    Elapsed: 3:30:53.\n",
      "  Batch   950  of  1,230.    Elapsed: 3:33:07.\n",
      "  Batch   960  of  1,230.    Elapsed: 3:35:22.\n",
      "  Batch   970  of  1,230.    Elapsed: 3:37:35.\n",
      "  Batch   980  of  1,230.    Elapsed: 3:39:49.\n",
      "  Batch   990  of  1,230.    Elapsed: 3:42:04.\n",
      "  Batch 1,000  of  1,230.    Elapsed: 3:44:18.\n",
      "  Batch 1,010  of  1,230.    Elapsed: 3:46:32.\n",
      "  Batch 1,020  of  1,230.    Elapsed: 3:48:46.\n",
      "  Batch 1,030  of  1,230.    Elapsed: 3:51:01.\n",
      "  Batch 1,040  of  1,230.    Elapsed: 3:53:15.\n",
      "  Batch 1,050  of  1,230.    Elapsed: 3:55:29.\n",
      "  Batch 1,060  of  1,230.    Elapsed: 3:57:43.\n",
      "  Batch 1,070  of  1,230.    Elapsed: 3:59:57.\n",
      "  Batch 1,080  of  1,230.    Elapsed: 4:02:12.\n",
      "  Batch 1,090  of  1,230.    Elapsed: 4:04:27.\n",
      "  Batch 1,100  of  1,230.    Elapsed: 4:06:41.\n",
      "  Batch 1,110  of  1,230.    Elapsed: 4:08:55.\n",
      "  Batch 1,120  of  1,230.    Elapsed: 4:11:09.\n",
      "  Batch 1,130  of  1,230.    Elapsed: 4:13:23.\n",
      "  Batch 1,140  of  1,230.    Elapsed: 4:15:37.\n",
      "  Batch 1,150  of  1,230.    Elapsed: 4:17:51.\n",
      "  Batch 1,160  of  1,230.    Elapsed: 4:20:04.\n",
      "  Batch 1,170  of  1,230.    Elapsed: 4:22:17.\n",
      "  Batch 1,180  of  1,230.    Elapsed: 4:24:31.\n",
      "  Batch 1,190  of  1,230.    Elapsed: 4:26:45.\n",
      "  Batch 1,200  of  1,230.    Elapsed: 4:28:59.\n",
      "  Batch 1,210  of  1,230.    Elapsed: 4:31:13.\n",
      "  Batch 1,220  of  1,230.    Elapsed: 4:33:27.\n",
      "Train loss: 0.1818620647053893\n",
      "Validation loss: 0.1211707437016668\n",
      "Validation Accuracy: 0.24341099417190065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  60%|██████    | 6/10 [28:55:13<19:18:59, 17384.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.8905476457013995\n",
      "  Batch    10  of  1,230.    Elapsed: 0:02:14.\n",
      "  Batch    20  of  1,230.    Elapsed: 0:04:28.\n",
      "  Batch    30  of  1,230.    Elapsed: 0:06:42.\n",
      "  Batch    40  of  1,230.    Elapsed: 0:08:58.\n",
      "  Batch    50  of  1,230.    Elapsed: 0:11:13.\n",
      "  Batch    60  of  1,230.    Elapsed: 0:13:28.\n",
      "  Batch    70  of  1,230.    Elapsed: 0:15:43.\n",
      "  Batch    80  of  1,230.    Elapsed: 0:17:57.\n",
      "  Batch    90  of  1,230.    Elapsed: 0:20:11.\n",
      "  Batch   100  of  1,230.    Elapsed: 0:22:25.\n",
      "  Batch   110  of  1,230.    Elapsed: 0:24:38.\n",
      "  Batch   120  of  1,230.    Elapsed: 0:26:54.\n",
      "  Batch   130  of  1,230.    Elapsed: 0:29:09.\n",
      "  Batch   140  of  1,230.    Elapsed: 0:31:23.\n",
      "  Batch   150  of  1,230.    Elapsed: 0:33:36.\n",
      "  Batch   160  of  1,230.    Elapsed: 0:35:50.\n",
      "  Batch   170  of  1,230.    Elapsed: 0:38:05.\n",
      "  Batch   180  of  1,230.    Elapsed: 0:40:19.\n",
      "  Batch   190  of  1,230.    Elapsed: 0:42:32.\n",
      "  Batch   200  of  1,230.    Elapsed: 0:44:46.\n",
      "  Batch   210  of  1,230.    Elapsed: 0:47:00.\n",
      "  Batch   220  of  1,230.    Elapsed: 0:49:15.\n",
      "  Batch   230  of  1,230.    Elapsed: 0:51:28.\n",
      "  Batch   240  of  1,230.    Elapsed: 0:53:43.\n",
      "  Batch   250  of  1,230.    Elapsed: 0:55:58.\n",
      "  Batch   260  of  1,230.    Elapsed: 0:58:12.\n",
      "  Batch   270  of  1,230.    Elapsed: 1:00:27.\n",
      "  Batch   280  of  1,230.    Elapsed: 1:02:42.\n",
      "  Batch   290  of  1,230.    Elapsed: 1:04:56.\n",
      "  Batch   300  of  1,230.    Elapsed: 1:07:11.\n",
      "  Batch   310  of  1,230.    Elapsed: 1:09:25.\n",
      "  Batch   320  of  1,230.    Elapsed: 1:11:39.\n",
      "  Batch   330  of  1,230.    Elapsed: 1:13:53.\n",
      "  Batch   340  of  1,230.    Elapsed: 1:16:08.\n",
      "  Batch   350  of  1,230.    Elapsed: 1:18:23.\n",
      "  Batch   360  of  1,230.    Elapsed: 1:20:36.\n",
      "  Batch   370  of  1,230.    Elapsed: 1:22:50.\n",
      "  Batch   380  of  1,230.    Elapsed: 1:25:04.\n",
      "  Batch   390  of  1,230.    Elapsed: 1:27:20.\n",
      "  Batch   400  of  1,230.    Elapsed: 1:29:34.\n",
      "  Batch   410  of  1,230.    Elapsed: 1:31:49.\n",
      "  Batch   420  of  1,230.    Elapsed: 1:34:03.\n",
      "  Batch   430  of  1,230.    Elapsed: 1:36:18.\n",
      "  Batch   440  of  1,230.    Elapsed: 1:38:34.\n",
      "  Batch   450  of  1,230.    Elapsed: 1:40:48.\n",
      "  Batch   460  of  1,230.    Elapsed: 1:43:03.\n",
      "  Batch   470  of  1,230.    Elapsed: 1:45:18.\n",
      "  Batch   480  of  1,230.    Elapsed: 1:47:32.\n",
      "  Batch   490  of  1,230.    Elapsed: 1:49:47.\n",
      "  Batch   500  of  1,230.    Elapsed: 1:52:02.\n",
      "  Batch   510  of  1,230.    Elapsed: 1:54:17.\n",
      "  Batch   520  of  1,230.    Elapsed: 1:56:32.\n",
      "  Batch   530  of  1,230.    Elapsed: 1:58:46.\n",
      "  Batch   540  of  1,230.    Elapsed: 2:01:00.\n",
      "  Batch   550  of  1,230.    Elapsed: 2:03:15.\n",
      "  Batch   560  of  1,230.    Elapsed: 2:05:28.\n",
      "  Batch   570  of  1,230.    Elapsed: 2:07:43.\n",
      "  Batch   580  of  1,230.    Elapsed: 2:09:57.\n",
      "  Batch   590  of  1,230.    Elapsed: 2:12:11.\n",
      "  Batch   600  of  1,230.    Elapsed: 2:14:25.\n",
      "  Batch   610  of  1,230.    Elapsed: 2:16:39.\n",
      "  Batch   620  of  1,230.    Elapsed: 2:18:55.\n",
      "  Batch   630  of  1,230.    Elapsed: 2:21:09.\n",
      "  Batch   640  of  1,230.    Elapsed: 2:23:23.\n",
      "  Batch   650  of  1,230.    Elapsed: 2:25:37.\n",
      "  Batch   660  of  1,230.    Elapsed: 2:27:53.\n",
      "  Batch   670  of  1,230.    Elapsed: 2:30:07.\n",
      "  Batch   680  of  1,230.    Elapsed: 2:32:22.\n",
      "  Batch   690  of  1,230.    Elapsed: 2:34:36.\n",
      "  Batch   700  of  1,230.    Elapsed: 2:36:49.\n",
      "  Batch   710  of  1,230.    Elapsed: 2:39:02.\n",
      "  Batch   720  of  1,230.    Elapsed: 2:41:17.\n",
      "  Batch   730  of  1,230.    Elapsed: 2:43:31.\n",
      "  Batch   740  of  1,230.    Elapsed: 2:45:47.\n",
      "  Batch   750  of  1,230.    Elapsed: 2:48:01.\n",
      "  Batch   760  of  1,230.    Elapsed: 2:50:15.\n",
      "  Batch   770  of  1,230.    Elapsed: 2:52:29.\n",
      "  Batch   780  of  1,230.    Elapsed: 2:54:44.\n",
      "  Batch   790  of  1,230.    Elapsed: 2:56:58.\n",
      "  Batch   800  of  1,230.    Elapsed: 2:59:11.\n",
      "  Batch   810  of  1,230.    Elapsed: 3:01:26.\n",
      "  Batch   820  of  1,230.    Elapsed: 3:03:42.\n",
      "  Batch   830  of  1,230.    Elapsed: 3:05:56.\n",
      "  Batch   840  of  1,230.    Elapsed: 3:08:10.\n",
      "  Batch   850  of  1,230.    Elapsed: 3:10:24.\n",
      "  Batch   860  of  1,230.    Elapsed: 3:12:38.\n",
      "  Batch   870  of  1,230.    Elapsed: 3:14:53.\n",
      "  Batch   880  of  1,230.    Elapsed: 3:17:08.\n",
      "  Batch   890  of  1,230.    Elapsed: 3:19:23.\n",
      "  Batch   900  of  1,230.    Elapsed: 3:21:37.\n",
      "  Batch   910  of  1,230.    Elapsed: 3:23:51.\n",
      "  Batch   920  of  1,230.    Elapsed: 3:26:05.\n",
      "  Batch   930  of  1,230.    Elapsed: 3:28:19.\n",
      "  Batch   940  of  1,230.    Elapsed: 3:30:34.\n",
      "  Batch   950  of  1,230.    Elapsed: 3:32:48.\n",
      "  Batch   960  of  1,230.    Elapsed: 3:35:02.\n",
      "  Batch   970  of  1,230.    Elapsed: 3:37:17.\n",
      "  Batch   980  of  1,230.    Elapsed: 3:39:31.\n",
      "  Batch   990  of  1,230.    Elapsed: 3:41:46.\n",
      "  Batch 1,000  of  1,230.    Elapsed: 3:44:00.\n",
      "  Batch 1,010  of  1,230.    Elapsed: 3:46:14.\n",
      "  Batch 1,020  of  1,230.    Elapsed: 3:48:28.\n",
      "  Batch 1,030  of  1,230.    Elapsed: 3:50:42.\n",
      "  Batch 1,040  of  1,230.    Elapsed: 3:52:56.\n",
      "  Batch 1,050  of  1,230.    Elapsed: 3:55:10.\n",
      "  Batch 1,060  of  1,230.    Elapsed: 3:57:24.\n",
      "  Batch 1,070  of  1,230.    Elapsed: 3:59:36.\n",
      "  Batch 1,080  of  1,230.    Elapsed: 4:01:50.\n",
      "  Batch 1,090  of  1,230.    Elapsed: 4:04:05.\n",
      "  Batch 1,100  of  1,230.    Elapsed: 4:06:21.\n",
      "  Batch 1,110  of  1,230.    Elapsed: 4:08:35.\n",
      "  Batch 1,120  of  1,230.    Elapsed: 4:10:50.\n",
      "  Batch 1,130  of  1,230.    Elapsed: 4:13:06.\n",
      "  Batch 1,140  of  1,230.    Elapsed: 4:15:20.\n",
      "  Batch 1,150  of  1,230.    Elapsed: 4:17:34.\n",
      "  Batch 1,160  of  1,230.    Elapsed: 4:19:49.\n",
      "  Batch 1,170  of  1,230.    Elapsed: 4:22:03.\n",
      "  Batch 1,180  of  1,230.    Elapsed: 4:24:17.\n",
      "  Batch 1,190  of  1,230.    Elapsed: 4:26:29.\n",
      "  Batch 1,200  of  1,230.    Elapsed: 4:28:43.\n",
      "  Batch 1,210  of  1,230.    Elapsed: 4:30:57.\n",
      "  Batch 1,220  of  1,230.    Elapsed: 4:33:11.\n",
      "Train loss: 0.15604332405013766\n",
      "Validation loss: 0.10744619274335186\n",
      "Validation Accuracy: 0.2435288116052736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  70%|███████   | 7/10 [33:46:10<14:30:19, 17406.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.8964830655648985\n",
      "  Batch    10  of  1,230.    Elapsed: 0:02:13.\n",
      "  Batch    20  of  1,230.    Elapsed: 0:04:26.\n",
      "  Batch    30  of  1,230.    Elapsed: 0:06:41.\n",
      "  Batch    40  of  1,230.    Elapsed: 0:08:57.\n",
      "  Batch    50  of  1,230.    Elapsed: 0:11:11.\n",
      "  Batch    60  of  1,230.    Elapsed: 0:13:25.\n",
      "  Batch    70  of  1,230.    Elapsed: 0:15:39.\n",
      "  Batch    80  of  1,230.    Elapsed: 0:17:52.\n",
      "  Batch    90  of  1,230.    Elapsed: 0:20:07.\n",
      "  Batch   100  of  1,230.    Elapsed: 0:22:21.\n",
      "  Batch   110  of  1,230.    Elapsed: 0:24:36.\n",
      "  Batch   120  of  1,230.    Elapsed: 0:26:51.\n",
      "  Batch   130  of  1,230.    Elapsed: 0:29:05.\n",
      "  Batch   140  of  1,230.    Elapsed: 0:31:18.\n",
      "  Batch   150  of  1,230.    Elapsed: 0:33:33.\n",
      "  Batch   160  of  1,230.    Elapsed: 0:35:47.\n",
      "  Batch   170  of  1,230.    Elapsed: 0:38:01.\n",
      "  Batch   180  of  1,230.    Elapsed: 0:40:15.\n",
      "  Batch   190  of  1,230.    Elapsed: 0:42:29.\n",
      "  Batch   200  of  1,230.    Elapsed: 0:44:44.\n",
      "  Batch   210  of  1,230.    Elapsed: 0:46:59.\n",
      "  Batch   220  of  1,230.    Elapsed: 0:49:13.\n",
      "  Batch   230  of  1,230.    Elapsed: 0:51:26.\n",
      "  Batch   240  of  1,230.    Elapsed: 0:53:41.\n",
      "  Batch   250  of  1,230.    Elapsed: 0:55:55.\n",
      "  Batch   260  of  1,230.    Elapsed: 0:58:09.\n",
      "  Batch   270  of  1,230.    Elapsed: 1:00:22.\n",
      "  Batch   280  of  1,230.    Elapsed: 1:02:37.\n",
      "  Batch   290  of  1,230.    Elapsed: 1:04:50.\n",
      "  Batch   300  of  1,230.    Elapsed: 1:07:04.\n",
      "  Batch   310  of  1,230.    Elapsed: 1:09:18.\n",
      "  Batch   320  of  1,230.    Elapsed: 1:11:33.\n",
      "  Batch   330  of  1,230.    Elapsed: 1:13:47.\n",
      "  Batch   340  of  1,230.    Elapsed: 1:16:01.\n",
      "  Batch   350  of  1,230.    Elapsed: 1:18:17.\n",
      "  Batch   360  of  1,230.    Elapsed: 1:20:31.\n",
      "  Batch   370  of  1,230.    Elapsed: 1:22:45.\n",
      "  Batch   380  of  1,230.    Elapsed: 1:24:59.\n",
      "  Batch   390  of  1,230.    Elapsed: 1:27:14.\n",
      "  Batch   400  of  1,230.    Elapsed: 1:29:27.\n",
      "  Batch   410  of  1,230.    Elapsed: 1:31:41.\n",
      "  Batch   420  of  1,230.    Elapsed: 1:33:57.\n",
      "  Batch   430  of  1,230.    Elapsed: 1:36:12.\n",
      "  Batch   440  of  1,230.    Elapsed: 1:38:25.\n",
      "  Batch   450  of  1,230.    Elapsed: 1:40:39.\n",
      "  Batch   460  of  1,230.    Elapsed: 1:42:54.\n",
      "  Batch   470  of  1,230.    Elapsed: 1:45:07.\n",
      "  Batch   480  of  1,230.    Elapsed: 1:47:21.\n",
      "  Batch   490  of  1,230.    Elapsed: 1:49:35.\n",
      "  Batch   500  of  1,230.    Elapsed: 1:51:47.\n",
      "  Batch   510  of  1,230.    Elapsed: 1:54:02.\n",
      "  Batch   520  of  1,230.    Elapsed: 1:56:16.\n",
      "  Batch   530  of  1,230.    Elapsed: 1:58:30.\n",
      "  Batch   540  of  1,230.    Elapsed: 2:00:44.\n",
      "  Batch   550  of  1,230.    Elapsed: 2:02:59.\n",
      "  Batch   560  of  1,230.    Elapsed: 2:05:14.\n",
      "  Batch   570  of  1,230.    Elapsed: 2:07:28.\n",
      "  Batch   580  of  1,230.    Elapsed: 2:09:44.\n",
      "  Batch   590  of  1,230.    Elapsed: 2:11:58.\n",
      "  Batch   600  of  1,230.    Elapsed: 2:14:13.\n",
      "  Batch   610  of  1,230.    Elapsed: 2:16:28.\n",
      "  Batch   620  of  1,230.    Elapsed: 2:18:43.\n",
      "  Batch   630  of  1,230.    Elapsed: 2:20:56.\n",
      "  Batch   640  of  1,230.    Elapsed: 2:23:11.\n",
      "  Batch   650  of  1,230.    Elapsed: 2:25:25.\n",
      "  Batch   660  of  1,230.    Elapsed: 2:27:39.\n",
      "  Batch   670  of  1,230.    Elapsed: 2:29:53.\n",
      "  Batch   680  of  1,230.    Elapsed: 2:32:07.\n",
      "  Batch   690  of  1,230.    Elapsed: 2:34:22.\n",
      "  Batch   700  of  1,230.    Elapsed: 2:36:37.\n",
      "  Batch   710  of  1,230.    Elapsed: 2:38:52.\n",
      "  Batch   720  of  1,230.    Elapsed: 2:41:07.\n",
      "  Batch   730  of  1,230.    Elapsed: 2:43:21.\n",
      "  Batch   740  of  1,230.    Elapsed: 2:45:36.\n",
      "  Batch   750  of  1,230.    Elapsed: 2:47:50.\n",
      "  Batch   760  of  1,230.    Elapsed: 2:50:03.\n",
      "  Batch   770  of  1,230.    Elapsed: 2:52:17.\n",
      "  Batch   780  of  1,230.    Elapsed: 2:54:32.\n",
      "  Batch   790  of  1,230.    Elapsed: 2:56:45.\n",
      "  Batch   800  of  1,230.    Elapsed: 2:59:00.\n",
      "  Batch   810  of  1,230.    Elapsed: 3:01:14.\n",
      "  Batch   820  of  1,230.    Elapsed: 3:03:29.\n",
      "  Batch   830  of  1,230.    Elapsed: 3:05:42.\n",
      "  Batch   840  of  1,230.    Elapsed: 3:07:56.\n",
      "  Batch   850  of  1,230.    Elapsed: 3:10:11.\n",
      "  Batch   860  of  1,230.    Elapsed: 3:12:24.\n",
      "  Batch   870  of  1,230.    Elapsed: 3:14:39.\n",
      "  Batch   880  of  1,230.    Elapsed: 3:16:54.\n",
      "  Batch   890  of  1,230.    Elapsed: 3:19:08.\n",
      "  Batch   900  of  1,230.    Elapsed: 3:21:22.\n",
      "  Batch   910  of  1,230.    Elapsed: 3:23:36.\n",
      "  Batch   920  of  1,230.    Elapsed: 3:25:51.\n",
      "  Batch   930  of  1,230.    Elapsed: 3:28:06.\n",
      "  Batch   940  of  1,230.    Elapsed: 3:30:21.\n",
      "  Batch   950  of  1,230.    Elapsed: 3:32:36.\n",
      "  Batch   960  of  1,230.    Elapsed: 3:34:51.\n",
      "  Batch   970  of  1,230.    Elapsed: 3:37:05.\n",
      "  Batch   980  of  1,230.    Elapsed: 3:39:19.\n",
      "  Batch   990  of  1,230.    Elapsed: 3:41:33.\n",
      "  Batch 1,000  of  1,230.    Elapsed: 3:43:47.\n",
      "  Batch 1,010  of  1,230.    Elapsed: 3:46:01.\n",
      "  Batch 1,020  of  1,230.    Elapsed: 3:48:15.\n",
      "  Batch 1,030  of  1,230.    Elapsed: 3:50:29.\n",
      "  Batch 1,040  of  1,230.    Elapsed: 3:52:44.\n",
      "  Batch 1,050  of  1,230.    Elapsed: 3:54:58.\n",
      "  Batch 1,060  of  1,230.    Elapsed: 3:57:11.\n",
      "  Batch 1,070  of  1,230.    Elapsed: 3:59:25.\n",
      "  Batch 1,080  of  1,230.    Elapsed: 4:01:40.\n",
      "  Batch 1,090  of  1,230.    Elapsed: 4:03:55.\n",
      "  Batch 1,100  of  1,230.    Elapsed: 4:06:08.\n",
      "  Batch 1,110  of  1,230.    Elapsed: 4:08:23.\n",
      "  Batch 1,120  of  1,230.    Elapsed: 4:10:38.\n",
      "  Batch 1,130  of  1,230.    Elapsed: 4:12:53.\n",
      "  Batch 1,140  of  1,230.    Elapsed: 4:15:08.\n",
      "  Batch 1,150  of  1,230.    Elapsed: 4:17:22.\n",
      "  Batch 1,160  of  1,230.    Elapsed: 4:19:37.\n",
      "  Batch 1,170  of  1,230.    Elapsed: 4:21:52.\n",
      "  Batch 1,180  of  1,230.    Elapsed: 4:24:06.\n",
      "  Batch 1,190  of  1,230.    Elapsed: 4:26:21.\n",
      "  Batch 1,200  of  1,230.    Elapsed: 4:28:34.\n",
      "  Batch 1,210  of  1,230.    Elapsed: 4:30:48.\n",
      "  Batch 1,220  of  1,230.    Elapsed: 4:33:02.\n",
      "Train loss: 0.1379555811000064\n",
      "Validation loss: 0.1477762524881502\n",
      "Validation Accuracy: 0.23960669170486057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  80%|████████  | 8/10 [38:37:05<9:40:41, 17420.93s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.8772330836818727\n",
      "  Batch    10  of  1,230.    Elapsed: 0:02:16.\n",
      "  Batch    20  of  1,230.    Elapsed: 0:04:30.\n",
      "  Batch    30  of  1,230.    Elapsed: 0:06:44.\n",
      "  Batch    40  of  1,230.    Elapsed: 0:08:58.\n",
      "  Batch    50  of  1,230.    Elapsed: 0:11:13.\n",
      "  Batch    60  of  1,230.    Elapsed: 0:13:27.\n",
      "  Batch    70  of  1,230.    Elapsed: 0:15:42.\n",
      "  Batch    80  of  1,230.    Elapsed: 0:17:56.\n",
      "  Batch    90  of  1,230.    Elapsed: 0:20:10.\n",
      "  Batch   100  of  1,230.    Elapsed: 0:22:24.\n",
      "  Batch   110  of  1,230.    Elapsed: 0:24:40.\n",
      "  Batch   120  of  1,230.    Elapsed: 0:26:55.\n",
      "  Batch   130  of  1,230.    Elapsed: 0:29:10.\n",
      "  Batch   140  of  1,230.    Elapsed: 0:31:24.\n",
      "  Batch   150  of  1,230.    Elapsed: 0:33:40.\n",
      "  Batch   160  of  1,230.    Elapsed: 0:35:55.\n",
      "  Batch   170  of  1,230.    Elapsed: 0:38:11.\n",
      "  Batch   180  of  1,230.    Elapsed: 0:40:26.\n",
      "  Batch   190  of  1,230.    Elapsed: 0:42:40.\n",
      "  Batch   200  of  1,230.    Elapsed: 0:44:55.\n",
      "  Batch   210  of  1,230.    Elapsed: 0:47:11.\n",
      "  Batch   220  of  1,230.    Elapsed: 0:49:26.\n",
      "  Batch   230  of  1,230.    Elapsed: 0:51:41.\n",
      "  Batch   240  of  1,230.    Elapsed: 0:53:56.\n",
      "  Batch   250  of  1,230.    Elapsed: 0:56:12.\n",
      "  Batch   260  of  1,230.    Elapsed: 0:58:28.\n",
      "  Batch   270  of  1,230.    Elapsed: 1:00:42.\n",
      "  Batch   280  of  1,230.    Elapsed: 1:02:55.\n",
      "  Batch   290  of  1,230.    Elapsed: 1:05:10.\n",
      "  Batch   300  of  1,230.    Elapsed: 1:07:26.\n",
      "  Batch   310  of  1,230.    Elapsed: 1:09:40.\n",
      "  Batch   320  of  1,230.    Elapsed: 1:11:54.\n",
      "  Batch   330  of  1,230.    Elapsed: 1:14:08.\n",
      "  Batch   340  of  1,230.    Elapsed: 1:16:22.\n",
      "  Batch   350  of  1,230.    Elapsed: 1:18:38.\n",
      "  Batch   360  of  1,230.    Elapsed: 1:20:52.\n",
      "  Batch   370  of  1,230.    Elapsed: 1:23:06.\n",
      "  Batch   380  of  1,230.    Elapsed: 1:25:20.\n",
      "  Batch   390  of  1,230.    Elapsed: 1:27:34.\n",
      "  Batch   400  of  1,230.    Elapsed: 1:29:49.\n",
      "  Batch   410  of  1,230.    Elapsed: 1:32:03.\n",
      "  Batch   420  of  1,230.    Elapsed: 1:34:17.\n",
      "  Batch   430  of  1,230.    Elapsed: 1:36:30.\n",
      "  Batch   440  of  1,230.    Elapsed: 1:38:45.\n",
      "  Batch   450  of  1,230.    Elapsed: 1:40:59.\n",
      "  Batch   460  of  1,230.    Elapsed: 1:43:13.\n",
      "  Batch   470  of  1,230.    Elapsed: 1:45:28.\n",
      "  Batch   480  of  1,230.    Elapsed: 1:47:43.\n",
      "  Batch   490  of  1,230.    Elapsed: 1:49:56.\n",
      "  Batch   500  of  1,230.    Elapsed: 1:52:11.\n",
      "  Batch   510  of  1,230.    Elapsed: 1:54:25.\n",
      "  Batch   520  of  1,230.    Elapsed: 1:56:39.\n",
      "  Batch   530  of  1,230.    Elapsed: 1:58:53.\n",
      "  Batch   540  of  1,230.    Elapsed: 2:01:08.\n",
      "  Batch   550  of  1,230.    Elapsed: 2:03:23.\n",
      "  Batch   560  of  1,230.    Elapsed: 2:05:38.\n",
      "  Batch   570  of  1,230.    Elapsed: 2:07:53.\n",
      "  Batch   580  of  1,230.    Elapsed: 2:10:08.\n",
      "  Batch   590  of  1,230.    Elapsed: 2:12:22.\n",
      "  Batch   600  of  1,230.    Elapsed: 2:14:36.\n",
      "  Batch   610  of  1,230.    Elapsed: 2:16:50.\n",
      "  Batch   620  of  1,230.    Elapsed: 2:19:04.\n",
      "  Batch   630  of  1,230.    Elapsed: 2:21:18.\n",
      "  Batch   640  of  1,230.    Elapsed: 2:23:32.\n",
      "  Batch   650  of  1,230.    Elapsed: 2:25:46.\n",
      "  Batch   660  of  1,230.    Elapsed: 2:28:02.\n",
      "  Batch   670  of  1,230.    Elapsed: 2:30:16.\n",
      "  Batch   680  of  1,230.    Elapsed: 2:32:30.\n",
      "  Batch   690  of  1,230.    Elapsed: 2:34:44.\n",
      "  Batch   700  of  1,230.    Elapsed: 2:36:57.\n",
      "  Batch   710  of  1,230.    Elapsed: 2:39:12.\n",
      "  Batch   720  of  1,230.    Elapsed: 2:41:25.\n",
      "  Batch   730  of  1,230.    Elapsed: 2:43:40.\n",
      "  Batch   740  of  1,230.    Elapsed: 2:45:55.\n",
      "  Batch   750  of  1,230.    Elapsed: 2:48:08.\n",
      "  Batch   760  of  1,230.    Elapsed: 2:50:22.\n",
      "  Batch   770  of  1,230.    Elapsed: 2:52:37.\n",
      "  Batch   780  of  1,230.    Elapsed: 2:54:51.\n",
      "  Batch   790  of  1,230.    Elapsed: 2:57:06.\n",
      "  Batch   800  of  1,230.    Elapsed: 2:59:21.\n",
      "  Batch   810  of  1,230.    Elapsed: 3:01:34.\n",
      "  Batch   820  of  1,230.    Elapsed: 3:03:48.\n",
      "  Batch   830  of  1,230.    Elapsed: 3:06:03.\n",
      "  Batch   840  of  1,230.    Elapsed: 3:08:18.\n",
      "  Batch   850  of  1,230.    Elapsed: 3:10:33.\n",
      "  Batch   860  of  1,230.    Elapsed: 3:12:46.\n",
      "  Batch   870  of  1,230.    Elapsed: 3:15:01.\n",
      "  Batch   880  of  1,230.    Elapsed: 3:17:14.\n",
      "  Batch   890  of  1,230.    Elapsed: 3:19:28.\n",
      "  Batch   900  of  1,230.    Elapsed: 3:21:42.\n",
      "  Batch   910  of  1,230.    Elapsed: 3:23:57.\n",
      "  Batch   920  of  1,230.    Elapsed: 3:26:12.\n",
      "  Batch   930  of  1,230.    Elapsed: 3:28:26.\n",
      "  Batch   940  of  1,230.    Elapsed: 3:30:41.\n",
      "  Batch   950  of  1,230.    Elapsed: 3:32:56.\n",
      "  Batch   960  of  1,230.    Elapsed: 3:35:10.\n",
      "  Batch   970  of  1,230.    Elapsed: 3:37:24.\n",
      "  Batch   980  of  1,230.    Elapsed: 3:39:38.\n",
      "  Batch   990  of  1,230.    Elapsed: 3:41:53.\n",
      "  Batch 1,000  of  1,230.    Elapsed: 3:44:07.\n",
      "  Batch 1,010  of  1,230.    Elapsed: 3:46:21.\n",
      "  Batch 1,020  of  1,230.    Elapsed: 3:48:37.\n",
      "  Batch 1,030  of  1,230.    Elapsed: 3:50:51.\n",
      "  Batch 1,040  of  1,230.    Elapsed: 3:53:06.\n",
      "  Batch 1,050  of  1,230.    Elapsed: 3:55:20.\n",
      "  Batch 1,060  of  1,230.    Elapsed: 3:57:35.\n",
      "  Batch 1,070  of  1,230.    Elapsed: 3:59:49.\n",
      "  Batch 1,080  of  1,230.    Elapsed: 4:02:03.\n",
      "  Batch 1,090  of  1,230.    Elapsed: 4:04:16.\n",
      "  Batch 1,100  of  1,230.    Elapsed: 4:06:30.\n",
      "  Batch 1,110  of  1,230.    Elapsed: 4:08:44.\n",
      "  Batch 1,120  of  1,230.    Elapsed: 4:10:59.\n",
      "  Batch 1,130  of  1,230.    Elapsed: 4:13:14.\n",
      "  Batch 1,140  of  1,230.    Elapsed: 4:15:28.\n",
      "  Batch 1,150  of  1,230.    Elapsed: 4:17:42.\n",
      "  Batch 1,160  of  1,230.    Elapsed: 4:19:56.\n",
      "  Batch 1,170  of  1,230.    Elapsed: 4:22:11.\n",
      "  Batch 1,180  of  1,230.    Elapsed: 4:24:24.\n",
      "  Batch 1,190  of  1,230.    Elapsed: 4:26:38.\n",
      "  Batch 1,200  of  1,230.    Elapsed: 4:28:51.\n",
      "  Batch 1,210  of  1,230.    Elapsed: 4:31:06.\n",
      "  Batch 1,220  of  1,230.    Elapsed: 4:33:19.\n",
      "Train loss: 0.12237776792570343\n",
      "Validation loss: 0.1056310157325581\n",
      "Validation Accuracy: 0.24297984566853392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  90%|█████████ | 9/10 [43:28:13<4:50:34, 17434.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.8982955602092547\n",
      "  Batch    10  of  1,230.    Elapsed: 0:02:14.\n",
      "  Batch    20  of  1,230.    Elapsed: 0:04:27.\n",
      "  Batch    30  of  1,230.    Elapsed: 0:06:42.\n",
      "  Batch    40  of  1,230.    Elapsed: 0:08:55.\n",
      "  Batch    50  of  1,230.    Elapsed: 0:11:10.\n",
      "  Batch    60  of  1,230.    Elapsed: 0:13:26.\n",
      "  Batch    70  of  1,230.    Elapsed: 0:15:40.\n",
      "  Batch    80  of  1,230.    Elapsed: 0:17:54.\n",
      "  Batch    90  of  1,230.    Elapsed: 0:20:09.\n",
      "  Batch   100  of  1,230.    Elapsed: 0:22:23.\n",
      "  Batch   110  of  1,230.    Elapsed: 0:24:39.\n",
      "  Batch   120  of  1,230.    Elapsed: 0:26:53.\n",
      "  Batch   130  of  1,230.    Elapsed: 0:29:07.\n",
      "  Batch   140  of  1,230.    Elapsed: 0:31:21.\n",
      "  Batch   150  of  1,230.    Elapsed: 0:33:35.\n",
      "  Batch   160  of  1,230.    Elapsed: 0:35:50.\n",
      "  Batch   170  of  1,230.    Elapsed: 0:38:04.\n",
      "  Batch   180  of  1,230.    Elapsed: 0:40:18.\n",
      "  Batch   190  of  1,230.    Elapsed: 0:42:33.\n",
      "  Batch   200  of  1,230.    Elapsed: 0:44:46.\n",
      "  Batch   210  of  1,230.    Elapsed: 0:47:00.\n",
      "  Batch   220  of  1,230.    Elapsed: 0:49:14.\n",
      "  Batch   230  of  1,230.    Elapsed: 0:51:28.\n",
      "  Batch   240  of  1,230.    Elapsed: 0:53:42.\n",
      "  Batch   250  of  1,230.    Elapsed: 0:55:58.\n",
      "  Batch   260  of  1,230.    Elapsed: 0:58:14.\n",
      "  Batch   270  of  1,230.    Elapsed: 1:00:28.\n",
      "  Batch   280  of  1,230.    Elapsed: 1:02:44.\n",
      "  Batch   290  of  1,230.    Elapsed: 1:04:58.\n",
      "  Batch   300  of  1,230.    Elapsed: 1:07:14.\n",
      "  Batch   310  of  1,230.    Elapsed: 1:09:28.\n",
      "  Batch   320  of  1,230.    Elapsed: 1:11:43.\n",
      "  Batch   330  of  1,230.    Elapsed: 1:13:58.\n",
      "  Batch   340  of  1,230.    Elapsed: 1:16:13.\n",
      "  Batch   350  of  1,230.    Elapsed: 1:18:28.\n",
      "  Batch   360  of  1,230.    Elapsed: 1:20:42.\n",
      "  Batch   370  of  1,230.    Elapsed: 1:22:57.\n",
      "  Batch   380  of  1,230.    Elapsed: 1:25:12.\n",
      "  Batch   390  of  1,230.    Elapsed: 1:27:27.\n",
      "  Batch   400  of  1,230.    Elapsed: 1:29:42.\n",
      "  Batch   410  of  1,230.    Elapsed: 1:31:57.\n",
      "  Batch   420  of  1,230.    Elapsed: 1:34:11.\n",
      "  Batch   430  of  1,230.    Elapsed: 1:36:25.\n",
      "  Batch   440  of  1,230.    Elapsed: 1:38:40.\n",
      "  Batch   450  of  1,230.    Elapsed: 1:40:55.\n",
      "  Batch   460  of  1,230.    Elapsed: 1:43:10.\n",
      "  Batch   470  of  1,230.    Elapsed: 1:45:26.\n",
      "  Batch   480  of  1,230.    Elapsed: 1:47:41.\n",
      "  Batch   490  of  1,230.    Elapsed: 1:49:56.\n",
      "  Batch   500  of  1,230.    Elapsed: 1:52:10.\n",
      "  Batch   510  of  1,230.    Elapsed: 1:54:24.\n",
      "  Batch   520  of  1,230.    Elapsed: 1:56:38.\n",
      "  Batch   530  of  1,230.    Elapsed: 1:58:53.\n",
      "  Batch   540  of  1,230.    Elapsed: 2:01:07.\n",
      "  Batch   550  of  1,230.    Elapsed: 2:03:22.\n",
      "  Batch   560  of  1,230.    Elapsed: 2:05:37.\n",
      "  Batch   570  of  1,230.    Elapsed: 2:07:51.\n",
      "  Batch   580  of  1,230.    Elapsed: 2:10:06.\n",
      "  Batch   590  of  1,230.    Elapsed: 2:12:22.\n",
      "  Batch   600  of  1,230.    Elapsed: 2:14:37.\n",
      "  Batch   610  of  1,230.    Elapsed: 2:16:53.\n",
      "  Batch   620  of  1,230.    Elapsed: 2:19:09.\n",
      "  Batch   630  of  1,230.    Elapsed: 2:21:24.\n",
      "  Batch   640  of  1,230.    Elapsed: 2:23:39.\n",
      "  Batch   650  of  1,230.    Elapsed: 2:25:53.\n",
      "  Batch   660  of  1,230.    Elapsed: 2:28:08.\n",
      "  Batch   670  of  1,230.    Elapsed: 2:30:24.\n",
      "  Batch   680  of  1,230.    Elapsed: 2:32:40.\n",
      "  Batch   690  of  1,230.    Elapsed: 2:34:55.\n",
      "  Batch   700  of  1,230.    Elapsed: 2:37:11.\n",
      "  Batch   710  of  1,230.    Elapsed: 2:39:26.\n",
      "  Batch   720  of  1,230.    Elapsed: 2:41:41.\n",
      "  Batch   730  of  1,230.    Elapsed: 2:43:57.\n",
      "  Batch   740  of  1,230.    Elapsed: 2:46:10.\n",
      "  Batch   750  of  1,230.    Elapsed: 2:48:25.\n",
      "  Batch   760  of  1,230.    Elapsed: 2:50:39.\n",
      "  Batch   770  of  1,230.    Elapsed: 2:52:53.\n",
      "  Batch   780  of  1,230.    Elapsed: 2:55:08.\n",
      "  Batch   790  of  1,230.    Elapsed: 2:57:22.\n",
      "  Batch   800  of  1,230.    Elapsed: 2:59:38.\n",
      "  Batch   810  of  1,230.    Elapsed: 3:01:53.\n",
      "  Batch   820  of  1,230.    Elapsed: 3:04:08.\n",
      "  Batch   830  of  1,230.    Elapsed: 3:06:21.\n",
      "  Batch   840  of  1,230.    Elapsed: 3:08:36.\n",
      "  Batch   850  of  1,230.    Elapsed: 3:10:51.\n",
      "  Batch   860  of  1,230.    Elapsed: 3:13:07.\n",
      "  Batch   870  of  1,230.    Elapsed: 3:15:24.\n",
      "  Batch   880  of  1,230.    Elapsed: 3:17:39.\n",
      "  Batch   890  of  1,230.    Elapsed: 3:19:56.\n",
      "  Batch   900  of  1,230.    Elapsed: 3:22:11.\n",
      "  Batch   910  of  1,230.    Elapsed: 3:24:27.\n",
      "  Batch   920  of  1,230.    Elapsed: 3:26:42.\n",
      "  Batch   930  of  1,230.    Elapsed: 3:28:57.\n",
      "  Batch   940  of  1,230.    Elapsed: 3:31:13.\n",
      "  Batch   950  of  1,230.    Elapsed: 3:33:27.\n",
      "  Batch   960  of  1,230.    Elapsed: 3:35:43.\n",
      "  Batch   970  of  1,230.    Elapsed: 3:37:59.\n",
      "  Batch   980  of  1,230.    Elapsed: 3:40:16.\n",
      "  Batch   990  of  1,230.    Elapsed: 3:42:32.\n",
      "  Batch 1,000  of  1,230.    Elapsed: 3:44:49.\n",
      "  Batch 1,010  of  1,230.    Elapsed: 3:47:05.\n",
      "  Batch 1,020  of  1,230.    Elapsed: 3:49:20.\n",
      "  Batch 1,030  of  1,230.    Elapsed: 3:51:36.\n",
      "  Batch 1,040  of  1,230.    Elapsed: 3:53:53.\n",
      "  Batch 1,050  of  1,230.    Elapsed: 3:56:09.\n",
      "  Batch 1,060  of  1,230.    Elapsed: 3:58:24.\n",
      "  Batch 1,070  of  1,230.    Elapsed: 4:00:40.\n",
      "  Batch 1,080  of  1,230.    Elapsed: 4:02:55.\n",
      "  Batch 1,090  of  1,230.    Elapsed: 4:05:11.\n",
      "  Batch 1,100  of  1,230.    Elapsed: 4:07:27.\n",
      "  Batch 1,110  of  1,230.    Elapsed: 4:09:42.\n",
      "  Batch 1,120  of  1,230.    Elapsed: 4:11:57.\n",
      "  Batch 1,130  of  1,230.    Elapsed: 4:14:13.\n",
      "  Batch 1,140  of  1,230.    Elapsed: 4:16:29.\n",
      "  Batch 1,150  of  1,230.    Elapsed: 4:18:44.\n",
      "  Batch 1,160  of  1,230.    Elapsed: 4:20:58.\n",
      "  Batch 1,170  of  1,230.    Elapsed: 4:23:12.\n",
      "  Batch 1,180  of  1,230.    Elapsed: 4:25:26.\n",
      "  Batch 1,190  of  1,230.    Elapsed: 4:27:39.\n",
      "  Batch 1,200  of  1,230.    Elapsed: 4:29:54.\n",
      "  Batch 1,210  of  1,230.    Elapsed: 4:32:08.\n",
      "  Batch 1,220  of  1,230.    Elapsed: 4:34:23.\n",
      "Train loss: 0.1120520415496293\n",
      "Validation loss: 0.09837092554373462\n",
      "Validation Accuracy: 0.24350299538844558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 10/10 [48:20:27<00:00, 17402.76s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.899932572383748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "max_grad_norm = 1.0\n",
    "total_t0 = time.time()\n",
    "\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    t0 = time.time()\n",
    "    # TRAIN loop\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        \n",
    "        if step % 10 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "            \n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # forward pass\n",
    "        loss, something_else = model(b_input_ids, token_type_ids=None,\n",
    "                     attention_mask=b_input_mask, labels=b_labels)\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # track train loss\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "    # print train loss per epoch\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    # VALIDATION on validation set\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions , true_labels = [], []\n",
    "    for batch in valid_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n",
    "                                  attention_mask=b_input_mask, labels=b_labels)[0]\n",
    "            logits = model(b_input_ids, token_type_ids=None,\n",
    "                           attention_mask=b_input_mask)[0]\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        true_labels.append(label_ids)\n",
    "        \n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        \n",
    "        nb_eval_examples += b_input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "    eval_loss = eval_loss/nb_eval_steps\n",
    "    print(\"Validation loss: {}\".format(eval_loss))\n",
    "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "    pred_tags = [tags_val[p_i] for p in predictions for p_i in p]\n",
    "    valid_tags = [tags_val[l_ii] for l in true_labels for l_i in l for l_ii in l_i]\n",
    "    print(\"F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "for batch in valid_dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n",
    "                              attention_mask=b_input_mask, labels=b_labels)[0]\n",
    "        logits = model(b_input_ids, token_type_ids=None,\n",
    "                       attention_mask=b_input_mask)[0]\n",
    "        \n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    true_labels.append(label_ids)\n",
    "#     tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "\n",
    "    eval_loss += tmp_eval_loss.mean().item()\n",
    "#     eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    nb_eval_examples += b_input_ids.size(0)\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "pred_tags = [[tags_val[p_i] for p_i in p] for p in predictions]\n",
    "valid_tags = [[tags_val[l_ii] for l_ii in l_i] for l in true_labels for l_i in l ]\n",
    "print(\"Validation loss: {}\".format(eval_loss/nb_eval_steps))\n",
    "# print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "print(\"Validation F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model, open('CamemBERT_POS', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Embeddings de doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import CamembertTokenizer, CamembertConfig\n",
    "from transformers import CamembertForTokenClassification\n",
    "from tqdm import tqdm, trange\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentences, labels = [], []\n",
    "with open(\"frwikinews-20130110-pages-articles.txt.tok.stanford-pos\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f.readlines():\n",
    "        sentence = []\n",
    "        sent_tag = []\n",
    "        tokens = line.replace(\"\\n\", \"\").split(\" \")\n",
    "        for token in tokens:\n",
    "            splits = token.split(\"_\")\n",
    "            if len(splits) != 2: continue\n",
    "            word, tag = splits\n",
    "            sentence.append(word)\n",
    "            sent_tag.append(tag)\n",
    "        sentences.append(\" \".join(sentence))\n",
    "        labels.append(sent_tag)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tags_val = list(set().union(*labels))\n",
    "tag2idx = {t:i for i,t in enumerate(tags_val)}\n",
    "# tag2idx[\"<PAD>\"] = len(tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 1733, 156.07599958838796)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = np.array(list(map(len, sentences)))\n",
    "lens.min(), lens.max(), lens.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 150\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tokenizer = CamembertTokenizer.from_pretrained('camembert-base/', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁à', '▁la', '▁suite', '▁de', '▁la', '▁parution', '▁le', '▁matin', '▁même', '▁d', \"'\", '▁un', '▁article', '▁2', '▁=', '▁le', '▁concernant', '▁dans', '▁le', '▁quotidien', '▁libération', '▁', ',', '▁christ', 'oph', 'e', '▁h', 'onde', 'la', 'tte', '▁décide', '▁de', '▁ne', '▁pas', '▁présenter', '▁le', '▁journal', '▁de', '▁13', '▁h', '▁00', '▁de', '▁france', '▁2', '▁', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "print(tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
    "                     maxlen=MAX_LEN, value=tag2idx[\"PONCT\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "attention_masks = [[float(i>0) for i in ii] for ii in input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags, \n",
    "                                                            random_state=2018, test_size=0.1)\n",
    "tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2018, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tr_inputs = torch.tensor(tr_inputs)\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "tr_tags = torch.tensor(tr_tags)\n",
    "val_tags = torch.tensor(val_tags)\n",
    "tr_masks = torch.tensor(tr_masks)\n",
    "val_masks = torch.tensor(val_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base/ were not used when initializing CamembertForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing CamembertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForTokenClassification were not initialized from the model checkpoint at camembert-base/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = CamembertForTokenClassification.from_pretrained(\"camembert-base/\", num_labels=len(tag2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "FULL_FINETUNING = True #True\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters()) \n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    10  of  1,230.    Elapsed: 0:02:31.\n",
      "  Batch    20  of  1,230.    Elapsed: 0:04:46.\n",
      "  Batch    30  of  1,230.    Elapsed: 0:07:00.\n",
      "  Batch    50  of  1,230.    Elapsed: 0:11:32.\n",
      "  Batch    60  of  1,230.    Elapsed: 0:13:45.\n",
      "  Batch    70  of  1,230.    Elapsed: 0:15:57.\n",
      "  Batch    80  of  1,230.    Elapsed: 0:18:15.\n",
      "  Batch    90  of  1,230.    Elapsed: 0:20:32.\n",
      "  Batch   100  of  1,230.    Elapsed: 0:22:43.\n",
      "  Batch   110  of  1,230.    Elapsed: 0:24:55.\n",
      "  Batch   120  of  1,230.    Elapsed: 0:27:06.\n",
      "  Batch   130  of  1,230.    Elapsed: 0:29:17.\n",
      "  Batch   140  of  1,230.    Elapsed: 0:31:28.\n",
      "  Batch   150  of  1,230.    Elapsed: 0:33:39.\n",
      "  Batch   160  of  1,230.    Elapsed: 0:35:50.\n",
      "  Batch   170  of  1,230.    Elapsed: 0:38:02.\n",
      "  Batch   180  of  1,230.    Elapsed: 0:40:14.\n",
      "  Batch   190  of  1,230.    Elapsed: 0:42:25.\n",
      "  Batch   200  of  1,230.    Elapsed: 0:44:36.\n",
      "  Batch   210  of  1,230.    Elapsed: 0:46:47.\n",
      "  Batch   220  of  1,230.    Elapsed: 0:48:59.\n",
      "  Batch   230  of  1,230.    Elapsed: 0:51:09.\n",
      "  Batch   240  of  1,230.    Elapsed: 0:53:20.\n",
      "  Batch   250  of  1,230.    Elapsed: 0:55:30.\n",
      "  Batch   260  of  1,230.    Elapsed: 0:57:42.\n",
      "  Batch   270  of  1,230.    Elapsed: 0:59:54.\n",
      "  Batch   280  of  1,230.    Elapsed: 1:02:06.\n",
      "  Batch   290  of  1,230.    Elapsed: 1:04:18.\n",
      "  Batch   300  of  1,230.    Elapsed: 1:06:29.\n",
      "  Batch   310  of  1,230.    Elapsed: 1:08:40.\n",
      "  Batch   320  of  1,230.    Elapsed: 1:10:51.\n",
      "  Batch   330  of  1,230.    Elapsed: 1:13:02.\n",
      "  Batch   340  of  1,230.    Elapsed: 1:15:13.\n",
      "  Batch   350  of  1,230.    Elapsed: 1:17:24.\n",
      "  Batch   360  of  1,230.    Elapsed: 1:19:37.\n",
      "  Batch   370  of  1,230.    Elapsed: 1:21:48.\n",
      "  Batch   380  of  1,230.    Elapsed: 1:23:59.\n",
      "  Batch   390  of  1,230.    Elapsed: 1:26:09.\n",
      "  Batch   400  of  1,230.    Elapsed: 1:28:19.\n",
      "  Batch   410  of  1,230.    Elapsed: 1:30:30.\n",
      "  Batch   420  of  1,230.    Elapsed: 1:32:41.\n",
      "  Batch   430  of  1,230.    Elapsed: 1:34:52.\n",
      "  Batch   440  of  1,230.    Elapsed: 1:37:04.\n",
      "  Batch   450  of  1,230.    Elapsed: 1:39:16.\n",
      "  Batch   460  of  1,230.    Elapsed: 1:41:28.\n",
      "  Batch   470  of  1,230.    Elapsed: 1:43:39.\n",
      "  Batch   480  of  1,230.    Elapsed: 1:45:52.\n",
      "  Batch   490  of  1,230.    Elapsed: 1:48:04.\n",
      "  Batch   500  of  1,230.    Elapsed: 1:50:16.\n",
      "  Batch   510  of  1,230.    Elapsed: 1:52:28.\n",
      "  Batch   520  of  1,230.    Elapsed: 1:54:42.\n",
      "  Batch   530  of  1,230.    Elapsed: 1:56:54.\n",
      "  Batch   540  of  1,230.    Elapsed: 1:59:06.\n",
      "  Batch   550  of  1,230.    Elapsed: 2:01:18.\n",
      "  Batch   560  of  1,230.    Elapsed: 2:03:30.\n",
      "  Batch   570  of  1,230.    Elapsed: 2:05:42.\n",
      "  Batch   580  of  1,230.    Elapsed: 2:07:55.\n",
      "  Batch   590  of  1,230.    Elapsed: 2:10:08.\n",
      "  Batch   600  of  1,230.    Elapsed: 2:12:21.\n",
      "  Batch   610  of  1,230.    Elapsed: 2:14:34.\n",
      "  Batch   620  of  1,230.    Elapsed: 2:16:46.\n",
      "  Batch   630  of  1,230.    Elapsed: 2:18:58.\n",
      "  Batch   640  of  1,230.    Elapsed: 2:21:10.\n",
      "  Batch   650  of  1,230.    Elapsed: 2:23:22.\n",
      "  Batch   660  of  1,230.    Elapsed: 2:25:34.\n",
      "  Batch   670  of  1,230.    Elapsed: 2:27:46.\n",
      "  Batch   680  of  1,230.    Elapsed: 2:29:58.\n",
      "  Batch   690  of  1,230.    Elapsed: 2:32:10.\n",
      "  Batch   700  of  1,230.    Elapsed: 2:34:22.\n",
      "  Batch   710  of  1,230.    Elapsed: 2:36:34.\n",
      "  Batch   720  of  1,230.    Elapsed: 2:38:44.\n",
      "  Batch   730  of  1,230.    Elapsed: 2:40:56.\n",
      "  Batch   740  of  1,230.    Elapsed: 2:43:09.\n",
      "  Batch   750  of  1,230.    Elapsed: 2:45:21.\n",
      "  Batch   760  of  1,230.    Elapsed: 2:47:32.\n",
      "  Batch   770  of  1,230.    Elapsed: 2:49:44.\n",
      "  Batch   780  of  1,230.    Elapsed: 2:51:57.\n",
      "  Batch   790  of  1,230.    Elapsed: 2:54:09.\n",
      "  Batch   800  of  1,230.    Elapsed: 2:56:21.\n",
      "  Batch   810  of  1,230.    Elapsed: 2:58:33.\n",
      "  Batch   820  of  1,230.    Elapsed: 3:00:45.\n",
      "  Batch   830  of  1,230.    Elapsed: 3:02:57.\n",
      "  Batch   840  of  1,230.    Elapsed: 3:05:09.\n",
      "  Batch   850  of  1,230.    Elapsed: 3:07:22.\n",
      "  Batch   860  of  1,230.    Elapsed: 3:09:34.\n",
      "  Batch   870  of  1,230.    Elapsed: 3:11:46.\n",
      "  Batch   880  of  1,230.    Elapsed: 3:13:59.\n",
      "  Batch   890  of  1,230.    Elapsed: 3:16:11.\n",
      "  Batch   900  of  1,230.    Elapsed: 3:18:22.\n",
      "  Batch   910  of  1,230.    Elapsed: 3:20:34.\n",
      "  Batch   920  of  1,230.    Elapsed: 3:22:46.\n",
      "  Batch   930  of  1,230.    Elapsed: 3:24:58.\n",
      "  Batch   940  of  1,230.    Elapsed: 3:27:10.\n",
      "  Batch   950  of  1,230.    Elapsed: 3:29:22.\n",
      "  Batch   960  of  1,230.    Elapsed: 3:31:34.\n",
      "  Batch   970  of  1,230.    Elapsed: 3:33:46.\n",
      "  Batch   980  of  1,230.    Elapsed: 3:35:58.\n",
      "  Batch   990  of  1,230.    Elapsed: 3:38:10.\n",
      "  Batch 1,000  of  1,230.    Elapsed: 3:40:23.\n",
      "  Batch 1,010  of  1,230.    Elapsed: 3:42:35.\n",
      "  Batch 1,020  of  1,230.    Elapsed: 3:44:47.\n",
      "  Batch 1,030  of  1,230.    Elapsed: 3:47:00.\n",
      "  Batch 1,040  of  1,230.    Elapsed: 3:49:12.\n",
      "  Batch 1,050  of  1,230.    Elapsed: 3:51:24.\n",
      "  Batch 1,060  of  1,230.    Elapsed: 3:53:36.\n",
      "  Batch 1,070  of  1,230.    Elapsed: 3:55:49.\n",
      "  Batch 1,080  of  1,230.    Elapsed: 3:58:01.\n",
      "  Batch 1,090  of  1,230.    Elapsed: 4:00:13.\n",
      "  Batch 1,100  of  1,230.    Elapsed: 4:02:26.\n",
      "  Batch 1,110  of  1,230.    Elapsed: 4:04:39.\n",
      "  Batch 1,120  of  1,230.    Elapsed: 4:06:52.\n",
      "  Batch 1,130  of  1,230.    Elapsed: 4:09:04.\n",
      "  Batch 1,140  of  1,230.    Elapsed: 4:11:17.\n",
      "  Batch 1,150  of  1,230.    Elapsed: 4:13:29.\n",
      "  Batch 1,160  of  1,230.    Elapsed: 4:15:44.\n",
      "  Batch 1,170  of  1,230.    Elapsed: 4:17:57.\n",
      "  Batch 1,180  of  1,230.    Elapsed: 4:20:09.\n",
      "  Batch 1,190  of  1,230.    Elapsed: 4:22:22.\n",
      "  Batch 1,200  of  1,230.    Elapsed: 4:24:34.\n",
      "  Batch 1,210  of  1,230.    Elapsed: 4:26:47.\n",
      "  Batch 1,220  of  1,230.    Elapsed: 4:28:59.\n",
      "Train loss: 1.5070774912834168\n",
      "Validation loss: 0.7504648927354465\n",
      "Validation Accuracy: 0.20958368698013924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  10%|█         | 1/10 [4:46:16<42:56:31, 17176.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.6678528399311532\n",
      "  Batch    10  of  1,230.    Elapsed: 0:02:13.\n",
      "  Batch    20  of  1,230.    Elapsed: 0:04:26.\n",
      "  Batch    30  of  1,230.    Elapsed: 0:06:39.\n",
      "  Batch    40  of  1,230.    Elapsed: 0:08:51.\n",
      "  Batch    50  of  1,230.    Elapsed: 0:11:03.\n",
      "  Batch    60  of  1,230.    Elapsed: 0:13:16.\n",
      "  Batch    70  of  1,230.    Elapsed: 0:15:26.\n",
      "  Batch    80  of  1,230.    Elapsed: 0:17:39.\n",
      "  Batch    90  of  1,230.    Elapsed: 0:19:50.\n",
      "  Batch   100  of  1,230.    Elapsed: 0:22:02.\n",
      "  Batch   110  of  1,230.    Elapsed: 0:24:14.\n",
      "  Batch   120  of  1,230.    Elapsed: 0:26:26.\n",
      "  Batch   130  of  1,230.    Elapsed: 0:28:38.\n",
      "  Batch   140  of  1,230.    Elapsed: 0:30:50.\n",
      "  Batch   150  of  1,230.    Elapsed: 0:33:01.\n",
      "  Batch   160  of  1,230.    Elapsed: 0:35:13.\n",
      "  Batch   170  of  1,230.    Elapsed: 0:37:25.\n",
      "  Batch   180  of  1,230.    Elapsed: 0:39:37.\n",
      "  Batch   190  of  1,230.    Elapsed: 0:41:50.\n",
      "  Batch   200  of  1,230.    Elapsed: 0:44:02.\n",
      "  Batch   210  of  1,230.    Elapsed: 0:46:15.\n",
      "  Batch   220  of  1,230.    Elapsed: 0:48:26.\n",
      "  Batch   230  of  1,230.    Elapsed: 0:50:38.\n",
      "  Batch   240  of  1,230.    Elapsed: 0:52:50.\n",
      "  Batch   250  of  1,230.    Elapsed: 0:55:02.\n",
      "  Batch   260  of  1,230.    Elapsed: 0:57:15.\n",
      "  Batch   270  of  1,230.    Elapsed: 0:59:27.\n",
      "  Batch   280  of  1,230.    Elapsed: 1:01:40.\n",
      "  Batch   290  of  1,230.    Elapsed: 1:03:51.\n",
      "  Batch   300  of  1,230.    Elapsed: 1:06:02.\n",
      "  Batch   310  of  1,230.    Elapsed: 1:08:14.\n",
      "  Batch   320  of  1,230.    Elapsed: 1:10:26.\n",
      "  Batch   330  of  1,230.    Elapsed: 1:12:38.\n",
      "  Batch   340  of  1,230.    Elapsed: 1:14:49.\n",
      "  Batch   350  of  1,230.    Elapsed: 1:17:02.\n",
      "  Batch   360  of  1,230.    Elapsed: 1:19:14.\n",
      "  Batch   370  of  1,230.    Elapsed: 1:21:26.\n",
      "  Batch   380  of  1,230.    Elapsed: 1:23:38.\n",
      "  Batch   390  of  1,230.    Elapsed: 1:25:50.\n",
      "  Batch   400  of  1,230.    Elapsed: 1:28:02.\n",
      "  Batch   410  of  1,230.    Elapsed: 1:30:14.\n",
      "  Batch   420  of  1,230.    Elapsed: 1:32:26.\n",
      "  Batch   430  of  1,230.    Elapsed: 1:34:38.\n",
      "  Batch   440  of  1,230.    Elapsed: 1:36:50.\n",
      "  Batch   450  of  1,230.    Elapsed: 1:39:02.\n",
      "  Batch   460  of  1,230.    Elapsed: 1:41:13.\n",
      "  Batch   470  of  1,230.    Elapsed: 1:43:25.\n",
      "  Batch   480  of  1,230.    Elapsed: 1:45:36.\n",
      "  Batch   490  of  1,230.    Elapsed: 1:47:52.\n",
      "  Batch   500  of  1,230.    Elapsed: 1:50:03.\n",
      "  Batch   510  of  1,230.    Elapsed: 1:52:14.\n",
      "  Batch   520  of  1,230.    Elapsed: 1:54:25.\n",
      "  Batch   530  of  1,230.    Elapsed: 1:56:37.\n",
      "  Batch   540  of  1,230.    Elapsed: 1:58:49.\n",
      "  Batch   550  of  1,230.    Elapsed: 2:01:00.\n",
      "  Batch   560  of  1,230.    Elapsed: 2:03:12.\n",
      "  Batch   570  of  1,230.    Elapsed: 2:05:25.\n",
      "  Batch   580  of  1,230.    Elapsed: 2:07:36.\n",
      "  Batch   590  of  1,230.    Elapsed: 2:09:49.\n",
      "  Batch   600  of  1,230.    Elapsed: 2:12:00.\n",
      "  Batch   610  of  1,230.    Elapsed: 2:14:13.\n",
      "  Batch   620  of  1,230.    Elapsed: 2:16:23.\n",
      "  Batch   630  of  1,230.    Elapsed: 2:18:35.\n",
      "  Batch   640  of  1,230.    Elapsed: 2:20:48.\n",
      "  Batch   650  of  1,230.    Elapsed: 2:23:00.\n",
      "  Batch   660  of  1,230.    Elapsed: 2:25:12.\n",
      "  Batch   670  of  1,230.    Elapsed: 2:27:23.\n",
      "  Batch   680  of  1,230.    Elapsed: 2:29:36.\n",
      "  Batch   690  of  1,230.    Elapsed: 2:31:49.\n",
      "  Batch   700  of  1,230.    Elapsed: 2:34:01.\n",
      "  Batch   710  of  1,230.    Elapsed: 2:36:11.\n",
      "  Batch   720  of  1,230.    Elapsed: 2:38:23.\n",
      "  Batch   730  of  1,230.    Elapsed: 2:40:35.\n",
      "  Batch   740  of  1,230.    Elapsed: 2:42:46.\n",
      "  Batch   750  of  1,230.    Elapsed: 2:44:56.\n",
      "  Batch   760  of  1,230.    Elapsed: 2:47:07.\n",
      "  Batch   770  of  1,230.    Elapsed: 2:49:19.\n",
      "  Batch   780  of  1,230.    Elapsed: 2:51:30.\n",
      "  Batch   790  of  1,230.    Elapsed: 2:53:41.\n",
      "  Batch   800  of  1,230.    Elapsed: 2:55:53.\n",
      "  Batch   810  of  1,230.    Elapsed: 2:58:04.\n",
      "  Batch   820  of  1,230.    Elapsed: 3:00:16.\n",
      "  Batch   830  of  1,230.    Elapsed: 3:02:28.\n",
      "  Batch   840  of  1,230.    Elapsed: 3:04:39.\n",
      "  Batch   850  of  1,230.    Elapsed: 3:06:51.\n",
      "  Batch   860  of  1,230.    Elapsed: 3:09:04.\n",
      "  Batch   870  of  1,230.    Elapsed: 3:11:15.\n",
      "  Batch   880  of  1,230.    Elapsed: 3:13:25.\n",
      "  Batch   890  of  1,230.    Elapsed: 3:15:37.\n",
      "  Batch   900  of  1,230.    Elapsed: 3:17:48.\n",
      "  Batch   910  of  1,230.    Elapsed: 3:19:59.\n",
      "  Batch   920  of  1,230.    Elapsed: 3:22:11.\n",
      "  Batch   930  of  1,230.    Elapsed: 3:24:23.\n",
      "  Batch   940  of  1,230.    Elapsed: 3:26:34.\n",
      "  Batch   950  of  1,230.    Elapsed: 3:28:44.\n",
      "  Batch   960  of  1,230.    Elapsed: 3:30:56.\n",
      "  Batch   970  of  1,230.    Elapsed: 3:33:08.\n",
      "  Batch   980  of  1,230.    Elapsed: 3:35:19.\n",
      "  Batch   990  of  1,230.    Elapsed: 3:37:31.\n",
      "  Batch 1,000  of  1,230.    Elapsed: 3:39:43.\n",
      "  Batch 1,010  of  1,230.    Elapsed: 3:41:55.\n",
      "  Batch 1,020  of  1,230.    Elapsed: 3:44:06.\n",
      "  Batch 1,030  of  1,230.    Elapsed: 3:46:18.\n",
      "  Batch 1,040  of  1,230.    Elapsed: 3:48:29.\n",
      "  Batch 1,050  of  1,230.    Elapsed: 3:50:40.\n",
      "  Batch 1,060  of  1,230.    Elapsed: 3:52:50.\n",
      "  Batch 1,070  of  1,230.    Elapsed: 3:55:01.\n",
      "  Batch 1,080  of  1,230.    Elapsed: 3:57:13.\n",
      "  Batch 1,090  of  1,230.    Elapsed: 3:59:26.\n",
      "  Batch 1,100  of  1,230.    Elapsed: 4:01:38.\n",
      "  Batch 1,110  of  1,230.    Elapsed: 4:03:51.\n",
      "  Batch 1,120  of  1,230.    Elapsed: 4:06:03.\n",
      "  Batch 1,130  of  1,230.    Elapsed: 4:08:16.\n",
      "  Batch 1,140  of  1,230.    Elapsed: 4:10:30.\n",
      "  Batch 1,150  of  1,230.    Elapsed: 4:12:44.\n",
      "  Batch 1,160  of  1,230.    Elapsed: 4:14:57.\n",
      "  Batch 1,170  of  1,230.    Elapsed: 4:17:10.\n",
      "  Batch 1,180  of  1,230.    Elapsed: 4:19:23.\n",
      "  Batch 1,190  of  1,230.    Elapsed: 4:21:35.\n",
      "  Batch 1,200  of  1,230.    Elapsed: 4:23:49.\n",
      "  Batch 1,210  of  1,230.    Elapsed: 4:26:02.\n",
      "  Batch 1,220  of  1,230.    Elapsed: 4:28:15.\n",
      "Train loss: 0.6372126102689805\n",
      "Validation loss: 0.7680652433068212\n",
      "Validation Accuracy: 0.19712785746619133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  20%|██        | 2/10 [9:32:01<38:08:58, 17167.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.6456441531748572\n",
      "  Batch    10  of  1,230.    Elapsed: 0:02:14.\n",
      "  Batch    20  of  1,230.    Elapsed: 0:04:27.\n",
      "  Batch    30  of  1,230.    Elapsed: 0:06:40.\n",
      "  Batch    40  of  1,230.    Elapsed: 0:08:54.\n",
      "  Batch    50  of  1,230.    Elapsed: 0:11:07.\n",
      "  Batch    60  of  1,230.    Elapsed: 0:13:22.\n",
      "  Batch    70  of  1,230.    Elapsed: 0:15:37.\n",
      "  Batch    80  of  1,230.    Elapsed: 0:17:51.\n",
      "  Batch    90  of  1,230.    Elapsed: 0:20:06.\n",
      "  Batch   100  of  1,230.    Elapsed: 0:22:21.\n",
      "  Batch   110  of  1,230.    Elapsed: 0:24:37.\n",
      "  Batch   120  of  1,230.    Elapsed: 0:26:50.\n",
      "  Batch   130  of  1,230.    Elapsed: 0:29:03.\n",
      "  Batch   140  of  1,230.    Elapsed: 0:31:15.\n",
      "  Batch   150  of  1,230.    Elapsed: 0:33:28.\n",
      "  Batch   160  of  1,230.    Elapsed: 0:35:41.\n",
      "  Batch   170  of  1,230.    Elapsed: 0:37:53.\n",
      "  Batch   180  of  1,230.    Elapsed: 0:40:07.\n",
      "  Batch   190  of  1,230.    Elapsed: 0:42:21.\n",
      "  Batch   200  of  1,230.    Elapsed: 0:44:35.\n",
      "  Batch   210  of  1,230.    Elapsed: 0:46:50.\n",
      "  Batch   220  of  1,230.    Elapsed: 0:49:04.\n",
      "  Batch   230  of  1,230.    Elapsed: 0:51:18.\n",
      "  Batch   240  of  1,230.    Elapsed: 0:53:31.\n",
      "  Batch   250  of  1,230.    Elapsed: 0:55:44.\n",
      "  Batch   260  of  1,230.    Elapsed: 0:57:58.\n",
      "  Batch   270  of  1,230.    Elapsed: 1:00:12.\n",
      "  Batch   280  of  1,230.    Elapsed: 1:02:25.\n",
      "  Batch   290  of  1,230.    Elapsed: 1:04:39.\n",
      "  Batch   300  of  1,230.    Elapsed: 1:06:53.\n",
      "  Batch   310  of  1,230.    Elapsed: 1:09:07.\n",
      "  Batch   320  of  1,230.    Elapsed: 1:11:21.\n",
      "  Batch   330  of  1,230.    Elapsed: 1:13:35.\n",
      "  Batch   340  of  1,230.    Elapsed: 1:15:49.\n",
      "  Batch   350  of  1,230.    Elapsed: 1:18:03.\n",
      "  Batch   360  of  1,230.    Elapsed: 1:20:17.\n",
      "  Batch   370  of  1,230.    Elapsed: 1:22:31.\n",
      "  Batch   380  of  1,230.    Elapsed: 1:24:45.\n",
      "  Batch   390  of  1,230.    Elapsed: 1:26:59.\n",
      "  Batch   400  of  1,230.    Elapsed: 1:29:13.\n",
      "  Batch   410  of  1,230.    Elapsed: 1:31:28.\n",
      "  Batch   420  of  1,230.    Elapsed: 1:33:43.\n",
      "  Batch   430  of  1,230.    Elapsed: 1:35:56.\n",
      "  Batch   440  of  1,230.    Elapsed: 1:38:11.\n",
      "  Batch   450  of  1,230.    Elapsed: 1:40:25.\n",
      "  Batch   460  of  1,230.    Elapsed: 1:42:40.\n",
      "  Batch   470  of  1,230.    Elapsed: 1:44:52.\n",
      "  Batch   480  of  1,230.    Elapsed: 1:47:07.\n",
      "  Batch   490  of  1,230.    Elapsed: 1:49:21.\n",
      "  Batch   500  of  1,230.    Elapsed: 1:51:34.\n",
      "  Batch   510  of  1,230.    Elapsed: 1:53:50.\n",
      "  Batch   520  of  1,230.    Elapsed: 1:56:04.\n",
      "  Batch   530  of  1,230.    Elapsed: 1:58:17.\n",
      "  Batch   540  of  1,230.    Elapsed: 2:00:31.\n",
      "  Batch   550  of  1,230.    Elapsed: 2:02:45.\n",
      "  Batch   560  of  1,230.    Elapsed: 2:04:58.\n",
      "  Batch   570  of  1,230.    Elapsed: 2:07:13.\n",
      "  Batch   580  of  1,230.    Elapsed: 2:09:29.\n",
      "  Batch   590  of  1,230.    Elapsed: 2:11:43.\n",
      "  Batch   600  of  1,230.    Elapsed: 2:13:57.\n",
      "  Batch   610  of  1,230.    Elapsed: 2:16:12.\n",
      "  Batch   620  of  1,230.    Elapsed: 2:18:26.\n",
      "  Batch   630  of  1,230.    Elapsed: 2:20:39.\n",
      "  Batch   640  of  1,230.    Elapsed: 2:22:54.\n",
      "  Batch   650  of  1,230.    Elapsed: 2:25:07.\n",
      "  Batch   660  of  1,230.    Elapsed: 2:27:21.\n",
      "  Batch   670  of  1,230.    Elapsed: 2:29:35.\n",
      "  Batch   680  of  1,230.    Elapsed: 2:31:48.\n",
      "  Batch   690  of  1,230.    Elapsed: 2:34:04.\n",
      "  Batch   700  of  1,230.    Elapsed: 2:36:19.\n",
      "  Batch   710  of  1,230.    Elapsed: 2:38:31.\n",
      "  Batch   720  of  1,230.    Elapsed: 2:40:45.\n",
      "  Batch   730  of  1,230.    Elapsed: 2:42:58.\n",
      "  Batch   740  of  1,230.    Elapsed: 2:45:12.\n",
      "  Batch   750  of  1,230.    Elapsed: 2:47:26.\n",
      "  Batch   760  of  1,230.    Elapsed: 2:49:42.\n",
      "  Batch   770  of  1,230.    Elapsed: 2:51:56.\n",
      "  Batch   780  of  1,230.    Elapsed: 2:54:09.\n",
      "  Batch   790  of  1,230.    Elapsed: 2:56:23.\n",
      "  Batch   800  of  1,230.    Elapsed: 2:58:36.\n",
      "  Batch   810  of  1,230.    Elapsed: 3:00:50.\n",
      "  Batch   820  of  1,230.    Elapsed: 3:03:04.\n",
      "  Batch   830  of  1,230.    Elapsed: 3:05:17.\n",
      "  Batch   840  of  1,230.    Elapsed: 3:07:31.\n",
      "  Batch   850  of  1,230.    Elapsed: 3:09:44.\n",
      "  Batch   860  of  1,230.    Elapsed: 3:11:59.\n",
      "  Batch   870  of  1,230.    Elapsed: 3:14:13.\n",
      "  Batch   880  of  1,230.    Elapsed: 3:16:27.\n",
      "  Batch   890  of  1,230.    Elapsed: 3:18:42.\n",
      "  Batch   900  of  1,230.    Elapsed: 3:20:56.\n",
      "  Batch   910  of  1,230.    Elapsed: 3:23:11.\n",
      "  Batch   920  of  1,230.    Elapsed: 3:25:25.\n",
      "  Batch   930  of  1,230.    Elapsed: 3:27:38.\n",
      "  Batch   940  of  1,230.    Elapsed: 3:29:52.\n",
      "  Batch   950  of  1,230.    Elapsed: 3:32:05.\n",
      "  Batch   960  of  1,230.    Elapsed: 3:34:18.\n",
      "  Batch   970  of  1,230.    Elapsed: 3:36:31.\n",
      "  Batch   980  of  1,230.    Elapsed: 3:38:45.\n",
      "  Batch   990  of  1,230.    Elapsed: 3:41:00.\n",
      "  Batch 1,000  of  1,230.    Elapsed: 3:43:14.\n",
      "  Batch 1,010  of  1,230.    Elapsed: 3:45:27.\n",
      "  Batch 1,020  of  1,230.    Elapsed: 3:47:41.\n",
      "  Batch 1,030  of  1,230.    Elapsed: 3:49:54.\n",
      "  Batch 1,040  of  1,230.    Elapsed: 3:52:09.\n",
      "  Batch 1,050  of  1,230.    Elapsed: 3:54:25.\n",
      "  Batch 1,060  of  1,230.    Elapsed: 3:56:41.\n",
      "  Batch 1,070  of  1,230.    Elapsed: 3:58:57.\n",
      "  Batch 1,080  of  1,230.    Elapsed: 4:01:12.\n",
      "  Batch 1,090  of  1,230.    Elapsed: 4:03:27.\n",
      "  Batch 1,100  of  1,230.    Elapsed: 4:05:41.\n",
      "  Batch 1,110  of  1,230.    Elapsed: 4:07:56.\n",
      "  Batch 1,120  of  1,230.    Elapsed: 4:10:12.\n",
      "  Batch 1,130  of  1,230.    Elapsed: 4:12:25.\n",
      "  Batch 1,140  of  1,230.    Elapsed: 4:14:41.\n",
      "  Batch 1,150  of  1,230.    Elapsed: 4:16:54.\n",
      "  Batch 1,160  of  1,230.    Elapsed: 4:19:10.\n",
      "  Batch 1,170  of  1,230.    Elapsed: 4:21:25.\n",
      "  Batch 1,180  of  1,230.    Elapsed: 4:23:40.\n",
      "  Batch 1,190  of  1,230.    Elapsed: 4:25:53.\n",
      "  Batch 1,200  of  1,230.    Elapsed: 4:28:08.\n",
      "  Batch 1,210  of  1,230.    Elapsed: 4:30:23.\n",
      "  Batch 1,220  of  1,230.    Elapsed: 4:32:37.\n",
      "Train loss: 0.3921777993077185\n",
      "Validation loss: 0.24186624412554025\n",
      "Validation Accuracy: 0.2336084882306343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  30%|███       | 3/10 [14:22:30<33:32:00, 17245.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.8512631076728391\n",
      "  Batch    10  of  1,230.    Elapsed: 0:02:13.\n",
      "  Batch    20  of  1,230.    Elapsed: 0:04:25.\n",
      "  Batch    30  of  1,230.    Elapsed: 0:06:39.\n",
      "  Batch    40  of  1,230.    Elapsed: 0:08:53.\n",
      "  Batch    50  of  1,230.    Elapsed: 0:11:06.\n",
      "  Batch    60  of  1,230.    Elapsed: 0:13:20.\n",
      "  Batch    70  of  1,230.    Elapsed: 0:15:36.\n",
      "  Batch    80  of  1,230.    Elapsed: 0:17:50.\n",
      "  Batch    90  of  1,230.    Elapsed: 0:20:04.\n",
      "  Batch   100  of  1,230.    Elapsed: 0:22:18.\n",
      "  Batch   110  of  1,230.    Elapsed: 0:24:32.\n",
      "  Batch   120  of  1,230.    Elapsed: 0:26:46.\n",
      "  Batch   130  of  1,230.    Elapsed: 0:29:02.\n",
      "  Batch   140  of  1,230.    Elapsed: 0:31:15.\n",
      "  Batch   150  of  1,230.    Elapsed: 0:33:31.\n",
      "  Batch   160  of  1,230.    Elapsed: 0:35:45.\n",
      "  Batch   170  of  1,230.    Elapsed: 0:37:58.\n",
      "  Batch   180  of  1,230.    Elapsed: 0:40:12.\n",
      "  Batch   190  of  1,230.    Elapsed: 0:42:26.\n",
      "  Batch   200  of  1,230.    Elapsed: 0:44:39.\n",
      "  Batch   210  of  1,230.    Elapsed: 0:46:53.\n",
      "  Batch   220  of  1,230.    Elapsed: 0:49:07.\n",
      "  Batch   230  of  1,230.    Elapsed: 0:51:22.\n",
      "  Batch   240  of  1,230.    Elapsed: 0:53:36.\n",
      "  Batch   250  of  1,230.    Elapsed: 0:55:50.\n",
      "  Batch   260  of  1,230.    Elapsed: 0:58:06.\n",
      "  Batch   270  of  1,230.    Elapsed: 1:00:20.\n",
      "  Batch   280  of  1,230.    Elapsed: 1:02:34.\n",
      "  Batch   290  of  1,230.    Elapsed: 1:04:48.\n",
      "  Batch   300  of  1,230.    Elapsed: 1:07:02.\n",
      "  Batch   310  of  1,230.    Elapsed: 1:09:16.\n",
      "  Batch   320  of  1,230.    Elapsed: 1:11:30.\n",
      "  Batch   330  of  1,230.    Elapsed: 1:13:45.\n",
      "  Batch   340  of  1,230.    Elapsed: 1:15:57.\n",
      "  Batch   350  of  1,230.    Elapsed: 1:18:12.\n",
      "  Batch   360  of  1,230.    Elapsed: 1:20:27.\n",
      "  Batch   370  of  1,230.    Elapsed: 1:22:43.\n",
      "  Batch   380  of  1,230.    Elapsed: 1:24:58.\n",
      "  Batch   390  of  1,230.    Elapsed: 1:27:13.\n",
      "  Batch   400  of  1,230.    Elapsed: 1:29:29.\n",
      "  Batch   410  of  1,230.    Elapsed: 1:31:44.\n",
      "  Batch   420  of  1,230.    Elapsed: 1:33:59.\n",
      "  Batch   430  of  1,230.    Elapsed: 1:36:12.\n",
      "  Batch   440  of  1,230.    Elapsed: 1:38:28.\n",
      "  Batch   450  of  1,230.    Elapsed: 1:40:43.\n",
      "  Batch   460  of  1,230.    Elapsed: 1:42:57.\n",
      "  Batch   470  of  1,230.    Elapsed: 1:45:12.\n",
      "  Batch   480  of  1,230.    Elapsed: 1:47:26.\n",
      "  Batch   490  of  1,230.    Elapsed: 1:49:40.\n",
      "  Batch   500  of  1,230.    Elapsed: 1:51:54.\n",
      "  Batch   510  of  1,230.    Elapsed: 1:54:08.\n",
      "  Batch   520  of  1,230.    Elapsed: 1:56:22.\n",
      "  Batch   530  of  1,230.    Elapsed: 1:58:37.\n",
      "  Batch   540  of  1,230.    Elapsed: 2:00:51.\n",
      "  Batch   550  of  1,230.    Elapsed: 2:03:06.\n",
      "  Batch   560  of  1,230.    Elapsed: 2:05:20.\n",
      "  Batch   570  of  1,230.    Elapsed: 2:07:35.\n",
      "  Batch   580  of  1,230.    Elapsed: 2:09:50.\n",
      "  Batch   590  of  1,230.    Elapsed: 2:12:04.\n",
      "  Batch   600  of  1,230.    Elapsed: 2:14:19.\n",
      "  Batch   610  of  1,230.    Elapsed: 2:16:32.\n",
      "  Batch   620  of  1,230.    Elapsed: 2:18:46.\n",
      "  Batch   630  of  1,230.    Elapsed: 2:20:59.\n",
      "  Batch   640  of  1,230.    Elapsed: 2:23:13.\n",
      "  Batch   650  of  1,230.    Elapsed: 2:25:27.\n",
      "  Batch   660  of  1,230.    Elapsed: 2:27:42.\n",
      "  Batch   670  of  1,230.    Elapsed: 2:29:57.\n",
      "  Batch   680  of  1,230.    Elapsed: 2:32:12.\n",
      "  Batch   690  of  1,230.    Elapsed: 2:34:26.\n",
      "  Batch   700  of  1,230.    Elapsed: 2:36:41.\n",
      "  Batch   710  of  1,230.    Elapsed: 2:38:55.\n",
      "  Batch   720  of  1,230.    Elapsed: 2:41:10.\n",
      "  Batch   730  of  1,230.    Elapsed: 2:43:23.\n",
      "  Batch   740  of  1,230.    Elapsed: 2:45:38.\n",
      "  Batch   750  of  1,230.    Elapsed: 2:47:53.\n",
      "  Batch   760  of  1,230.    Elapsed: 2:50:07.\n",
      "  Batch   770  of  1,230.    Elapsed: 2:52:21.\n",
      "  Batch   780  of  1,230.    Elapsed: 2:54:36.\n",
      "  Batch   790  of  1,230.    Elapsed: 2:56:50.\n",
      "  Batch   800  of  1,230.    Elapsed: 2:59:04.\n",
      "  Batch   810  of  1,230.    Elapsed: 3:01:18.\n",
      "  Batch   820  of  1,230.    Elapsed: 3:03:31.\n",
      "  Batch   830  of  1,230.    Elapsed: 3:05:44.\n",
      "  Batch   840  of  1,230.    Elapsed: 3:07:58.\n",
      "  Batch   850  of  1,230.    Elapsed: 3:10:11.\n",
      "  Batch   860  of  1,230.    Elapsed: 3:12:25.\n",
      "  Batch   870  of  1,230.    Elapsed: 3:14:38.\n",
      "  Batch   880  of  1,230.    Elapsed: 3:16:52.\n",
      "  Batch   890  of  1,230.    Elapsed: 3:19:07.\n",
      "  Batch   900  of  1,230.    Elapsed: 3:21:20.\n",
      "  Batch   910  of  1,230.    Elapsed: 3:23:35.\n",
      "  Batch   920  of  1,230.    Elapsed: 3:25:49.\n",
      "  Batch   930  of  1,230.    Elapsed: 3:28:04.\n",
      "  Batch   940  of  1,230.    Elapsed: 3:30:18.\n",
      "  Batch   950  of  1,230.    Elapsed: 3:32:32.\n",
      "  Batch   960  of  1,230.    Elapsed: 3:34:45.\n",
      "  Batch   970  of  1,230.    Elapsed: 3:36:59.\n",
      "  Batch   980  of  1,230.    Elapsed: 3:39:13.\n",
      "  Batch   990  of  1,230.    Elapsed: 3:41:28.\n",
      "  Batch 1,000  of  1,230.    Elapsed: 3:43:43.\n",
      "  Batch 1,010  of  1,230.    Elapsed: 3:45:57.\n",
      "  Batch 1,020  of  1,230.    Elapsed: 3:48:10.\n",
      "  Batch 1,030  of  1,230.    Elapsed: 3:50:25.\n",
      "  Batch 1,040  of  1,230.    Elapsed: 3:52:39.\n",
      "  Batch 1,050  of  1,230.    Elapsed: 3:54:53.\n",
      "  Batch 1,060  of  1,230.    Elapsed: 3:57:07.\n",
      "  Batch 1,070  of  1,230.    Elapsed: 3:59:21.\n",
      "  Batch 1,080  of  1,230.    Elapsed: 4:01:35.\n",
      "  Batch 1,090  of  1,230.    Elapsed: 4:03:49.\n",
      "  Batch 1,100  of  1,230.    Elapsed: 4:06:03.\n",
      "  Batch 1,110  of  1,230.    Elapsed: 4:08:16.\n",
      "  Batch 1,120  of  1,230.    Elapsed: 4:10:30.\n",
      "  Batch 1,130  of  1,230.    Elapsed: 4:12:45.\n",
      "  Batch 1,140  of  1,230.    Elapsed: 4:14:59.\n",
      "  Batch 1,150  of  1,230.    Elapsed: 4:17:13.\n",
      "  Batch 1,160  of  1,230.    Elapsed: 4:19:27.\n",
      "  Batch 1,170  of  1,230.    Elapsed: 4:21:41.\n",
      "  Batch 1,180  of  1,230.    Elapsed: 4:23:56.\n",
      "  Batch 1,190  of  1,230.    Elapsed: 4:26:09.\n",
      "  Batch 1,200  of  1,230.    Elapsed: 4:28:23.\n",
      "  Batch 1,210  of  1,230.    Elapsed: 4:30:38.\n",
      "  Batch 1,220  of  1,230.    Elapsed: 4:32:52.\n",
      "Train loss: 0.2821777369191007\n",
      "Validation loss: 0.21323060010471484\n",
      "Validation Accuracy: 0.23461974127199678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  40%|████      | 4/10 [19:13:06<28:50:16, 17302.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.8555478903830289\n",
      "  Batch    10  of  1,230.    Elapsed: 0:02:14.\n",
      "  Batch    20  of  1,230.    Elapsed: 0:04:29.\n",
      "  Batch    30  of  1,230.    Elapsed: 0:06:44.\n",
      "  Batch    40  of  1,230.    Elapsed: 0:08:58.\n",
      "  Batch    50  of  1,230.    Elapsed: 0:11:13.\n",
      "  Batch    60  of  1,230.    Elapsed: 0:13:25.\n",
      "  Batch    70  of  1,230.    Elapsed: 0:15:39.\n",
      "  Batch    80  of  1,230.    Elapsed: 0:17:55.\n",
      "  Batch    90  of  1,230.    Elapsed: 0:20:09.\n",
      "  Batch   100  of  1,230.    Elapsed: 0:22:24.\n",
      "  Batch   110  of  1,230.    Elapsed: 0:24:39.\n",
      "  Batch   120  of  1,230.    Elapsed: 0:26:55.\n",
      "  Batch   130  of  1,230.    Elapsed: 0:29:09.\n",
      "  Batch   140  of  1,230.    Elapsed: 0:31:23.\n",
      "  Batch   150  of  1,230.    Elapsed: 0:33:36.\n",
      "  Batch   160  of  1,230.    Elapsed: 0:35:50.\n",
      "  Batch   170  of  1,230.    Elapsed: 0:38:05.\n",
      "  Batch   180  of  1,230.    Elapsed: 0:40:20.\n",
      "  Batch   190  of  1,230.    Elapsed: 0:42:34.\n",
      "  Batch   200  of  1,230.    Elapsed: 0:44:48.\n",
      "  Batch   210  of  1,230.    Elapsed: 0:47:02.\n",
      "  Batch   220  of  1,230.    Elapsed: 0:49:17.\n",
      "  Batch   230  of  1,230.    Elapsed: 0:51:32.\n",
      "  Batch   240  of  1,230.    Elapsed: 0:53:45.\n",
      "  Batch   250  of  1,230.    Elapsed: 0:55:59.\n",
      "  Batch   260  of  1,230.    Elapsed: 0:58:14.\n",
      "  Batch   270  of  1,230.    Elapsed: 1:00:29.\n",
      "  Batch   280  of  1,230.    Elapsed: 1:02:42.\n",
      "  Batch   290  of  1,230.    Elapsed: 1:04:56.\n",
      "  Batch   300  of  1,230.    Elapsed: 1:07:11.\n",
      "  Batch   310  of  1,230.    Elapsed: 1:09:26.\n",
      "  Batch   320  of  1,230.    Elapsed: 1:11:41.\n",
      "  Batch   330  of  1,230.    Elapsed: 1:13:54.\n",
      "  Batch   340  of  1,230.    Elapsed: 1:16:08.\n",
      "  Batch   350  of  1,230.    Elapsed: 1:18:23.\n",
      "  Batch   360  of  1,230.    Elapsed: 1:20:37.\n",
      "  Batch   370  of  1,230.    Elapsed: 1:22:53.\n",
      "  Batch   380  of  1,230.    Elapsed: 1:25:08.\n",
      "  Batch   390  of  1,230.    Elapsed: 1:27:22.\n",
      "  Batch   400  of  1,230.    Elapsed: 1:29:37.\n",
      "  Batch   410  of  1,230.    Elapsed: 1:31:52.\n",
      "  Batch   420  of  1,230.    Elapsed: 1:34:06.\n",
      "  Batch   430  of  1,230.    Elapsed: 1:36:20.\n",
      "  Batch   440  of  1,230.    Elapsed: 1:38:35.\n",
      "  Batch   450  of  1,230.    Elapsed: 1:40:49.\n",
      "  Batch   460  of  1,230.    Elapsed: 1:43:04.\n",
      "  Batch   470  of  1,230.    Elapsed: 1:45:19.\n",
      "  Batch   480  of  1,230.    Elapsed: 1:47:32.\n",
      "  Batch   490  of  1,230.    Elapsed: 1:49:46.\n",
      "  Batch   500  of  1,230.    Elapsed: 1:51:59.\n",
      "  Batch   510  of  1,230.    Elapsed: 1:54:14.\n",
      "  Batch   520  of  1,230.    Elapsed: 1:56:28.\n",
      "  Batch   530  of  1,230.    Elapsed: 1:58:43.\n",
      "  Batch   540  of  1,230.    Elapsed: 2:00:58.\n",
      "  Batch   550  of  1,230.    Elapsed: 2:03:12.\n",
      "  Batch   560  of  1,230.    Elapsed: 2:05:27.\n",
      "  Batch   570  of  1,230.    Elapsed: 2:07:41.\n",
      "  Batch   580  of  1,230.    Elapsed: 2:09:55.\n",
      "  Batch   590  of  1,230.    Elapsed: 2:12:10.\n",
      "  Batch   600  of  1,230.    Elapsed: 2:14:24.\n",
      "  Batch   610  of  1,230.    Elapsed: 2:16:38.\n",
      "  Batch   620  of  1,230.    Elapsed: 2:18:52.\n",
      "  Batch   630  of  1,230.    Elapsed: 2:21:07.\n",
      "  Batch   640  of  1,230.    Elapsed: 2:23:18.\n",
      "  Batch   650  of  1,230.    Elapsed: 2:25:32.\n",
      "  Batch   660  of  1,230.    Elapsed: 2:27:45.\n",
      "  Batch   670  of  1,230.    Elapsed: 2:29:58.\n",
      "  Batch   680  of  1,230.    Elapsed: 2:32:13.\n",
      "  Batch   690  of  1,230.    Elapsed: 2:34:27.\n",
      "  Batch   700  of  1,230.    Elapsed: 2:36:41.\n",
      "  Batch   710  of  1,230.    Elapsed: 2:38:55.\n",
      "  Batch   720  of  1,230.    Elapsed: 2:41:10.\n",
      "  Batch   730  of  1,230.    Elapsed: 2:43:24.\n",
      "  Batch   740  of  1,230.    Elapsed: 2:45:38.\n",
      "  Batch   750  of  1,230.    Elapsed: 2:47:52.\n",
      "  Batch   760  of  1,230.    Elapsed: 2:50:06.\n",
      "  Batch   770  of  1,230.    Elapsed: 2:52:20.\n",
      "  Batch   780  of  1,230.    Elapsed: 2:54:35.\n",
      "  Batch   790  of  1,230.    Elapsed: 2:56:48.\n",
      "  Batch   800  of  1,230.    Elapsed: 2:59:02.\n",
      "  Batch   810  of  1,230.    Elapsed: 3:01:16.\n",
      "  Batch   820  of  1,230.    Elapsed: 3:03:31.\n",
      "  Batch   830  of  1,230.    Elapsed: 3:05:45.\n",
      "  Batch   840  of  1,230.    Elapsed: 3:07:58.\n",
      "  Batch   850  of  1,230.    Elapsed: 3:10:12.\n",
      "  Batch   860  of  1,230.    Elapsed: 3:12:26.\n",
      "  Batch   870  of  1,230.    Elapsed: 3:14:39.\n",
      "  Batch   880  of  1,230.    Elapsed: 3:16:53.\n",
      "  Batch   890  of  1,230.    Elapsed: 3:19:09.\n",
      "  Batch   900  of  1,230.    Elapsed: 3:21:23.\n",
      "  Batch   910  of  1,230.    Elapsed: 3:23:38.\n",
      "  Batch   920  of  1,230.    Elapsed: 3:25:51.\n",
      "  Batch   930  of  1,230.    Elapsed: 3:28:06.\n",
      "  Batch   940  of  1,230.    Elapsed: 3:30:21.\n",
      "  Batch   950  of  1,230.    Elapsed: 3:32:36.\n",
      "  Batch   960  of  1,230.    Elapsed: 3:34:51.\n",
      "  Batch   970  of  1,230.    Elapsed: 3:37:07.\n",
      "  Batch   980  of  1,230.    Elapsed: 3:39:21.\n",
      "  Batch   990  of  1,230.    Elapsed: 3:41:34.\n",
      "  Batch 1,000  of  1,230.    Elapsed: 3:43:49.\n",
      "  Batch 1,010  of  1,230.    Elapsed: 3:46:03.\n",
      "  Batch 1,020  of  1,230.    Elapsed: 3:48:18.\n",
      "  Batch 1,030  of  1,230.    Elapsed: 3:50:34.\n",
      "  Batch 1,040  of  1,230.    Elapsed: 3:52:49.\n",
      "  Batch 1,050  of  1,230.    Elapsed: 3:55:05.\n",
      "  Batch 1,060  of  1,230.    Elapsed: 3:57:20.\n",
      "  Batch 1,070  of  1,230.    Elapsed: 3:59:35.\n",
      "  Batch 1,080  of  1,230.    Elapsed: 4:01:50.\n",
      "  Batch 1,090  of  1,230.    Elapsed: 4:04:05.\n",
      "  Batch 1,100  of  1,230.    Elapsed: 4:06:21.\n",
      "  Batch 1,110  of  1,230.    Elapsed: 4:08:37.\n",
      "  Batch 1,120  of  1,230.    Elapsed: 4:10:51.\n",
      "  Batch 1,130  of  1,230.    Elapsed: 4:13:04.\n",
      "  Batch 1,140  of  1,230.    Elapsed: 4:15:18.\n",
      "  Batch 1,150  of  1,230.    Elapsed: 4:17:32.\n",
      "  Batch 1,160  of  1,230.    Elapsed: 4:19:46.\n",
      "  Batch 1,170  of  1,230.    Elapsed: 4:22:01.\n",
      "  Batch 1,180  of  1,230.    Elapsed: 4:24:16.\n",
      "  Batch 1,190  of  1,230.    Elapsed: 4:26:32.\n",
      "  Batch 1,200  of  1,230.    Elapsed: 4:28:46.\n",
      "  Batch 1,210  of  1,230.    Elapsed: 4:31:00.\n",
      "  Batch 1,220  of  1,230.    Elapsed: 4:33:14.\n",
      "Train loss: 0.21357444705880754\n",
      "Validation loss: 0.19522546144732594\n",
      "Validation Accuracy: 0.23548037613874262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 5/10 [24:04:10<24:05:56, 17351.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.8583599313546852\n",
      "  Batch    10  of  1,230.    Elapsed: 0:02:15.\n",
      "  Batch    20  of  1,230.    Elapsed: 0:04:28.\n",
      "  Batch    30  of  1,230.    Elapsed: 0:06:41.\n",
      "  Batch    40  of  1,230.    Elapsed: 0:08:54.\n",
      "  Batch    50  of  1,230.    Elapsed: 0:11:08.\n",
      "  Batch    60  of  1,230.    Elapsed: 0:13:21.\n",
      "  Batch    70  of  1,230.    Elapsed: 0:15:35.\n",
      "  Batch    80  of  1,230.    Elapsed: 0:17:51.\n",
      "  Batch    90  of  1,230.    Elapsed: 0:20:10.\n",
      "  Batch   100  of  1,230.    Elapsed: 0:22:26.\n",
      "  Batch   110  of  1,230.    Elapsed: 0:24:42.\n",
      "  Batch   120  of  1,230.    Elapsed: 0:26:58.\n",
      "  Batch   130  of  1,230.    Elapsed: 0:29:13.\n",
      "  Batch   140  of  1,230.    Elapsed: 0:31:28.\n",
      "  Batch   150  of  1,230.    Elapsed: 0:33:45.\n",
      "  Batch   160  of  1,230.    Elapsed: 0:36:00.\n",
      "  Batch   170  of  1,230.    Elapsed: 0:38:16.\n",
      "  Batch   180  of  1,230.    Elapsed: 0:40:31.\n",
      "  Batch   190  of  1,230.    Elapsed: 0:42:46.\n",
      "  Batch   200  of  1,230.    Elapsed: 0:45:02.\n",
      "  Batch   210  of  1,230.    Elapsed: 0:47:18.\n",
      "  Batch   220  of  1,230.    Elapsed: 0:49:34.\n",
      "  Batch   230  of  1,230.    Elapsed: 0:51:49.\n",
      "  Batch   240  of  1,230.    Elapsed: 0:54:04.\n",
      "  Batch   250  of  1,230.    Elapsed: 0:56:18.\n",
      "  Batch   260  of  1,230.    Elapsed: 0:58:32.\n",
      "  Batch   270  of  1,230.    Elapsed: 1:00:47.\n",
      "  Batch   280  of  1,230.    Elapsed: 1:03:03.\n",
      "  Batch   290  of  1,230.    Elapsed: 1:05:18.\n",
      "  Batch   300  of  1,230.    Elapsed: 1:07:34.\n",
      "  Batch   310  of  1,230.    Elapsed: 1:09:48.\n",
      "  Batch   320  of  1,230.    Elapsed: 1:12:01.\n",
      "  Batch   330  of  1,230.    Elapsed: 1:14:15.\n",
      "  Batch   340  of  1,230.    Elapsed: 1:16:30.\n",
      "  Batch   350  of  1,230.    Elapsed: 1:18:45.\n",
      "  Batch   360  of  1,230.    Elapsed: 1:20:59.\n",
      "  Batch   370  of  1,230.    Elapsed: 1:23:14.\n",
      "  Batch   380  of  1,230.    Elapsed: 1:25:29.\n",
      "  Batch   390  of  1,230.    Elapsed: 1:27:45.\n",
      "  Batch   400  of  1,230.    Elapsed: 1:29:59.\n",
      "  Batch   410  of  1,230.    Elapsed: 1:32:13.\n",
      "  Batch   420  of  1,230.    Elapsed: 1:34:27.\n",
      "  Batch   430  of  1,230.    Elapsed: 1:36:40.\n",
      "  Batch   440  of  1,230.    Elapsed: 1:38:53.\n",
      "  Batch   450  of  1,230.    Elapsed: 1:41:08.\n",
      "  Batch   460  of  1,230.    Elapsed: 1:43:23.\n",
      "  Batch   470  of  1,230.    Elapsed: 1:45:36.\n",
      "  Batch   480  of  1,230.    Elapsed: 1:47:51.\n",
      "  Batch   490  of  1,230.    Elapsed: 1:50:05.\n",
      "  Batch   500  of  1,230.    Elapsed: 1:52:19.\n",
      "  Batch   510  of  1,230.    Elapsed: 1:54:34.\n",
      "  Batch   520  of  1,230.    Elapsed: 1:56:48.\n",
      "  Batch   530  of  1,230.    Elapsed: 1:59:02.\n",
      "  Batch   540  of  1,230.    Elapsed: 2:01:16.\n",
      "  Batch   550  of  1,230.    Elapsed: 2:03:30.\n",
      "  Batch   560  of  1,230.    Elapsed: 2:05:45.\n",
      "  Batch   570  of  1,230.    Elapsed: 2:07:59.\n",
      "  Batch   580  of  1,230.    Elapsed: 2:10:15.\n",
      "  Batch   590  of  1,230.    Elapsed: 2:12:29.\n",
      "  Batch   600  of  1,230.    Elapsed: 2:14:43.\n",
      "  Batch   610  of  1,230.    Elapsed: 2:16:57.\n",
      "  Batch   620  of  1,230.    Elapsed: 2:19:11.\n",
      "  Batch   630  of  1,230.    Elapsed: 2:21:25.\n",
      "  Batch   640  of  1,230.    Elapsed: 2:23:39.\n",
      "  Batch   650  of  1,230.    Elapsed: 2:25:54.\n",
      "  Batch   660  of  1,230.    Elapsed: 2:28:10.\n",
      "  Batch   670  of  1,230.    Elapsed: 2:30:25.\n",
      "  Batch   680  of  1,230.    Elapsed: 2:32:41.\n",
      "  Batch   690  of  1,230.    Elapsed: 2:34:56.\n",
      "  Batch   700  of  1,230.    Elapsed: 2:37:09.\n",
      "  Batch   710  of  1,230.    Elapsed: 2:39:24.\n",
      "  Batch   720  of  1,230.    Elapsed: 2:41:39.\n",
      "  Batch   730  of  1,230.    Elapsed: 2:43:52.\n",
      "  Batch   740  of  1,230.    Elapsed: 2:46:06.\n",
      "  Batch   750  of  1,230.    Elapsed: 2:48:21.\n",
      "  Batch   760  of  1,230.    Elapsed: 2:50:35.\n",
      "  Batch   770  of  1,230.    Elapsed: 2:52:50.\n",
      "  Batch   780  of  1,230.    Elapsed: 2:55:04.\n",
      "  Batch   790  of  1,230.    Elapsed: 2:57:17.\n",
      "  Batch   800  of  1,230.    Elapsed: 2:59:31.\n",
      "  Batch   810  of  1,230.    Elapsed: 3:01:44.\n",
      "  Batch   820  of  1,230.    Elapsed: 3:03:59.\n",
      "  Batch   830  of  1,230.    Elapsed: 3:06:14.\n",
      "  Batch   840  of  1,230.    Elapsed: 3:08:28.\n",
      "  Batch   850  of  1,230.    Elapsed: 3:10:43.\n",
      "  Batch   860  of  1,230.    Elapsed: 3:12:57.\n",
      "  Batch   870  of  1,230.    Elapsed: 3:15:11.\n",
      "  Batch   880  of  1,230.    Elapsed: 3:17:26.\n",
      "  Batch   890  of  1,230.    Elapsed: 3:19:42.\n",
      "  Batch   900  of  1,230.    Elapsed: 3:21:57.\n",
      "  Batch   910  of  1,230.    Elapsed: 3:24:11.\n",
      "  Batch   920  of  1,230.    Elapsed: 3:26:26.\n",
      "  Batch   930  of  1,230.    Elapsed: 3:28:40.\n",
      "  Batch   940  of  1,230.    Elapsed: 3:30:53.\n",
      "  Batch   950  of  1,230.    Elapsed: 3:33:07.\n",
      "  Batch   960  of  1,230.    Elapsed: 3:35:22.\n",
      "  Batch   970  of  1,230.    Elapsed: 3:37:35.\n",
      "  Batch   980  of  1,230.    Elapsed: 3:39:49.\n",
      "  Batch   990  of  1,230.    Elapsed: 3:42:04.\n",
      "  Batch 1,000  of  1,230.    Elapsed: 3:44:18.\n",
      "  Batch 1,010  of  1,230.    Elapsed: 3:46:32.\n",
      "  Batch 1,020  of  1,230.    Elapsed: 3:48:46.\n",
      "  Batch 1,030  of  1,230.    Elapsed: 3:51:01.\n",
      "  Batch 1,040  of  1,230.    Elapsed: 3:53:15.\n",
      "  Batch 1,050  of  1,230.    Elapsed: 3:55:29.\n",
      "  Batch 1,060  of  1,230.    Elapsed: 3:57:43.\n",
      "  Batch 1,070  of  1,230.    Elapsed: 3:59:57.\n",
      "  Batch 1,080  of  1,230.    Elapsed: 4:02:12.\n",
      "  Batch 1,090  of  1,230.    Elapsed: 4:04:27.\n",
      "  Batch 1,100  of  1,230.    Elapsed: 4:06:41.\n",
      "  Batch 1,110  of  1,230.    Elapsed: 4:08:55.\n",
      "  Batch 1,120  of  1,230.    Elapsed: 4:11:09.\n",
      "  Batch 1,130  of  1,230.    Elapsed: 4:13:23.\n",
      "  Batch 1,140  of  1,230.    Elapsed: 4:15:37.\n",
      "  Batch 1,150  of  1,230.    Elapsed: 4:17:51.\n",
      "  Batch 1,160  of  1,230.    Elapsed: 4:20:04.\n",
      "  Batch 1,170  of  1,230.    Elapsed: 4:22:17.\n",
      "  Batch 1,180  of  1,230.    Elapsed: 4:24:31.\n",
      "  Batch 1,190  of  1,230.    Elapsed: 4:26:45.\n",
      "  Batch 1,200  of  1,230.    Elapsed: 4:28:59.\n",
      "  Batch 1,210  of  1,230.    Elapsed: 4:31:13.\n",
      "  Batch 1,220  of  1,230.    Elapsed: 4:33:27.\n",
      "Train loss: 0.1818620647053893\n",
      "Validation loss: 0.1211707437016668\n",
      "Validation Accuracy: 0.24341099417190065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  60%|██████    | 6/10 [28:55:13<19:18:59, 17384.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.8905476457013995\n",
      "  Batch    10  of  1,230.    Elapsed: 0:02:14.\n",
      "  Batch    20  of  1,230.    Elapsed: 0:04:28.\n",
      "  Batch    30  of  1,230.    Elapsed: 0:06:42.\n",
      "  Batch    40  of  1,230.    Elapsed: 0:08:58.\n",
      "  Batch    50  of  1,230.    Elapsed: 0:11:13.\n",
      "  Batch    60  of  1,230.    Elapsed: 0:13:28.\n",
      "  Batch    70  of  1,230.    Elapsed: 0:15:43.\n",
      "  Batch    80  of  1,230.    Elapsed: 0:17:57.\n",
      "  Batch    90  of  1,230.    Elapsed: 0:20:11.\n",
      "  Batch   100  of  1,230.    Elapsed: 0:22:25.\n",
      "  Batch   110  of  1,230.    Elapsed: 0:24:38.\n",
      "  Batch   120  of  1,230.    Elapsed: 0:26:54.\n",
      "  Batch   130  of  1,230.    Elapsed: 0:29:09.\n",
      "  Batch   140  of  1,230.    Elapsed: 0:31:23.\n",
      "  Batch   150  of  1,230.    Elapsed: 0:33:36.\n",
      "  Batch   160  of  1,230.    Elapsed: 0:35:50.\n",
      "  Batch   170  of  1,230.    Elapsed: 0:38:05.\n",
      "  Batch   180  of  1,230.    Elapsed: 0:40:19.\n",
      "  Batch   190  of  1,230.    Elapsed: 0:42:32.\n",
      "  Batch   200  of  1,230.    Elapsed: 0:44:46.\n",
      "  Batch   210  of  1,230.    Elapsed: 0:47:00.\n",
      "  Batch   220  of  1,230.    Elapsed: 0:49:15.\n",
      "  Batch   230  of  1,230.    Elapsed: 0:51:28.\n",
      "  Batch   240  of  1,230.    Elapsed: 0:53:43.\n",
      "  Batch   250  of  1,230.    Elapsed: 0:55:58.\n",
      "  Batch   260  of  1,230.    Elapsed: 0:58:12.\n",
      "  Batch   270  of  1,230.    Elapsed: 1:00:27.\n",
      "  Batch   280  of  1,230.    Elapsed: 1:02:42.\n",
      "  Batch   290  of  1,230.    Elapsed: 1:04:56.\n",
      "  Batch   300  of  1,230.    Elapsed: 1:07:11.\n",
      "  Batch   310  of  1,230.    Elapsed: 1:09:25.\n",
      "  Batch   320  of  1,230.    Elapsed: 1:11:39.\n",
      "  Batch   330  of  1,230.    Elapsed: 1:13:53.\n",
      "  Batch   340  of  1,230.    Elapsed: 1:16:08.\n",
      "  Batch   350  of  1,230.    Elapsed: 1:18:23.\n",
      "  Batch   360  of  1,230.    Elapsed: 1:20:36.\n",
      "  Batch   370  of  1,230.    Elapsed: 1:22:50.\n",
      "  Batch   380  of  1,230.    Elapsed: 1:25:04.\n",
      "  Batch   390  of  1,230.    Elapsed: 1:27:20.\n",
      "  Batch   400  of  1,230.    Elapsed: 1:29:34.\n",
      "  Batch   410  of  1,230.    Elapsed: 1:31:49.\n",
      "  Batch   420  of  1,230.    Elapsed: 1:34:03.\n",
      "  Batch   430  of  1,230.    Elapsed: 1:36:18.\n",
      "  Batch   440  of  1,230.    Elapsed: 1:38:34.\n",
      "  Batch   450  of  1,230.    Elapsed: 1:40:48.\n",
      "  Batch   460  of  1,230.    Elapsed: 1:43:03.\n",
      "  Batch   470  of  1,230.    Elapsed: 1:45:18.\n",
      "  Batch   480  of  1,230.    Elapsed: 1:47:32.\n",
      "  Batch   490  of  1,230.    Elapsed: 1:49:47.\n",
      "  Batch   500  of  1,230.    Elapsed: 1:52:02.\n",
      "  Batch   510  of  1,230.    Elapsed: 1:54:17.\n",
      "  Batch   520  of  1,230.    Elapsed: 1:56:32.\n",
      "  Batch   530  of  1,230.    Elapsed: 1:58:46.\n",
      "  Batch   540  of  1,230.    Elapsed: 2:01:00.\n",
      "  Batch   550  of  1,230.    Elapsed: 2:03:15.\n",
      "  Batch   560  of  1,230.    Elapsed: 2:05:28.\n",
      "  Batch   570  of  1,230.    Elapsed: 2:07:43.\n",
      "  Batch   580  of  1,230.    Elapsed: 2:09:57.\n",
      "  Batch   590  of  1,230.    Elapsed: 2:12:11.\n",
      "  Batch   600  of  1,230.    Elapsed: 2:14:25.\n",
      "  Batch   610  of  1,230.    Elapsed: 2:16:39.\n",
      "  Batch   620  of  1,230.    Elapsed: 2:18:55.\n",
      "  Batch   630  of  1,230.    Elapsed: 2:21:09.\n",
      "  Batch   640  of  1,230.    Elapsed: 2:23:23.\n",
      "  Batch   650  of  1,230.    Elapsed: 2:25:37.\n",
      "  Batch   660  of  1,230.    Elapsed: 2:27:53.\n",
      "  Batch   670  of  1,230.    Elapsed: 2:30:07.\n",
      "  Batch   680  of  1,230.    Elapsed: 2:32:22.\n",
      "  Batch   690  of  1,230.    Elapsed: 2:34:36.\n",
      "  Batch   700  of  1,230.    Elapsed: 2:36:49.\n",
      "  Batch   710  of  1,230.    Elapsed: 2:39:02.\n",
      "  Batch   720  of  1,230.    Elapsed: 2:41:17.\n",
      "  Batch   730  of  1,230.    Elapsed: 2:43:31.\n",
      "  Batch   740  of  1,230.    Elapsed: 2:45:47.\n",
      "  Batch   750  of  1,230.    Elapsed: 2:48:01.\n",
      "  Batch   760  of  1,230.    Elapsed: 2:50:15.\n",
      "  Batch   770  of  1,230.    Elapsed: 2:52:29.\n",
      "  Batch   780  of  1,230.    Elapsed: 2:54:44.\n",
      "  Batch   790  of  1,230.    Elapsed: 2:56:58.\n",
      "  Batch   800  of  1,230.    Elapsed: 2:59:11.\n",
      "  Batch   810  of  1,230.    Elapsed: 3:01:26.\n",
      "  Batch   820  of  1,230.    Elapsed: 3:03:42.\n",
      "  Batch   830  of  1,230.    Elapsed: 3:05:56.\n",
      "  Batch   840  of  1,230.    Elapsed: 3:08:10.\n",
      "  Batch   850  of  1,230.    Elapsed: 3:10:24.\n",
      "  Batch   860  of  1,230.    Elapsed: 3:12:38.\n",
      "  Batch   870  of  1,230.    Elapsed: 3:14:53.\n",
      "  Batch   880  of  1,230.    Elapsed: 3:17:08.\n",
      "  Batch   890  of  1,230.    Elapsed: 3:19:23.\n",
      "  Batch   900  of  1,230.    Elapsed: 3:21:37.\n",
      "  Batch   910  of  1,230.    Elapsed: 3:23:51.\n",
      "  Batch   920  of  1,230.    Elapsed: 3:26:05.\n",
      "  Batch   930  of  1,230.    Elapsed: 3:28:19.\n",
      "  Batch   940  of  1,230.    Elapsed: 3:30:34.\n",
      "  Batch   950  of  1,230.    Elapsed: 3:32:48.\n",
      "  Batch   960  of  1,230.    Elapsed: 3:35:02.\n",
      "  Batch   970  of  1,230.    Elapsed: 3:37:17.\n",
      "  Batch   980  of  1,230.    Elapsed: 3:39:31.\n",
      "  Batch   990  of  1,230.    Elapsed: 3:41:46.\n",
      "  Batch 1,000  of  1,230.    Elapsed: 3:44:00.\n",
      "  Batch 1,010  of  1,230.    Elapsed: 3:46:14.\n",
      "  Batch 1,020  of  1,230.    Elapsed: 3:48:28.\n",
      "  Batch 1,030  of  1,230.    Elapsed: 3:50:42.\n",
      "  Batch 1,040  of  1,230.    Elapsed: 3:52:56.\n",
      "  Batch 1,050  of  1,230.    Elapsed: 3:55:10.\n",
      "  Batch 1,060  of  1,230.    Elapsed: 3:57:24.\n",
      "  Batch 1,070  of  1,230.    Elapsed: 3:59:36.\n",
      "  Batch 1,080  of  1,230.    Elapsed: 4:01:50.\n",
      "  Batch 1,090  of  1,230.    Elapsed: 4:04:05.\n",
      "  Batch 1,100  of  1,230.    Elapsed: 4:06:21.\n",
      "  Batch 1,110  of  1,230.    Elapsed: 4:08:35.\n",
      "  Batch 1,120  of  1,230.    Elapsed: 4:10:50.\n",
      "  Batch 1,130  of  1,230.    Elapsed: 4:13:06.\n",
      "  Batch 1,140  of  1,230.    Elapsed: 4:15:20.\n",
      "  Batch 1,150  of  1,230.    Elapsed: 4:17:34.\n",
      "  Batch 1,160  of  1,230.    Elapsed: 4:19:49.\n",
      "  Batch 1,170  of  1,230.    Elapsed: 4:22:03.\n",
      "  Batch 1,180  of  1,230.    Elapsed: 4:24:17.\n",
      "  Batch 1,190  of  1,230.    Elapsed: 4:26:29.\n",
      "  Batch 1,200  of  1,230.    Elapsed: 4:28:43.\n",
      "  Batch 1,210  of  1,230.    Elapsed: 4:30:57.\n",
      "  Batch 1,220  of  1,230.    Elapsed: 4:33:11.\n",
      "Train loss: 0.15604332405013766\n",
      "Validation loss: 0.10744619274335186\n",
      "Validation Accuracy: 0.2435288116052736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  70%|███████   | 7/10 [33:46:10<14:30:19, 17406.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.8964830655648985\n",
      "  Batch    10  of  1,230.    Elapsed: 0:02:13.\n",
      "  Batch    20  of  1,230.    Elapsed: 0:04:26.\n",
      "  Batch    30  of  1,230.    Elapsed: 0:06:41.\n",
      "  Batch    40  of  1,230.    Elapsed: 0:08:57.\n",
      "  Batch    50  of  1,230.    Elapsed: 0:11:11.\n",
      "  Batch    60  of  1,230.    Elapsed: 0:13:25.\n",
      "  Batch    70  of  1,230.    Elapsed: 0:15:39.\n",
      "  Batch    80  of  1,230.    Elapsed: 0:17:52.\n",
      "  Batch    90  of  1,230.    Elapsed: 0:20:07.\n",
      "  Batch   100  of  1,230.    Elapsed: 0:22:21.\n",
      "  Batch   110  of  1,230.    Elapsed: 0:24:36.\n",
      "  Batch   120  of  1,230.    Elapsed: 0:26:51.\n",
      "  Batch   130  of  1,230.    Elapsed: 0:29:05.\n",
      "  Batch   140  of  1,230.    Elapsed: 0:31:18.\n",
      "  Batch   150  of  1,230.    Elapsed: 0:33:33.\n",
      "  Batch   160  of  1,230.    Elapsed: 0:35:47.\n",
      "  Batch   170  of  1,230.    Elapsed: 0:38:01.\n",
      "  Batch   180  of  1,230.    Elapsed: 0:40:15.\n",
      "  Batch   190  of  1,230.    Elapsed: 0:42:29.\n",
      "  Batch   200  of  1,230.    Elapsed: 0:44:44.\n",
      "  Batch   210  of  1,230.    Elapsed: 0:46:59.\n",
      "  Batch   220  of  1,230.    Elapsed: 0:49:13.\n",
      "  Batch   230  of  1,230.    Elapsed: 0:51:26.\n",
      "  Batch   240  of  1,230.    Elapsed: 0:53:41.\n",
      "  Batch   250  of  1,230.    Elapsed: 0:55:55.\n",
      "  Batch   260  of  1,230.    Elapsed: 0:58:09.\n",
      "  Batch   270  of  1,230.    Elapsed: 1:00:22.\n",
      "  Batch   280  of  1,230.    Elapsed: 1:02:37.\n",
      "  Batch   290  of  1,230.    Elapsed: 1:04:50.\n",
      "  Batch   300  of  1,230.    Elapsed: 1:07:04.\n",
      "  Batch   310  of  1,230.    Elapsed: 1:09:18.\n",
      "  Batch   320  of  1,230.    Elapsed: 1:11:33.\n",
      "  Batch   330  of  1,230.    Elapsed: 1:13:47.\n",
      "  Batch   340  of  1,230.    Elapsed: 1:16:01.\n",
      "  Batch   350  of  1,230.    Elapsed: 1:18:17.\n",
      "  Batch   360  of  1,230.    Elapsed: 1:20:31.\n",
      "  Batch   370  of  1,230.    Elapsed: 1:22:45.\n",
      "  Batch   380  of  1,230.    Elapsed: 1:24:59.\n",
      "  Batch   390  of  1,230.    Elapsed: 1:27:14.\n",
      "  Batch   400  of  1,230.    Elapsed: 1:29:27.\n",
      "  Batch   410  of  1,230.    Elapsed: 1:31:41.\n",
      "  Batch   420  of  1,230.    Elapsed: 1:33:57.\n",
      "  Batch   430  of  1,230.    Elapsed: 1:36:12.\n",
      "  Batch   440  of  1,230.    Elapsed: 1:38:25.\n",
      "  Batch   450  of  1,230.    Elapsed: 1:40:39.\n",
      "  Batch   460  of  1,230.    Elapsed: 1:42:54.\n",
      "  Batch   470  of  1,230.    Elapsed: 1:45:07.\n",
      "  Batch   480  of  1,230.    Elapsed: 1:47:21.\n",
      "  Batch   490  of  1,230.    Elapsed: 1:49:35.\n",
      "  Batch   500  of  1,230.    Elapsed: 1:51:47.\n",
      "  Batch   510  of  1,230.    Elapsed: 1:54:02.\n",
      "  Batch   520  of  1,230.    Elapsed: 1:56:16.\n",
      "  Batch   530  of  1,230.    Elapsed: 1:58:30.\n",
      "  Batch   540  of  1,230.    Elapsed: 2:00:44.\n",
      "  Batch   550  of  1,230.    Elapsed: 2:02:59.\n",
      "  Batch   560  of  1,230.    Elapsed: 2:05:14.\n",
      "  Batch   570  of  1,230.    Elapsed: 2:07:28.\n",
      "  Batch   580  of  1,230.    Elapsed: 2:09:44.\n",
      "  Batch   590  of  1,230.    Elapsed: 2:11:58.\n",
      "  Batch   600  of  1,230.    Elapsed: 2:14:13.\n",
      "  Batch   610  of  1,230.    Elapsed: 2:16:28.\n",
      "  Batch   620  of  1,230.    Elapsed: 2:18:43.\n",
      "  Batch   630  of  1,230.    Elapsed: 2:20:56.\n",
      "  Batch   640  of  1,230.    Elapsed: 2:23:11.\n",
      "  Batch   650  of  1,230.    Elapsed: 2:25:25.\n",
      "  Batch   660  of  1,230.    Elapsed: 2:27:39.\n",
      "  Batch   670  of  1,230.    Elapsed: 2:29:53.\n",
      "  Batch   680  of  1,230.    Elapsed: 2:32:07.\n",
      "  Batch   690  of  1,230.    Elapsed: 2:34:22.\n",
      "  Batch   700  of  1,230.    Elapsed: 2:36:37.\n",
      "  Batch   710  of  1,230.    Elapsed: 2:38:52.\n",
      "  Batch   720  of  1,230.    Elapsed: 2:41:07.\n",
      "  Batch   730  of  1,230.    Elapsed: 2:43:21.\n",
      "  Batch   740  of  1,230.    Elapsed: 2:45:36.\n",
      "  Batch   750  of  1,230.    Elapsed: 2:47:50.\n",
      "  Batch   760  of  1,230.    Elapsed: 2:50:03.\n",
      "  Batch   770  of  1,230.    Elapsed: 2:52:17.\n",
      "  Batch   780  of  1,230.    Elapsed: 2:54:32.\n",
      "  Batch   790  of  1,230.    Elapsed: 2:56:45.\n",
      "  Batch   800  of  1,230.    Elapsed: 2:59:00.\n",
      "  Batch   810  of  1,230.    Elapsed: 3:01:14.\n",
      "  Batch   820  of  1,230.    Elapsed: 3:03:29.\n",
      "  Batch   830  of  1,230.    Elapsed: 3:05:42.\n",
      "  Batch   840  of  1,230.    Elapsed: 3:07:56.\n",
      "  Batch   850  of  1,230.    Elapsed: 3:10:11.\n",
      "  Batch   860  of  1,230.    Elapsed: 3:12:24.\n",
      "  Batch   870  of  1,230.    Elapsed: 3:14:39.\n",
      "  Batch   880  of  1,230.    Elapsed: 3:16:54.\n",
      "  Batch   890  of  1,230.    Elapsed: 3:19:08.\n",
      "  Batch   900  of  1,230.    Elapsed: 3:21:22.\n",
      "  Batch   910  of  1,230.    Elapsed: 3:23:36.\n",
      "  Batch   920  of  1,230.    Elapsed: 3:25:51.\n",
      "  Batch   930  of  1,230.    Elapsed: 3:28:06.\n",
      "  Batch   940  of  1,230.    Elapsed: 3:30:21.\n",
      "  Batch   950  of  1,230.    Elapsed: 3:32:36.\n",
      "  Batch   960  of  1,230.    Elapsed: 3:34:51.\n",
      "  Batch   970  of  1,230.    Elapsed: 3:37:05.\n",
      "  Batch   980  of  1,230.    Elapsed: 3:39:19.\n",
      "  Batch   990  of  1,230.    Elapsed: 3:41:33.\n",
      "  Batch 1,000  of  1,230.    Elapsed: 3:43:47.\n",
      "  Batch 1,010  of  1,230.    Elapsed: 3:46:01.\n",
      "  Batch 1,020  of  1,230.    Elapsed: 3:48:15.\n",
      "  Batch 1,030  of  1,230.    Elapsed: 3:50:29.\n",
      "  Batch 1,040  of  1,230.    Elapsed: 3:52:44.\n",
      "  Batch 1,050  of  1,230.    Elapsed: 3:54:58.\n",
      "  Batch 1,060  of  1,230.    Elapsed: 3:57:11.\n",
      "  Batch 1,070  of  1,230.    Elapsed: 3:59:25.\n",
      "  Batch 1,080  of  1,230.    Elapsed: 4:01:40.\n",
      "  Batch 1,090  of  1,230.    Elapsed: 4:03:55.\n",
      "  Batch 1,100  of  1,230.    Elapsed: 4:06:08.\n",
      "  Batch 1,110  of  1,230.    Elapsed: 4:08:23.\n",
      "  Batch 1,120  of  1,230.    Elapsed: 4:10:38.\n",
      "  Batch 1,130  of  1,230.    Elapsed: 4:12:53.\n",
      "  Batch 1,140  of  1,230.    Elapsed: 4:15:08.\n",
      "  Batch 1,150  of  1,230.    Elapsed: 4:17:22.\n",
      "  Batch 1,160  of  1,230.    Elapsed: 4:19:37.\n",
      "  Batch 1,170  of  1,230.    Elapsed: 4:21:52.\n",
      "  Batch 1,180  of  1,230.    Elapsed: 4:24:06.\n",
      "  Batch 1,190  of  1,230.    Elapsed: 4:26:21.\n",
      "  Batch 1,200  of  1,230.    Elapsed: 4:28:34.\n",
      "  Batch 1,210  of  1,230.    Elapsed: 4:30:48.\n",
      "  Batch 1,220  of  1,230.    Elapsed: 4:33:02.\n",
      "Train loss: 0.1379555811000064\n",
      "Validation loss: 0.1477762524881502\n",
      "Validation Accuracy: 0.23960669170486057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  80%|████████  | 8/10 [38:37:05<9:40:41, 17420.93s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.8772330836818727\n",
      "  Batch    10  of  1,230.    Elapsed: 0:02:16.\n",
      "  Batch    20  of  1,230.    Elapsed: 0:04:30.\n",
      "  Batch    30  of  1,230.    Elapsed: 0:06:44.\n",
      "  Batch    40  of  1,230.    Elapsed: 0:08:58.\n",
      "  Batch    50  of  1,230.    Elapsed: 0:11:13.\n",
      "  Batch    60  of  1,230.    Elapsed: 0:13:27.\n",
      "  Batch    70  of  1,230.    Elapsed: 0:15:42.\n",
      "  Batch    80  of  1,230.    Elapsed: 0:17:56.\n",
      "  Batch    90  of  1,230.    Elapsed: 0:20:10.\n",
      "  Batch   100  of  1,230.    Elapsed: 0:22:24.\n",
      "  Batch   110  of  1,230.    Elapsed: 0:24:40.\n",
      "  Batch   120  of  1,230.    Elapsed: 0:26:55.\n",
      "  Batch   130  of  1,230.    Elapsed: 0:29:10.\n",
      "  Batch   140  of  1,230.    Elapsed: 0:31:24.\n",
      "  Batch   150  of  1,230.    Elapsed: 0:33:40.\n",
      "  Batch   160  of  1,230.    Elapsed: 0:35:55.\n",
      "  Batch   170  of  1,230.    Elapsed: 0:38:11.\n",
      "  Batch   180  of  1,230.    Elapsed: 0:40:26.\n",
      "  Batch   190  of  1,230.    Elapsed: 0:42:40.\n",
      "  Batch   200  of  1,230.    Elapsed: 0:44:55.\n",
      "  Batch   210  of  1,230.    Elapsed: 0:47:11.\n",
      "  Batch   220  of  1,230.    Elapsed: 0:49:26.\n",
      "  Batch   230  of  1,230.    Elapsed: 0:51:41.\n",
      "  Batch   240  of  1,230.    Elapsed: 0:53:56.\n",
      "  Batch   250  of  1,230.    Elapsed: 0:56:12.\n",
      "  Batch   260  of  1,230.    Elapsed: 0:58:28.\n",
      "  Batch   270  of  1,230.    Elapsed: 1:00:42.\n",
      "  Batch   280  of  1,230.    Elapsed: 1:02:55.\n",
      "  Batch   290  of  1,230.    Elapsed: 1:05:10.\n",
      "  Batch   300  of  1,230.    Elapsed: 1:07:26.\n",
      "  Batch   310  of  1,230.    Elapsed: 1:09:40.\n",
      "  Batch   320  of  1,230.    Elapsed: 1:11:54.\n",
      "  Batch   330  of  1,230.    Elapsed: 1:14:08.\n",
      "  Batch   340  of  1,230.    Elapsed: 1:16:22.\n",
      "  Batch   350  of  1,230.    Elapsed: 1:18:38.\n",
      "  Batch   360  of  1,230.    Elapsed: 1:20:52.\n",
      "  Batch   370  of  1,230.    Elapsed: 1:23:06.\n",
      "  Batch   380  of  1,230.    Elapsed: 1:25:20.\n",
      "  Batch   390  of  1,230.    Elapsed: 1:27:34.\n",
      "  Batch   400  of  1,230.    Elapsed: 1:29:49.\n",
      "  Batch   410  of  1,230.    Elapsed: 1:32:03.\n",
      "  Batch   420  of  1,230.    Elapsed: 1:34:17.\n",
      "  Batch   430  of  1,230.    Elapsed: 1:36:30.\n",
      "  Batch   440  of  1,230.    Elapsed: 1:38:45.\n",
      "  Batch   450  of  1,230.    Elapsed: 1:40:59.\n",
      "  Batch   460  of  1,230.    Elapsed: 1:43:13.\n",
      "  Batch   470  of  1,230.    Elapsed: 1:45:28.\n",
      "  Batch   480  of  1,230.    Elapsed: 1:47:43.\n",
      "  Batch   490  of  1,230.    Elapsed: 1:49:56.\n",
      "  Batch   500  of  1,230.    Elapsed: 1:52:11.\n",
      "  Batch   510  of  1,230.    Elapsed: 1:54:25.\n",
      "  Batch   520  of  1,230.    Elapsed: 1:56:39.\n",
      "  Batch   530  of  1,230.    Elapsed: 1:58:53.\n",
      "  Batch   540  of  1,230.    Elapsed: 2:01:08.\n",
      "  Batch   550  of  1,230.    Elapsed: 2:03:23.\n",
      "  Batch   560  of  1,230.    Elapsed: 2:05:38.\n",
      "  Batch   570  of  1,230.    Elapsed: 2:07:53.\n",
      "  Batch   580  of  1,230.    Elapsed: 2:10:08.\n",
      "  Batch   590  of  1,230.    Elapsed: 2:12:22.\n",
      "  Batch   600  of  1,230.    Elapsed: 2:14:36.\n",
      "  Batch   610  of  1,230.    Elapsed: 2:16:50.\n",
      "  Batch   620  of  1,230.    Elapsed: 2:19:04.\n",
      "  Batch   630  of  1,230.    Elapsed: 2:21:18.\n",
      "  Batch   640  of  1,230.    Elapsed: 2:23:32.\n",
      "  Batch   650  of  1,230.    Elapsed: 2:25:46.\n",
      "  Batch   660  of  1,230.    Elapsed: 2:28:02.\n",
      "  Batch   670  of  1,230.    Elapsed: 2:30:16.\n",
      "  Batch   680  of  1,230.    Elapsed: 2:32:30.\n",
      "  Batch   690  of  1,230.    Elapsed: 2:34:44.\n",
      "  Batch   700  of  1,230.    Elapsed: 2:36:57.\n",
      "  Batch   710  of  1,230.    Elapsed: 2:39:12.\n",
      "  Batch   720  of  1,230.    Elapsed: 2:41:25.\n",
      "  Batch   730  of  1,230.    Elapsed: 2:43:40.\n",
      "  Batch   740  of  1,230.    Elapsed: 2:45:55.\n",
      "  Batch   750  of  1,230.    Elapsed: 2:48:08.\n",
      "  Batch   760  of  1,230.    Elapsed: 2:50:22.\n",
      "  Batch   770  of  1,230.    Elapsed: 2:52:37.\n",
      "  Batch   780  of  1,230.    Elapsed: 2:54:51.\n",
      "  Batch   790  of  1,230.    Elapsed: 2:57:06.\n",
      "  Batch   800  of  1,230.    Elapsed: 2:59:21.\n",
      "  Batch   810  of  1,230.    Elapsed: 3:01:34.\n",
      "  Batch   820  of  1,230.    Elapsed: 3:03:48.\n",
      "  Batch   830  of  1,230.    Elapsed: 3:06:03.\n",
      "  Batch   840  of  1,230.    Elapsed: 3:08:18.\n",
      "  Batch   850  of  1,230.    Elapsed: 3:10:33.\n",
      "  Batch   860  of  1,230.    Elapsed: 3:12:46.\n",
      "  Batch   870  of  1,230.    Elapsed: 3:15:01.\n",
      "  Batch   880  of  1,230.    Elapsed: 3:17:14.\n",
      "  Batch   890  of  1,230.    Elapsed: 3:19:28.\n",
      "  Batch   900  of  1,230.    Elapsed: 3:21:42.\n",
      "  Batch   910  of  1,230.    Elapsed: 3:23:57.\n",
      "  Batch   920  of  1,230.    Elapsed: 3:26:12.\n",
      "  Batch   930  of  1,230.    Elapsed: 3:28:26.\n",
      "  Batch   940  of  1,230.    Elapsed: 3:30:41.\n",
      "  Batch   950  of  1,230.    Elapsed: 3:32:56.\n",
      "  Batch   960  of  1,230.    Elapsed: 3:35:10.\n",
      "  Batch   970  of  1,230.    Elapsed: 3:37:24.\n",
      "  Batch   980  of  1,230.    Elapsed: 3:39:38.\n",
      "  Batch   990  of  1,230.    Elapsed: 3:41:53.\n",
      "  Batch 1,000  of  1,230.    Elapsed: 3:44:07.\n",
      "  Batch 1,010  of  1,230.    Elapsed: 3:46:21.\n",
      "  Batch 1,020  of  1,230.    Elapsed: 3:48:37.\n",
      "  Batch 1,030  of  1,230.    Elapsed: 3:50:51.\n",
      "  Batch 1,040  of  1,230.    Elapsed: 3:53:06.\n",
      "  Batch 1,050  of  1,230.    Elapsed: 3:55:20.\n",
      "  Batch 1,060  of  1,230.    Elapsed: 3:57:35.\n",
      "  Batch 1,070  of  1,230.    Elapsed: 3:59:49.\n",
      "  Batch 1,080  of  1,230.    Elapsed: 4:02:03.\n",
      "  Batch 1,090  of  1,230.    Elapsed: 4:04:16.\n",
      "  Batch 1,100  of  1,230.    Elapsed: 4:06:30.\n",
      "  Batch 1,110  of  1,230.    Elapsed: 4:08:44.\n",
      "  Batch 1,120  of  1,230.    Elapsed: 4:10:59.\n",
      "  Batch 1,130  of  1,230.    Elapsed: 4:13:14.\n",
      "  Batch 1,140  of  1,230.    Elapsed: 4:15:28.\n",
      "  Batch 1,150  of  1,230.    Elapsed: 4:17:42.\n",
      "  Batch 1,160  of  1,230.    Elapsed: 4:19:56.\n",
      "  Batch 1,170  of  1,230.    Elapsed: 4:22:11.\n",
      "  Batch 1,180  of  1,230.    Elapsed: 4:24:24.\n",
      "  Batch 1,190  of  1,230.    Elapsed: 4:26:38.\n",
      "  Batch 1,200  of  1,230.    Elapsed: 4:28:51.\n",
      "  Batch 1,210  of  1,230.    Elapsed: 4:31:06.\n",
      "  Batch 1,220  of  1,230.    Elapsed: 4:33:19.\n",
      "Train loss: 0.12237776792570343\n",
      "Validation loss: 0.1056310157325581\n",
      "Validation Accuracy: 0.24297984566853392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  90%|█████████ | 9/10 [43:28:13<4:50:34, 17434.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.8982955602092547\n",
      "  Batch    10  of  1,230.    Elapsed: 0:02:14.\n",
      "  Batch    20  of  1,230.    Elapsed: 0:04:27.\n",
      "  Batch    30  of  1,230.    Elapsed: 0:06:42.\n",
      "  Batch    40  of  1,230.    Elapsed: 0:08:55.\n",
      "  Batch    50  of  1,230.    Elapsed: 0:11:10.\n",
      "  Batch    60  of  1,230.    Elapsed: 0:13:26.\n",
      "  Batch    70  of  1,230.    Elapsed: 0:15:40.\n",
      "  Batch    80  of  1,230.    Elapsed: 0:17:54.\n",
      "  Batch    90  of  1,230.    Elapsed: 0:20:09.\n",
      "  Batch   100  of  1,230.    Elapsed: 0:22:23.\n",
      "  Batch   110  of  1,230.    Elapsed: 0:24:39.\n",
      "  Batch   120  of  1,230.    Elapsed: 0:26:53.\n",
      "  Batch   130  of  1,230.    Elapsed: 0:29:07.\n",
      "  Batch   140  of  1,230.    Elapsed: 0:31:21.\n",
      "  Batch   150  of  1,230.    Elapsed: 0:33:35.\n",
      "  Batch   160  of  1,230.    Elapsed: 0:35:50.\n",
      "  Batch   170  of  1,230.    Elapsed: 0:38:04.\n",
      "  Batch   180  of  1,230.    Elapsed: 0:40:18.\n",
      "  Batch   190  of  1,230.    Elapsed: 0:42:33.\n",
      "  Batch   200  of  1,230.    Elapsed: 0:44:46.\n",
      "  Batch   210  of  1,230.    Elapsed: 0:47:00.\n",
      "  Batch   220  of  1,230.    Elapsed: 0:49:14.\n",
      "  Batch   230  of  1,230.    Elapsed: 0:51:28.\n",
      "  Batch   240  of  1,230.    Elapsed: 0:53:42.\n",
      "  Batch   250  of  1,230.    Elapsed: 0:55:58.\n",
      "  Batch   260  of  1,230.    Elapsed: 0:58:14.\n",
      "  Batch   270  of  1,230.    Elapsed: 1:00:28.\n",
      "  Batch   280  of  1,230.    Elapsed: 1:02:44.\n",
      "  Batch   290  of  1,230.    Elapsed: 1:04:58.\n",
      "  Batch   300  of  1,230.    Elapsed: 1:07:14.\n",
      "  Batch   310  of  1,230.    Elapsed: 1:09:28.\n",
      "  Batch   320  of  1,230.    Elapsed: 1:11:43.\n",
      "  Batch   330  of  1,230.    Elapsed: 1:13:58.\n",
      "  Batch   340  of  1,230.    Elapsed: 1:16:13.\n",
      "  Batch   350  of  1,230.    Elapsed: 1:18:28.\n",
      "  Batch   360  of  1,230.    Elapsed: 1:20:42.\n",
      "  Batch   370  of  1,230.    Elapsed: 1:22:57.\n",
      "  Batch   380  of  1,230.    Elapsed: 1:25:12.\n",
      "  Batch   390  of  1,230.    Elapsed: 1:27:27.\n",
      "  Batch   400  of  1,230.    Elapsed: 1:29:42.\n",
      "  Batch   410  of  1,230.    Elapsed: 1:31:57.\n",
      "  Batch   420  of  1,230.    Elapsed: 1:34:11.\n",
      "  Batch   430  of  1,230.    Elapsed: 1:36:25.\n",
      "  Batch   440  of  1,230.    Elapsed: 1:38:40.\n",
      "  Batch   450  of  1,230.    Elapsed: 1:40:55.\n",
      "  Batch   460  of  1,230.    Elapsed: 1:43:10.\n",
      "  Batch   470  of  1,230.    Elapsed: 1:45:26.\n",
      "  Batch   480  of  1,230.    Elapsed: 1:47:41.\n",
      "  Batch   490  of  1,230.    Elapsed: 1:49:56.\n",
      "  Batch   500  of  1,230.    Elapsed: 1:52:10.\n",
      "  Batch   510  of  1,230.    Elapsed: 1:54:24.\n",
      "  Batch   520  of  1,230.    Elapsed: 1:56:38.\n",
      "  Batch   530  of  1,230.    Elapsed: 1:58:53.\n",
      "  Batch   540  of  1,230.    Elapsed: 2:01:07.\n",
      "  Batch   550  of  1,230.    Elapsed: 2:03:22.\n",
      "  Batch   560  of  1,230.    Elapsed: 2:05:37.\n",
      "  Batch   570  of  1,230.    Elapsed: 2:07:51.\n",
      "  Batch   580  of  1,230.    Elapsed: 2:10:06.\n",
      "  Batch   590  of  1,230.    Elapsed: 2:12:22.\n",
      "  Batch   600  of  1,230.    Elapsed: 2:14:37.\n",
      "  Batch   610  of  1,230.    Elapsed: 2:16:53.\n",
      "  Batch   620  of  1,230.    Elapsed: 2:19:09.\n",
      "  Batch   630  of  1,230.    Elapsed: 2:21:24.\n",
      "  Batch   640  of  1,230.    Elapsed: 2:23:39.\n",
      "  Batch   650  of  1,230.    Elapsed: 2:25:53.\n",
      "  Batch   660  of  1,230.    Elapsed: 2:28:08.\n",
      "  Batch   670  of  1,230.    Elapsed: 2:30:24.\n",
      "  Batch   680  of  1,230.    Elapsed: 2:32:40.\n",
      "  Batch   690  of  1,230.    Elapsed: 2:34:55.\n",
      "  Batch   700  of  1,230.    Elapsed: 2:37:11.\n",
      "  Batch   710  of  1,230.    Elapsed: 2:39:26.\n",
      "  Batch   720  of  1,230.    Elapsed: 2:41:41.\n",
      "  Batch   730  of  1,230.    Elapsed: 2:43:57.\n",
      "  Batch   740  of  1,230.    Elapsed: 2:46:10.\n",
      "  Batch   750  of  1,230.    Elapsed: 2:48:25.\n",
      "  Batch   760  of  1,230.    Elapsed: 2:50:39.\n",
      "  Batch   770  of  1,230.    Elapsed: 2:52:53.\n",
      "  Batch   780  of  1,230.    Elapsed: 2:55:08.\n",
      "  Batch   790  of  1,230.    Elapsed: 2:57:22.\n",
      "  Batch   800  of  1,230.    Elapsed: 2:59:38.\n",
      "  Batch   810  of  1,230.    Elapsed: 3:01:53.\n",
      "  Batch   820  of  1,230.    Elapsed: 3:04:08.\n",
      "  Batch   830  of  1,230.    Elapsed: 3:06:21.\n",
      "  Batch   840  of  1,230.    Elapsed: 3:08:36.\n",
      "  Batch   850  of  1,230.    Elapsed: 3:10:51.\n",
      "  Batch   860  of  1,230.    Elapsed: 3:13:07.\n",
      "  Batch   870  of  1,230.    Elapsed: 3:15:24.\n",
      "  Batch   880  of  1,230.    Elapsed: 3:17:39.\n",
      "  Batch   890  of  1,230.    Elapsed: 3:19:56.\n",
      "  Batch   900  of  1,230.    Elapsed: 3:22:11.\n",
      "  Batch   910  of  1,230.    Elapsed: 3:24:27.\n",
      "  Batch   920  of  1,230.    Elapsed: 3:26:42.\n",
      "  Batch   930  of  1,230.    Elapsed: 3:28:57.\n",
      "  Batch   940  of  1,230.    Elapsed: 3:31:13.\n",
      "  Batch   950  of  1,230.    Elapsed: 3:33:27.\n",
      "  Batch   960  of  1,230.    Elapsed: 3:35:43.\n",
      "  Batch   970  of  1,230.    Elapsed: 3:37:59.\n",
      "  Batch   980  of  1,230.    Elapsed: 3:40:16.\n",
      "  Batch   990  of  1,230.    Elapsed: 3:42:32.\n",
      "  Batch 1,000  of  1,230.    Elapsed: 3:44:49.\n",
      "  Batch 1,010  of  1,230.    Elapsed: 3:47:05.\n",
      "  Batch 1,020  of  1,230.    Elapsed: 3:49:20.\n",
      "  Batch 1,030  of  1,230.    Elapsed: 3:51:36.\n",
      "  Batch 1,040  of  1,230.    Elapsed: 3:53:53.\n",
      "  Batch 1,050  of  1,230.    Elapsed: 3:56:09.\n",
      "  Batch 1,060  of  1,230.    Elapsed: 3:58:24.\n",
      "  Batch 1,070  of  1,230.    Elapsed: 4:00:40.\n",
      "  Batch 1,080  of  1,230.    Elapsed: 4:02:55.\n",
      "  Batch 1,090  of  1,230.    Elapsed: 4:05:11.\n",
      "  Batch 1,100  of  1,230.    Elapsed: 4:07:27.\n",
      "  Batch 1,110  of  1,230.    Elapsed: 4:09:42.\n",
      "  Batch 1,120  of  1,230.    Elapsed: 4:11:57.\n",
      "  Batch 1,130  of  1,230.    Elapsed: 4:14:13.\n",
      "  Batch 1,140  of  1,230.    Elapsed: 4:16:29.\n",
      "  Batch 1,150  of  1,230.    Elapsed: 4:18:44.\n",
      "  Batch 1,160  of  1,230.    Elapsed: 4:20:58.\n",
      "  Batch 1,170  of  1,230.    Elapsed: 4:23:12.\n",
      "  Batch 1,180  of  1,230.    Elapsed: 4:25:26.\n",
      "  Batch 1,190  of  1,230.    Elapsed: 4:27:39.\n",
      "  Batch 1,200  of  1,230.    Elapsed: 4:29:54.\n",
      "  Batch 1,210  of  1,230.    Elapsed: 4:32:08.\n",
      "  Batch 1,220  of  1,230.    Elapsed: 4:34:23.\n",
      "Train loss: 0.1120520415496293\n",
      "Validation loss: 0.09837092554373462\n",
      "Validation Accuracy: 0.24350299538844558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 10/10 [48:20:27<00:00, 17402.76s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.899932572383748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "max_grad_norm = 1.0\n",
    "total_t0 = time.time()\n",
    "\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    t0 = time.time()\n",
    "    # TRAIN loop\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        \n",
    "        if step % 10 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "            \n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # forward pass\n",
    "        loss, something_else = model(b_input_ids, token_type_ids=None,\n",
    "                     attention_mask=b_input_mask, labels=b_labels)\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # track train loss\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "    # print train loss per epoch\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    # VALIDATION on validation set\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions , true_labels = [], []\n",
    "    for batch in valid_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n",
    "                                  attention_mask=b_input_mask, labels=b_labels)[0]\n",
    "            logits = model(b_input_ids, token_type_ids=None,\n",
    "                           attention_mask=b_input_mask)[0]\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        true_labels.append(label_ids)\n",
    "        \n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        \n",
    "        nb_eval_examples += b_input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "    eval_loss = eval_loss/nb_eval_steps\n",
    "    print(\"Validation loss: {}\".format(eval_loss))\n",
    "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "    pred_tags = [tags_val[p_i] for p in predictions for p_i in p]\n",
    "    valid_tags = [tags_val[l_ii] for l in true_labels for l_i in l for l_ii in l_i]\n",
    "    print(\"F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "for batch in valid_dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n",
    "                              attention_mask=b_input_mask, labels=b_labels)[0]\n",
    "        logits = model(b_input_ids, token_type_ids=None,\n",
    "                       attention_mask=b_input_mask)[0]\n",
    "        \n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    true_labels.append(label_ids)\n",
    "#     tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "\n",
    "    eval_loss += tmp_eval_loss.mean().item()\n",
    "#     eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    nb_eval_examples += b_input_ids.size(0)\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "pred_tags = [[tags_val[p_i] for p_i in p] for p in predictions]\n",
    "valid_tags = [[tags_val[l_ii] for l_ii in l_i] for l in true_labels for l_i in l ]\n",
    "print(\"Validation loss: {}\".format(eval_loss/nb_eval_steps))\n",
    "# print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "print(\"Validation F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model, open('CamemBERT_POS', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "TGp0qeXvtFeW"
   },
   "source": [
    "## LDA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Introduces Gensim's LDA model and demonstrates its use on the NIPS corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "w2Hcagg7tFeY"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "eHfA8Ss9tFel"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os.path\n",
    "import re\n",
    "import tarfile\n",
    "import nltk\n",
    "\n",
    "import smart_open\n",
    "\n",
    "def extract_documents(url='https://cs.nyu.edu/~roweis/data/nips12raw_str602.tgz'):\n",
    "    fname = url.split('/')[-1]\n",
    "    \n",
    "    # Download the file to local storage first.\n",
    "    # We can't read it on the fly because of \n",
    "    # https://github.com/RaRe-Technologies/smart_open/issues/331\n",
    "    if not os.path.isfile(fname):\n",
    "        with smart_open.open(url, \"rb\") as fin:\n",
    "            with smart_open.open(fname, 'wb') as fout:\n",
    "                while True:\n",
    "                    buf = fin.read(io.DEFAULT_BUFFER_SIZE)\n",
    "                    if not buf:\n",
    "                        break\n",
    "                    fout.write(buf)\n",
    "                         \n",
    "    with tarfile.open(fname, mode='r:gz') as tar:\n",
    "        # Ignore directory entries, as well as files like README, etc.\n",
    "        files = [\n",
    "            m for m in tar.getmembers()\n",
    "            if m.isfile() and re.search(r'nipstxt/nips\\d+/\\d+\\.txt', m.name)\n",
    "        ]\n",
    "        for member in sorted(files, key=lambda x: x.name):\n",
    "            member_bytes = tar.extractfile(member).read()\n",
    "            yield member_bytes.decode('utf-8', errors='replace')\n",
    "\n",
    "docs = list(extract_documents())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "jr7W3VG2tFes"
   },
   "source": [
    "So we have a list of 1740 documents, where each document is a Unicode string. \n",
    "If you're thinking about using your own corpus, then you need to make sure\n",
    "that it's in the same format (list of Unicode strings) before proceeding\n",
    "with the rest of this tutorial.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "hidden": true,
    "id": "1wUTd2bctFe1",
    "outputId": "9d12f933-5e16-41fd-dca2-8331cfcf5614"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1740\n",
      "1 \n",
      "CONNECTIVITY VERSUS ENTROPY \n",
      "Yaser S. Abu-Mostafa \n",
      "California Institute of Technology \n",
      "Pasadena, CA 91125 \n",
      "ABSTRACT \n",
      "How does the connectivity of a neural network (number of synapses per \n",
      "neuron) relate to the complexity of the problems it can handle (measured by \n",
      "the entropy)? Switching theory would suggest no relation at all, since all Boolean \n",
      "functions can be implemented using a circuit with very low connectivity (e.g., \n",
      "using two-input NAND gates). However, for a network that learns a pr\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))\n",
    "print(docs[0][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "Lr2-gpuHtFe8"
   },
   "outputs": [],
   "source": [
    "# Tokenize the documents.\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Split the documents into tokens.\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for idx in range(len(docs)):\n",
    "    docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "    docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "# Remove numbers, but not words that contain numbers.\n",
    "docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "\n",
    "# Remove words that are only one character.\n",
    "docs = [[token for token in doc if len(token) > 1] for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "S1oN4BL4tFfD"
   },
   "source": [
    "We use the WordNet lemmatizer from NLTK. A lemmatizer is preferred over a\n",
    "stemmer in this case because it produces more readable words. Output that is\n",
    "easy to read is very desirable in topic modelling.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "hidden": true,
    "id": "p2D2GTM-tFfE",
    "outputId": "047e8101-67c9-4282-a52c-434dfcedf1b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize the documents.\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "wjyVTiretFfK"
   },
   "source": [
    "We find bigrams in the documents. Bigrams are sets of two adjacent words.\n",
    "Using bigrams we can get phrases like \"machine_learning\" in our output\n",
    "(spaces are replaced with underscores); without bigrams we would only get\n",
    "\"machine\" and \"learning\".\n",
    "\n",
    "Note that in the code below, we find bigrams and then add them to the\n",
    "original data, because we would like to keep the words \"machine\" and\n",
    "\"learning\" as well as the bigram \"machine_learning\".\n",
    "\n",
    ".. Important::\n",
    "    Computing n-grams of large dataset can be very computationally\n",
    "    and memory intensive.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 146
    },
    "hidden": true,
    "id": "_bwLiLmptFfL",
    "outputId": "67258211-2afd-4e79-e485-7f5b1a90d361"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-23 11:26:47,213 : INFO : 'pattern' package not found; tag filters are not available for English\n",
      "2020-09-23 11:26:47,230 : INFO : collecting all words and their counts\n",
      "2020-09-23 11:26:47,232 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2020-09-23 11:27:28,531 : INFO : collected 8598 word types from a corpus of 33052479 words (unigram + bigrams) and 1740 sentences\n",
      "2020-09-23 11:27:28,532 : INFO : using 8598 counts as vocab in Phrases<0 vocab, min_count=20, threshold=10.0, max_vocab_size=40000000>\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "# Compute bigrams.\n",
    "from gensim.models import Phrases\n",
    "\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "213Kpo2ztFfP"
   },
   "source": [
    "We remove rare words and common words based on their *document frequency*.\n",
    "Below we remove words that appear in less than 20 documents or in more than\n",
    "50% of the documents. Consider trying to remove words only based on their\n",
    "frequency, or maybe combining that with this approach.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109
    },
    "hidden": true,
    "id": "CRNGpD0EtFfP",
    "outputId": "7b53cf12-5c4f-4c97-da58-6a1e860d97a9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-23 11:29:23,774 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-09-23 11:29:28,855 : INFO : built Dictionary(98 unique tokens: ['\\n', ' ', '\"', '&', \"'\"]...) from 1740 documents (total 33061618 corpus positions)\n",
      "2020-09-23 11:29:28,856 : INFO : discarding 89 tokens: [('\\n', 1740), (' ', 1740), ('\"', 1589), ('&', 1012), (\"'\", 1727), ('(', 1740), (')', 1740), ('+', 1506), (',', 1740), ('-', 1739)]...\n",
      "2020-09-23 11:29:28,857 : INFO : keeping 9 tokens which were in no less than 20 and no more than 870 (=50.0%) documents\n",
      "2020-09-23 11:29:28,857 : INFO : resulting dictionary: Dictionary(9 unique tokens: ['@', '^', '!', '$', '\\\\']...)\n"
     ]
    }
   ],
   "source": [
    "# Remove rare and common tokens.\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "Do9JhUYhtFfV"
   },
   "source": [
    "Finally, we transform the documents to a vectorized form. We simply compute\n",
    "the frequency of each word, including the bigrams.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "n_hZjiu3tFfV"
   },
   "outputs": [],
   "source": [
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "hRVkC8v1tFfb"
   },
   "source": [
    "Let's see how many tokens and documents we have to train on.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "hidden": true,
    "id": "mJf9QhAatFfc",
    "outputId": "cceddb80-a33d-4fed-d819-5eaa58a3c9ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 9\n",
      "Number of documents: 1740\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "hidden": true,
    "id": "qKqG2x8ytFfj",
    "outputId": "443c8794-6b03-4d6a-bc53-682a1a656d70",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-23 11:29:33,872 : INFO : using autotuned alpha, starting with [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "2020-09-23 11:29:33,875 : INFO : using serial LDA version on this node\n",
      "2020-09-23 11:29:33,876 : INFO : running online (multi-pass) LDA training, 10 topics, 20 passes over the supplied corpus of 1740 documents, updating model once every 1740 documents, evaluating perplexity every 0 documents, iterating 400x with a convergence threshold of 0.001000\n",
      "2020-09-23 11:29:33,877 : INFO : PROGRESS: pass 0, at document #1740/1740\n",
      "2020-09-23 11:29:34,461 : INFO : optimized alpha [0.07986111, 0.086652175, 0.083130755, 0.09890915, 0.124905825, 0.083698735, 0.08218231, 0.08168751, 0.07862009, 0.078591436]\n",
      "2020-09-23 11:29:34,462 : INFO : topic #9 (0.079): 0.609*\"^\" + 0.089*\"@\" + 0.073*\"Q\" + 0.073*\"#\" + 0.049*\"$\" + 0.041*\"!\" + 0.033*\"\\\" + 0.017*\"~\" + 0.017*\"`\"\n",
      "2020-09-23 11:29:34,466 : INFO : topic #8 (0.079): 0.625*\"#\" + 0.192*\"!\" + 0.086*\"Q\" + 0.036*\"@\" + 0.029*\"$\" + 0.029*\"^\" + 0.001*\"\\\" + 0.001*\"~\" + 0.001*\"`\"\n",
      "2020-09-23 11:29:34,466 : INFO : topic #1 (0.087): 0.702*\"!\" + 0.110*\"\\\" + 0.053*\"Q\" + 0.043*\"$\" + 0.032*\"`\" + 0.017*\"@\" + 0.015*\"#\" + 0.015*\"~\" + 0.014*\"^\"\n",
      "2020-09-23 11:29:34,469 : INFO : topic #3 (0.099): 0.841*\"Q\" + 0.040*\"$\" + 0.033*\"@\" + 0.026*\"!\" + 0.024*\"#\" + 0.016*\"\\\" + 0.009*\"~\" + 0.007*\"^\" + 0.004*\"`\"\n",
      "2020-09-23 11:29:34,470 : INFO : topic #4 (0.125): 0.392*\"$\" + 0.278*\"Q\" + 0.187*\"!\" + 0.083*\"@\" + 0.025*\"#\" + 0.012*\"~\" + 0.011*\"^\" + 0.008*\"\\\" + 0.004*\"`\"\n",
      "2020-09-23 11:29:34,471 : INFO : topic diff=1.691439, rho=1.000000\n",
      "2020-09-23 11:29:34,474 : INFO : PROGRESS: pass 1, at document #1740/1740\n",
      "2020-09-23 11:29:34,888 : INFO : optimized alpha [0.074526235, 0.08747225, 0.07829813, 0.10602915, 0.13208993, 0.083532564, 0.07744984, 0.07309991, 0.071980625, 0.07322692]\n",
      "2020-09-23 11:29:34,889 : INFO : topic #8 (0.072): 0.676*\"#\" + 0.203*\"!\" + 0.063*\"Q\" + 0.024*\"@\" + 0.023*\"$\" + 0.010*\"^\" + 0.001*\"\\\" + 0.001*\"~\" + 0.001*\"`\"\n",
      "2020-09-23 11:29:34,894 : INFO : topic #7 (0.073): 0.355*\"@\" + 0.241*\"!\" + 0.113*\"$\" + 0.093*\"\\\" + 0.086*\"^\" + 0.052*\"Q\" + 0.030*\"#\" + 0.028*\"~\" + 0.001*\"`\"\n",
      "2020-09-23 11:29:34,895 : INFO : topic #1 (0.087): 0.761*\"!\" + 0.109*\"\\\" + 0.046*\"$\" + 0.031*\"Q\" + 0.023*\"`\" + 0.008*\"@\" + 0.008*\"#\" + 0.007*\"~\" + 0.006*\"^\"\n",
      "2020-09-23 11:29:34,897 : INFO : topic #3 (0.106): 0.888*\"Q\" + 0.034*\"$\" + 0.025*\"@\" + 0.021*\"!\" + 0.012*\"#\" + 0.010*\"\\\" + 0.005*\"~\" + 0.004*\"^\" + 0.002*\"`\"\n",
      "2020-09-23 11:29:34,901 : INFO : topic #4 (0.132): 0.513*\"$\" + 0.224*\"Q\" + 0.150*\"!\" + 0.072*\"@\" + 0.018*\"#\" + 0.008*\"~\" + 0.007*\"^\" + 0.006*\"\\\" + 0.003*\"`\"\n",
      "2020-09-23 11:29:34,901 : INFO : topic diff=0.541436, rho=0.577350\n",
      "2020-09-23 11:29:34,903 : INFO : PROGRESS: pass 2, at document #1740/1740\n",
      "2020-09-23 11:29:35,285 : INFO : optimized alpha [0.071261674, 0.08902055, 0.07521573, 0.113941975, 0.1394512, 0.08423864, 0.074317366, 0.0673929, 0.06764005, 0.06969388]\n",
      "2020-09-23 11:29:35,286 : INFO : topic #7 (0.067): 0.315*\"@\" + 0.225*\"\\\" + 0.185*\"!\" + 0.126*\"$\" + 0.058*\"^\" + 0.044*\"Q\" + 0.025*\"#\" + 0.021*\"~\" + 0.001*\"`\"\n",
      "2020-09-23 11:29:35,289 : INFO : topic #8 (0.068): 0.699*\"#\" + 0.203*\"!\" + 0.053*\"Q\" + 0.020*\"@\" + 0.018*\"$\" + 0.005*\"^\" + 0.001*\"\\\" + 0.001*\"~\" + 0.001*\"`\"\n",
      "2020-09-23 11:29:35,290 : INFO : topic #1 (0.089): 0.794*\"!\" + 0.102*\"\\\" + 0.044*\"$\" + 0.022*\"Q\" + 0.019*\"`\" + 0.005*\"#\" + 0.005*\"@\" + 0.004*\"~\" + 0.004*\"^\"\n",
      "2020-09-23 11:29:35,291 : INFO : topic #3 (0.114): 0.912*\"Q\" + 0.029*\"$\" + 0.020*\"@\" + 0.018*\"!\" + 0.007*\"\\\" + 0.007*\"#\" + 0.003*\"~\" + 0.002*\"^\" + 0.001*\"`\"\n",
      "2020-09-23 11:29:35,294 : INFO : topic #4 (0.139): 0.605*\"$\" + 0.181*\"Q\" + 0.122*\"!\" + 0.063*\"@\" + 0.013*\"#\" + 0.005*\"~\" + 0.005*\"^\" + 0.004*\"\\\" + 0.002*\"`\"\n",
      "2020-09-23 11:29:35,296 : INFO : topic diff=0.362512, rho=0.500000\n",
      "2020-09-23 11:29:35,303 : INFO : PROGRESS: pass 3, at document #1740/1740\n",
      "2020-09-23 11:29:35,632 : INFO : optimized alpha [0.06880226, 0.09019613, 0.07284596, 0.12219567, 0.14642778, 0.085020036, 0.07212623, 0.06439509, 0.06463247, 0.06705017]\n",
      "2020-09-23 11:29:35,633 : INFO : topic #7 (0.064): 0.456*\"\\\" + 0.231*\"@\" + 0.125*\"!\" + 0.109*\"$\" + 0.034*\"Q\" + 0.022*\"^\" + 0.012*\"#\" + 0.010*\"~\" + 0.001*\"`\"\n",
      "2020-09-23 11:29:35,638 : INFO : topic #8 (0.065): 0.712*\"#\" + 0.205*\"!\" + 0.047*\"Q\" + 0.016*\"@\" + 0.014*\"$\" + 0.003*\"^\" + 0.001*\"\\\" + 0.001*\"~\" + 0.001*\"`\"\n",
      "2020-09-23 11:29:35,639 : INFO : topic #1 (0.090): 0.838*\"!\" + 0.076*\"\\\" + 0.041*\"$\" + 0.016*\"`\" + 0.016*\"Q\" + 0.004*\"#\" + 0.003*\"^\" + 0.003*\"@\" + 0.003*\"~\"\n",
      "2020-09-23 11:29:35,640 : INFO : topic #3 (0.122): 0.928*\"Q\" + 0.025*\"$\" + 0.017*\"@\" + 0.015*\"!\" + 0.005*\"\\\" + 0.005*\"#\" + 0.002*\"~\" + 0.001*\"^\" + 0.001*\"`\"\n",
      "2020-09-23 11:29:35,641 : INFO : topic #4 (0.146): 0.672*\"$\" + 0.149*\"Q\" + 0.103*\"!\" + 0.055*\"@\" + 0.009*\"#\" + 0.003*\"\\\" + 0.003*\"^\" + 0.003*\"~\" + 0.001*\"`\"\n",
      "2020-09-23 11:29:35,642 : INFO : topic diff=0.331440, rho=0.447214\n",
      "2020-09-23 11:29:35,645 : INFO : PROGRESS: pass 4, at document #1740/1740\n",
      "2020-09-23 11:29:35,961 : INFO : optimized alpha [0.06701227, 0.0920312, 0.07069479, 0.13044831, 0.15336859, 0.08642773, 0.07037993, 0.062482856, 0.062354393, 0.06497536]\n",
      "2020-09-23 11:29:35,962 : INFO : topic #8 (0.062): 0.717*\"#\" + 0.209*\"!\" + 0.044*\"Q\" + 0.014*\"@\" + 0.010*\"$\" + 0.002*\"^\" + 0.001*\"\\\" + 0.001*\"~\" + 0.001*\"`\"\n",
      "2020-09-23 11:29:35,968 : INFO : topic #7 (0.062): 0.559*\"\\\" + 0.189*\"@\" + 0.106*\"!\" + 0.093*\"$\" + 0.028*\"Q\" + 0.011*\"^\" + 0.008*\"#\" + 0.005*\"~\" + 0.001*\"`\"\n",
      "2020-09-23 11:29:35,969 : INFO : topic #1 (0.092): 0.872*\"!\" + 0.055*\"\\\" + 0.038*\"$\" + 0.014*\"`\" + 0.012*\"Q\" + 0.003*\"#\" + 0.003*\"^\" + 0.002*\"@\" + 0.002*\"~\"\n",
      "2020-09-23 11:29:35,970 : INFO : topic #3 (0.130): 0.940*\"Q\" + 0.022*\"$\" + 0.015*\"@\" + 0.013*\"!\" + 0.004*\"\\\" + 0.003*\"#\" + 0.001*\"~\" + 0.001*\"^\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:35,971 : INFO : topic #4 (0.153): 0.724*\"$\" + 0.124*\"Q\" + 0.089*\"!\" + 0.049*\"@\" + 0.007*\"#\" + 0.002*\"\\\" + 0.002*\"^\" + 0.002*\"~\" + 0.001*\"`\"\n",
      "2020-09-23 11:29:35,976 : INFO : topic diff=0.289958, rho=0.408248\n",
      "2020-09-23 11:29:35,980 : INFO : PROGRESS: pass 5, at document #1740/1740\n",
      "2020-09-23 11:29:36,282 : INFO : optimized alpha [0.0655298, 0.09427288, 0.06889601, 0.13894023, 0.15998437, 0.08802594, 0.06897155, 0.061141614, 0.060596798, 0.06328864]\n",
      "2020-09-23 11:29:36,284 : INFO : topic #8 (0.061): 0.723*\"#\" + 0.213*\"!\" + 0.039*\"Q\" + 0.013*\"@\" + 0.008*\"$\" + 0.001*\"^\" + 0.001*\"\\\" + 0.001*\"~\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:36,286 : INFO : topic #7 (0.061): 0.624*\"\\\" + 0.163*\"@\" + 0.096*\"!\" + 0.079*\"$\" + 0.022*\"Q\" + 0.007*\"^\" + 0.005*\"#\" + 0.003*\"~\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:36,287 : INFO : topic #1 (0.094): 0.896*\"!\" + 0.041*\"\\\" + 0.035*\"$\" + 0.012*\"`\" + 0.009*\"Q\" + 0.002*\"#\" + 0.002*\"^\" + 0.001*\"@\" + 0.001*\"~\"\n",
      "2020-09-23 11:29:36,288 : INFO : topic #3 (0.139): 0.948*\"Q\" + 0.020*\"$\" + 0.013*\"@\" + 0.012*\"!\" + 0.003*\"\\\" + 0.002*\"#\" + 0.001*\"~\" + 0.001*\"^\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:36,289 : INFO : topic #4 (0.160): 0.763*\"$\" + 0.105*\"Q\" + 0.078*\"!\" + 0.043*\"@\" + 0.005*\"#\" + 0.002*\"\\\" + 0.002*\"^\" + 0.002*\"~\" + 0.001*\"`\"\n",
      "2020-09-23 11:29:36,290 : INFO : topic diff=0.277780, rho=0.377964\n",
      "2020-09-23 11:29:36,291 : INFO : PROGRESS: pass 6, at document #1740/1740\n",
      "2020-09-23 11:29:36,606 : INFO : optimized alpha [0.06422961, 0.09674266, 0.06741014, 0.14717335, 0.1662295, 0.089885004, 0.06779815, 0.06018561, 0.059206422, 0.061874013]\n",
      "2020-09-23 11:29:36,607 : INFO : topic #8 (0.059): 0.727*\"#\" + 0.217*\"!\" + 0.035*\"Q\" + 0.012*\"@\" + 0.007*\"$\" + 0.001*\"^\" + 0.001*\"\\\" + 0.001*\"~\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:36,609 : INFO : topic #7 (0.060): 0.673*\"\\\" + 0.144*\"@\" + 0.087*\"!\" + 0.067*\"$\" + 0.018*\"Q\" + 0.004*\"^\" + 0.004*\"#\" + 0.002*\"~\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:36,610 : INFO : topic #1 (0.097): 0.914*\"!\" + 0.032*\"$\" + 0.031*\"\\\" + 0.010*\"`\" + 0.007*\"Q\" + 0.002*\"#\" + 0.001*\"^\" + 0.001*\"@\" + 0.001*\"~\"\n",
      "2020-09-23 11:29:36,615 : INFO : topic #3 (0.147): 0.954*\"Q\" + 0.018*\"$\" + 0.012*\"@\" + 0.011*\"!\" + 0.002*\"\\\" + 0.002*\"#\" + 0.001*\"~\" + 0.000*\"^\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:36,616 : INFO : topic #4 (0.166): 0.795*\"$\" + 0.089*\"Q\" + 0.069*\"!\" + 0.039*\"@\" + 0.004*\"#\" + 0.001*\"\\\" + 0.001*\"~\" + 0.001*\"^\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:36,617 : INFO : topic diff=0.276872, rho=0.353553\n",
      "2020-09-23 11:29:36,620 : INFO : PROGRESS: pass 7, at document #1740/1740\n",
      "2020-09-23 11:29:36,904 : INFO : optimized alpha [0.0632925, 0.099422224, 0.066147946, 0.1553938, 0.17239042, 0.091895655, 0.06680935, 0.059452094, 0.057901587, 0.06067165]\n",
      "2020-09-23 11:29:36,905 : INFO : topic #8 (0.058): 0.730*\"#\" + 0.220*\"!\" + 0.032*\"Q\" + 0.010*\"@\" + 0.005*\"$\" + 0.001*\"^\" + 0.001*\"\\\" + 0.001*\"~\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:36,906 : INFO : topic #7 (0.059): 0.716*\"\\\" + 0.125*\"@\" + 0.079*\"!\" + 0.058*\"$\" + 0.015*\"Q\" + 0.003*\"^\" + 0.003*\"#\" + 0.002*\"~\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:36,907 : INFO : topic #1 (0.099): 0.928*\"!\" + 0.029*\"$\" + 0.023*\"\\\" + 0.009*\"`\" + 0.006*\"Q\" + 0.001*\"#\" + 0.001*\"^\" + 0.001*\"@\" + 0.001*\"~\"\n",
      "2020-09-23 11:29:36,908 : INFO : topic #3 (0.155): 0.959*\"Q\" + 0.017*\"$\" + 0.011*\"@\" + 0.010*\"!\" + 0.002*\"\\\" + 0.001*\"#\" + 0.000*\"~\" + 0.000*\"^\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:36,909 : INFO : topic #4 (0.172): 0.820*\"$\" + 0.077*\"Q\" + 0.063*\"!\" + 0.035*\"@\" + 0.003*\"#\" + 0.001*\"\\\" + 0.001*\"~\" + 0.001*\"^\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:36,909 : INFO : topic diff=0.277358, rho=0.333333\n",
      "2020-09-23 11:29:36,912 : INFO : PROGRESS: pass 8, at document #1740/1740\n",
      "2020-09-23 11:29:37,203 : INFO : optimized alpha [0.062459823, 0.102438726, 0.06486303, 0.1633902, 0.17859997, 0.09417412, 0.0659777, 0.05886957, 0.056817416, 0.059637304]\n",
      "2020-09-23 11:29:37,204 : INFO : topic #8 (0.057): 0.732*\"#\" + 0.223*\"!\" + 0.029*\"Q\" + 0.009*\"@\" + 0.005*\"$\" + 0.001*\"^\" + 0.001*\"\\\" + 0.000*\"~\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:37,206 : INFO : topic #7 (0.059): 0.752*\"\\\" + 0.108*\"@\" + 0.072*\"!\" + 0.050*\"$\" + 0.012*\"Q\" + 0.002*\"^\" + 0.002*\"#\" + 0.001*\"~\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:37,207 : INFO : topic #1 (0.102): 0.940*\"!\" + 0.026*\"$\" + 0.018*\"\\\" + 0.008*\"`\" + 0.005*\"Q\" + 0.001*\"#\" + 0.001*\"^\" + 0.001*\"@\" + 0.000*\"~\"\n",
      "2020-09-23 11:29:37,208 : INFO : topic #3 (0.163): 0.963*\"Q\" + 0.015*\"$\" + 0.010*\"@\" + 0.009*\"!\" + 0.001*\"\\\" + 0.001*\"#\" + 0.000*\"~\" + 0.000*\"^\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:37,209 : INFO : topic #4 (0.179): 0.841*\"$\" + 0.067*\"Q\" + 0.057*\"!\" + 0.031*\"@\" + 0.002*\"#\" + 0.001*\"\\\" + 0.001*\"~\" + 0.001*\"^\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:37,210 : INFO : topic diff=0.280674, rho=0.316228\n",
      "2020-09-23 11:29:37,212 : INFO : PROGRESS: pass 9, at document #1740/1740\n",
      "2020-09-23 11:29:37,497 : INFO : optimized alpha [0.06194802, 0.105647646, 0.06380414, 0.17122515, 0.18464032, 0.0963838, 0.06527451, 0.058379114, 0.055690583, 0.058736857]\n",
      "2020-09-23 11:29:37,498 : INFO : topic #8 (0.056): 0.734*\"#\" + 0.225*\"!\" + 0.027*\"Q\" + 0.008*\"@\" + 0.004*\"$\" + 0.001*\"^\" + 0.001*\"\\\" + 0.000*\"~\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:37,499 : INFO : topic #7 (0.058): 0.781*\"\\\" + 0.095*\"@\" + 0.066*\"!\" + 0.043*\"$\" + 0.011*\"Q\" + 0.002*\"#\" + 0.002*\"^\" + 0.001*\"~\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:37,500 : INFO : topic #1 (0.106): 0.949*\"!\" + 0.024*\"$\" + 0.014*\"\\\" + 0.007*\"`\" + 0.004*\"Q\" + 0.001*\"#\" + 0.001*\"^\" + 0.001*\"@\" + 0.000*\"~\"\n",
      "2020-09-23 11:29:37,501 : INFO : topic #3 (0.171): 0.966*\"Q\" + 0.014*\"$\" + 0.010*\"@\" + 0.008*\"!\" + 0.001*\"\\\" + 0.001*\"#\" + 0.000*\"~\" + 0.000*\"^\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:37,502 : INFO : topic #4 (0.185): 0.858*\"$\" + 0.058*\"Q\" + 0.052*\"!\" + 0.028*\"@\" + 0.002*\"#\" + 0.001*\"\\\" + 0.000*\"~\" + 0.000*\"^\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:37,503 : INFO : topic diff=0.283600, rho=0.301511\n",
      "2020-09-23 11:29:37,504 : INFO : PROGRESS: pass 10, at document #1740/1740\n",
      "2020-09-23 11:29:37,780 : INFO : optimized alpha [0.061432566, 0.10903471, 0.06272035, 0.1789094, 0.19044593, 0.09883001, 0.06466743, 0.057965383, 0.054756694, 0.057946034]\n",
      "2020-09-23 11:29:37,781 : INFO : topic #8 (0.055): 0.734*\"#\" + 0.228*\"!\" + 0.025*\"Q\" + 0.007*\"@\" + 0.004*\"$\" + 0.001*\"\\\" + 0.001*\"^\" + 0.000*\"~\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:37,782 : INFO : topic #7 (0.058): 0.805*\"\\\" + 0.083*\"@\" + 0.061*\"!\" + 0.038*\"$\" + 0.009*\"Q\" + 0.001*\"#\" + 0.001*\"^\" + 0.001*\"~\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:37,783 : INFO : topic #1 (0.109): 0.957*\"!\" + 0.022*\"$\" + 0.011*\"\\\" + 0.006*\"`\" + 0.003*\"Q\" + 0.001*\"#\" + 0.000*\"^\" + 0.000*\"@\" + 0.000*\"~\"\n",
      "2020-09-23 11:29:37,784 : INFO : topic #3 (0.179): 0.969*\"Q\" + 0.013*\"$\" + 0.009*\"@\" + 0.008*\"!\" + 0.001*\"\\\" + 0.000*\"#\" + 0.000*\"~\" + 0.000*\"^\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:37,785 : INFO : topic #4 (0.190): 0.874*\"$\" + 0.051*\"Q\" + 0.047*\"!\" + 0.026*\"@\" + 0.001*\"#\" + 0.000*\"\\\" + 0.000*\"~\" + 0.000*\"^\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:37,787 : INFO : topic diff=0.289585, rho=0.288675\n",
      "2020-09-23 11:29:37,789 : INFO : PROGRESS: pass 11, at document #1740/1740\n",
      "2020-09-23 11:29:38,056 : INFO : optimized alpha [0.06086298, 0.11232922, 0.06184003, 0.18639879, 0.19601858, 0.101376064, 0.064139254, 0.057615474, 0.054043494, 0.057245698]\n",
      "2020-09-23 11:29:38,057 : INFO : topic #8 (0.054): 0.735*\"#\" + 0.232*\"!\" + 0.022*\"Q\" + 0.006*\"@\" + 0.003*\"$\" + 0.001*\"\\\" + 0.001*\"^\" + 0.000*\"~\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:38,058 : INFO : topic #9 (0.057): 0.953*\"^\" + 0.019*\"Q\" + 0.008*\"!\" + 0.008*\"$\" + 0.007*\"@\" + 0.004*\"#\" + 0.001*\"\\\" + 0.001*\"~\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:38,059 : INFO : topic #1 (0.112): 0.963*\"!\" + 0.020*\"$\" + 0.008*\"\\\" + 0.005*\"`\" + 0.003*\"Q\" + 0.001*\"#\" + 0.000*\"@\" + 0.000*\"^\" + 0.000*\"~\"\n",
      "2020-09-23 11:29:38,060 : INFO : topic #3 (0.186): 0.972*\"Q\" + 0.012*\"$\" + 0.008*\"@\" + 0.007*\"!\" + 0.001*\"\\\" + 0.000*\"#\" + 0.000*\"~\" + 0.000*\"^\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:38,061 : INFO : topic #4 (0.196): 0.887*\"$\" + 0.045*\"Q\" + 0.043*\"!\" + 0.023*\"@\" + 0.001*\"#\" + 0.000*\"\\\" + 0.000*\"~\" + 0.000*\"^\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:38,061 : INFO : topic diff=0.298801, rho=0.277350\n",
      "2020-09-23 11:29:38,064 : INFO : PROGRESS: pass 12, at document #1740/1740\n",
      "2020-09-23 11:29:38,341 : INFO : optimized alpha [0.06041203, 0.11594845, 0.060875926, 0.1937348, 0.20141612, 0.10413171, 0.06368019, 0.057313755, 0.05335475, 0.05662351]\n",
      "2020-09-23 11:29:38,342 : INFO : topic #8 (0.053): 0.734*\"#\" + 0.236*\"!\" + 0.020*\"Q\" + 0.006*\"@\" + 0.003*\"$\" + 0.001*\"\\\" + 0.000*\"^\" + 0.000*\"~\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:38,343 : INFO : topic #9 (0.057): 0.959*\"^\" + 0.017*\"Q\" + 0.007*\"!\" + 0.007*\"$\" + 0.006*\"@\" + 0.003*\"#\" + 0.001*\"\\\" + 0.001*\"~\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:38,344 : INFO : topic #1 (0.116): 0.968*\"!\" + 0.018*\"$\" + 0.006*\"\\\" + 0.004*\"`\" + 0.002*\"Q\" + 0.000*\"#\" + 0.000*\"@\" + 0.000*\"^\" + 0.000*\"~\"\n",
      "2020-09-23 11:29:38,345 : INFO : topic #3 (0.194): 0.974*\"Q\" + 0.011*\"$\" + 0.008*\"@\" + 0.007*\"!\" + 0.000*\"\\\" + 0.000*\"#\" + 0.000*\"~\" + 0.000*\"^\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:38,346 : INFO : topic #4 (0.201): 0.898*\"$\" + 0.040*\"Q\" + 0.039*\"!\" + 0.021*\"@\" + 0.001*\"#\" + 0.000*\"\\\" + 0.000*\"~\" + 0.000*\"^\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:38,347 : INFO : topic diff=0.305981, rho=0.267261\n",
      "2020-09-23 11:29:38,348 : INFO : PROGRESS: pass 13, at document #1740/1740\n",
      "2020-09-23 11:29:38,618 : INFO : optimized alpha [0.060143076, 0.119662136, 0.06000578, 0.20092806, 0.20664236, 0.10680302, 0.06327846, 0.0570521, 0.0526266, 0.056066554]\n",
      "2020-09-23 11:29:38,619 : INFO : topic #8 (0.053): 0.735*\"#\" + 0.237*\"!\" + 0.019*\"Q\" + 0.005*\"@\" + 0.002*\"$\" + 0.001*\"\\\" + 0.000*\"^\" + 0.000*\"~\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:38,620 : INFO : topic #9 (0.056): 0.964*\"^\" + 0.015*\"Q\" + 0.006*\"!\" + 0.006*\"$\" + 0.005*\"@\" + 0.002*\"#\" + 0.001*\"\\\" + 0.000*\"~\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:38,622 : INFO : topic #1 (0.120): 0.972*\"!\" + 0.017*\"$\" + 0.005*\"\\\" + 0.003*\"`\" + 0.002*\"Q\" + 0.000*\"#\" + 0.000*\"@\" + 0.000*\"^\" + 0.000*\"~\"\n",
      "2020-09-23 11:29:38,625 : INFO : topic #3 (0.201): 0.976*\"Q\" + 0.010*\"$\" + 0.008*\"@\" + 0.006*\"!\" + 0.000*\"\\\" + 0.000*\"#\" + 0.000*\"~\" + 0.000*\"^\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:38,626 : INFO : topic #4 (0.207): 0.908*\"$\" + 0.036*\"!\" + 0.035*\"Q\" + 0.019*\"@\" + 0.001*\"#\" + 0.000*\"\\\" + 0.000*\"~\" + 0.000*\"^\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:38,628 : INFO : topic diff=0.315844, rho=0.258199\n",
      "2020-09-23 11:29:38,631 : INFO : PROGRESS: pass 14, at document #1740/1740\n",
      "2020-09-23 11:29:38,886 : INFO : optimized alpha [0.059843574, 0.123613, 0.05912113, 0.20789412, 0.21187058, 0.10960285, 0.06292758, 0.056826573, 0.05202965, 0.05556749]\n",
      "2020-09-23 11:29:38,888 : INFO : topic #8 (0.052): 0.737*\"#\" + 0.238*\"!\" + 0.017*\"Q\" + 0.005*\"@\" + 0.002*\"$\" + 0.001*\"\\\" + 0.000*\"^\" + 0.000*\"~\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:38,890 : INFO : topic #9 (0.056): 0.969*\"^\" + 0.013*\"Q\" + 0.005*\"!\" + 0.005*\"$\" + 0.004*\"@\" + 0.002*\"#\" + 0.001*\"\\\" + 0.000*\"~\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:38,891 : INFO : topic #1 (0.124): 0.975*\"!\" + 0.015*\"$\" + 0.004*\"\\\" + 0.003*\"`\" + 0.002*\"Q\" + 0.000*\"#\" + 0.000*\"@\" + 0.000*\"^\" + 0.000*\"~\"\n",
      "2020-09-23 11:29:38,894 : INFO : topic #3 (0.208): 0.978*\"Q\" + 0.009*\"$\" + 0.007*\"@\" + 0.006*\"!\" + 0.000*\"\\\" + 0.000*\"#\" + 0.000*\"~\" + 0.000*\"^\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:38,895 : INFO : topic #4 (0.212): 0.917*\"$\" + 0.033*\"!\" + 0.031*\"Q\" + 0.017*\"@\" + 0.001*\"#\" + 0.000*\"\\\" + 0.000*\"~\" + 0.000*\"^\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:38,897 : INFO : topic diff=0.326117, rho=0.250000\n",
      "2020-09-23 11:29:38,899 : INFO : PROGRESS: pass 15, at document #1740/1740\n",
      "2020-09-23 11:29:39,167 : INFO : optimized alpha [0.05960505, 0.12733346, 0.05835489, 0.2145985, 0.21694636, 0.112274595, 0.06261662, 0.056629326, 0.05146856, 0.055114858]\n",
      "2020-09-23 11:29:39,168 : INFO : topic #8 (0.051): 0.737*\"#\" + 0.240*\"!\" + 0.015*\"Q\" + 0.004*\"@\" + 0.002*\"$\" + 0.000*\"\\\" + 0.000*\"^\" + 0.000*\"~\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:39,173 : INFO : topic #9 (0.055): 0.973*\"^\" + 0.012*\"Q\" + 0.005*\"!\" + 0.004*\"$\" + 0.004*\"@\" + 0.002*\"#\" + 0.001*\"\\\" + 0.000*\"~\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:39,174 : INFO : topic #1 (0.127): 0.978*\"!\" + 0.014*\"$\" + 0.003*\"\\\" + 0.002*\"`\" + 0.001*\"Q\" + 0.000*\"#\" + 0.000*\"@\" + 0.000*\"^\" + 0.000*\"~\"\n",
      "2020-09-23 11:29:39,178 : INFO : topic #3 (0.215): 0.979*\"Q\" + 0.008*\"$\" + 0.007*\"@\" + 0.005*\"!\" + 0.000*\"\\\" + 0.000*\"#\" + 0.000*\"~\" + 0.000*\"^\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:39,181 : INFO : topic #4 (0.217): 0.925*\"$\" + 0.031*\"!\" + 0.027*\"Q\" + 0.016*\"@\" + 0.000*\"#\" + 0.000*\"\\\" + 0.000*\"~\" + 0.000*\"^\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:39,183 : INFO : topic diff=0.337096, rho=0.242536\n",
      "2020-09-23 11:29:39,186 : INFO : PROGRESS: pass 16, at document #1740/1740\n",
      "2020-09-23 11:29:39,465 : INFO : optimized alpha [0.05937253, 0.13095227, 0.057685234, 0.22105682, 0.22180362, 0.114938654, 0.0623413, 0.056464396, 0.050975185, 0.054703392]\n",
      "2020-09-23 11:29:39,466 : INFO : topic #8 (0.051): 0.736*\"#\" + 0.243*\"!\" + 0.014*\"Q\" + 0.004*\"@\" + 0.002*\"$\" + 0.000*\"\\\" + 0.000*\"^\" + 0.000*\"~\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:39,468 : INFO : topic #9 (0.055): 0.976*\"^\" + 0.010*\"Q\" + 0.004*\"!\" + 0.004*\"$\" + 0.003*\"@\" + 0.001*\"#\" + 0.001*\"\\\" + 0.000*\"~\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:39,471 : INFO : topic #1 (0.131): 0.980*\"!\" + 0.013*\"$\" + 0.003*\"\\\" + 0.002*\"`\" + 0.001*\"Q\" + 0.000*\"#\" + 0.000*\"@\" + 0.000*\"^\" + 0.000*\"~\"\n",
      "2020-09-23 11:29:39,472 : INFO : topic #3 (0.221): 0.980*\"Q\" + 0.008*\"$\" + 0.007*\"@\" + 0.005*\"!\" + 0.000*\"\\\" + 0.000*\"#\" + 0.000*\"~\" + 0.000*\"^\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:39,473 : INFO : topic #4 (0.222): 0.932*\"$\" + 0.029*\"!\" + 0.024*\"Q\" + 0.014*\"@\" + 0.000*\"#\" + 0.000*\"\\\" + 0.000*\"~\" + 0.000*\"^\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:39,474 : INFO : topic diff=0.348455, rho=0.235702\n",
      "2020-09-23 11:29:39,476 : INFO : PROGRESS: pass 17, at document #1740/1740\n",
      "2020-09-23 11:29:39,727 : INFO : optimized alpha [0.05914275, 0.13468087, 0.05705655, 0.22729811, 0.22649743, 0.11760584, 0.06209859, 0.056321975, 0.050543714, 0.054329187]\n",
      "2020-09-23 11:29:39,728 : INFO : topic #8 (0.051): 0.737*\"#\" + 0.244*\"!\" + 0.013*\"Q\" + 0.004*\"@\" + 0.001*\"$\" + 0.000*\"\\\" + 0.000*\"^\" + 0.000*\"~\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:39,731 : INFO : topic #9 (0.054): 0.979*\"^\" + 0.009*\"Q\" + 0.004*\"!\" + 0.003*\"$\" + 0.003*\"@\" + 0.001*\"#\" + 0.001*\"\\\" + 0.000*\"~\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:39,732 : INFO : topic #1 (0.135): 0.982*\"!\" + 0.012*\"$\" + 0.002*\"\\\" + 0.002*\"`\" + 0.001*\"Q\" + 0.000*\"#\" + 0.000*\"@\" + 0.000*\"^\" + 0.000*\"~\"\n",
      "2020-09-23 11:29:39,733 : INFO : topic #4 (0.226): 0.938*\"$\" + 0.026*\"!\" + 0.022*\"Q\" + 0.013*\"@\" + 0.000*\"#\" + 0.000*\"\\\" + 0.000*\"~\" + 0.000*\"^\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:39,734 : INFO : topic #3 (0.227): 0.982*\"Q\" + 0.007*\"$\" + 0.006*\"@\" + 0.005*\"!\" + 0.000*\"\\\" + 0.000*\"#\" + 0.000*\"~\" + 0.000*\"^\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:39,735 : INFO : topic diff=0.359350, rho=0.229416\n",
      "2020-09-23 11:29:39,738 : INFO : PROGRESS: pass 18, at document #1740/1740\n",
      "2020-09-23 11:29:39,993 : INFO : optimized alpha [0.05894691, 0.13852082, 0.05644804, 0.23335564, 0.23106362, 0.12033746, 0.061885316, 0.056200232, 0.050128154, 0.053988237]\n",
      "2020-09-23 11:29:39,994 : INFO : topic #8 (0.050): 0.738*\"#\" + 0.245*\"!\" + 0.011*\"Q\" + 0.003*\"@\" + 0.001*\"$\" + 0.000*\"\\\" + 0.000*\"^\" + 0.000*\"~\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:39,997 : INFO : topic #9 (0.054): 0.981*\"^\" + 0.008*\"Q\" + 0.003*\"!\" + 0.003*\"$\" + 0.002*\"@\" + 0.001*\"#\" + 0.001*\"\\\" + 0.000*\"~\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:39,999 : INFO : topic #1 (0.139): 0.984*\"!\" + 0.011*\"$\" + 0.002*\"\\\" + 0.002*\"`\" + 0.001*\"Q\" + 0.000*\"@\" + 0.000*\"#\" + 0.000*\"^\" + 0.000*\"~\"\n",
      "2020-09-23 11:29:40,001 : INFO : topic #4 (0.231): 0.944*\"$\" + 0.024*\"!\" + 0.019*\"Q\" + 0.012*\"@\" + 0.000*\"#\" + 0.000*\"\\\" + 0.000*\"~\" + 0.000*\"^\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:40,003 : INFO : topic #3 (0.233): 0.983*\"Q\" + 0.007*\"$\" + 0.006*\"@\" + 0.004*\"!\" + 0.000*\"\\\" + 0.000*\"#\" + 0.000*\"~\" + 0.000*\"^\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:40,004 : INFO : topic diff=0.370538, rho=0.223607\n",
      "2020-09-23 11:29:40,006 : INFO : PROGRESS: pass 19, at document #1740/1740\n",
      "2020-09-23 11:29:40,267 : INFO : optimized alpha [0.0588233, 0.14252481, 0.055833813, 0.23923153, 0.23551112, 0.1231463, 0.061698742, 0.05609737, 0.04971025, 0.053677373]\n",
      "2020-09-23 11:29:40,269 : INFO : topic #8 (0.050): 0.740*\"#\" + 0.244*\"!\" + 0.010*\"Q\" + 0.003*\"@\" + 0.001*\"$\" + 0.000*\"\\\" + 0.000*\"^\" + 0.000*\"~\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:40,270 : INFO : topic #9 (0.054): 0.983*\"^\" + 0.007*\"Q\" + 0.003*\"!\" + 0.002*\"$\" + 0.002*\"@\" + 0.001*\"#\" + 0.001*\"\\\" + 0.000*\"~\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:40,272 : INFO : topic #1 (0.143): 0.986*\"!\" + 0.010*\"$\" + 0.002*\"\\\" + 0.001*\"`\" + 0.001*\"Q\" + 0.000*\"@\" + 0.000*\"#\" + 0.000*\"^\" + 0.000*\"~\"\n",
      "2020-09-23 11:29:40,273 : INFO : topic #4 (0.236): 0.949*\"$\" + 0.022*\"!\" + 0.017*\"Q\" + 0.011*\"@\" + 0.000*\"#\" + 0.000*\"\\\" + 0.000*\"~\" + 0.000*\"^\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:40,275 : INFO : topic #3 (0.239): 0.984*\"Q\" + 0.006*\"$\" + 0.006*\"@\" + 0.004*\"!\" + 0.000*\"\\\" + 0.000*\"#\" + 0.000*\"~\" + 0.000*\"^\" + 0.000*\"`\"\n",
      "2020-09-23 11:29:40,277 : INFO : topic diff=0.379090, rho=0.218218\n"
     ]
    }
   ],
   "source": [
    "# Train LDA model.\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 10\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "hidden": true,
    "id": "AZr6b66itFfr",
    "outputId": "b4a2468b-f331-451c-d053-7873c0a63e77",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-23 11:29:40,294 : INFO : CorpusAccumulator accumulated stats from 1000 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average topic coherence: -2.1677.\n",
      "[([(0.6268966, '~'),\n",
      "   (0.27164382, '`'),\n",
      "   (0.09518634, '$'),\n",
      "   (0.0018629064, '@'),\n",
      "   (0.0016628734, '!'),\n",
      "   (0.0009513289, 'Q'),\n",
      "   (0.0009442948, '#'),\n",
      "   (0.0006141935, '\\\\'),\n",
      "   (0.0002376064, '^')],\n",
      "  -1.6473896068873706),\n",
      " ([(0.9857895, '!'),\n",
      "   (0.010243523, '$'),\n",
      "   (0.0015068037, '\\\\'),\n",
      "   (0.0013442617, '`'),\n",
      "   (0.00069610035, 'Q'),\n",
      "   (0.00014837748, '@'),\n",
      "   (0.00013696036, '#'),\n",
      "   (7.871825e-05, '^'),\n",
      "   (5.576831e-05, '~')],\n",
      "  -1.944851946168662),\n",
      " ([(0.98330337, '^'),\n",
      "   (0.007277961, 'Q'),\n",
      "   (0.0028545922, '!'),\n",
      "   (0.002443288, '$'),\n",
      "   (0.0020506608, '@'),\n",
      "   (0.00094967434, '#'),\n",
      "   (0.0005177544, '\\\\'),\n",
      "   (0.0003117447, '~'),\n",
      "   (0.0002909759, '`')],\n",
      "  -2.1400889916602437),\n",
      " ([(0.91968215, '\\\\'),\n",
      "   (0.032944962, '!'),\n",
      "   (0.031398878, '@'),\n",
      "   (0.01228923, '$'),\n",
      "   (0.0023316042, 'Q'),\n",
      "   (0.0005094729, '#'),\n",
      "   (0.0003296285, '^'),\n",
      "   (0.00028613838, '~'),\n",
      "   (0.00022803883, '`')],\n",
      "  -2.1725458691945105),\n",
      " ([(0.9731228, '#'),\n",
      "   (0.017937109, '@'),\n",
      "   (0.004636237, 'Q'),\n",
      "   (0.0028784955, '$'),\n",
      "   (0.0005548056, '!'),\n",
      "   (0.0003104302, '\\\\'),\n",
      "   (0.00021660207, '~'),\n",
      "   (0.00018734303, '^'),\n",
      "   (0.00015617382, '`')],\n",
      "  -2.2326602287928425),\n",
      " ([(0.74058986, '#'),\n",
      "   (0.2439411, '!'),\n",
      "   (0.010259859, 'Q'),\n",
      "   (0.0028748321, '@'),\n",
      "   (0.0011503856, '$'),\n",
      "   (0.00038077493, '\\\\'),\n",
      "   (0.00028902048, '^'),\n",
      "   (0.00026719333, '~'),\n",
      "   (0.00024693637, '`')],\n",
      "  -2.2388324504699466),\n",
      " ([(0.9950574, '@'),\n",
      "   (0.0020229698, 'Q'),\n",
      "   (0.0012110877, '!'),\n",
      "   (0.0005765657, '$'),\n",
      "   (0.00054105074, '\\\\'),\n",
      "   (0.00019703296, '#'),\n",
      "   (0.00016188272, '`'),\n",
      "   (0.00012338185, '~'),\n",
      "   (0.00010868377, '^')],\n",
      "  -2.2459286611811606),\n",
      " ([(0.9842863, 'Q'),\n",
      "   (0.006116237, '$'),\n",
      "   (0.0055873515, '@'),\n",
      "   (0.003782508, '!'),\n",
      "   (9.484411e-05, '\\\\'),\n",
      "   (7.093035e-05, '#'),\n",
      "   (2.6157115e-05, '~'),\n",
      "   (2.06135e-05, '^'),\n",
      "   (1.5158054e-05, '`')],\n",
      "  -2.3437410739508198),\n",
      " ([(0.72700584, '!'),\n",
      "   (0.26767784, '@'),\n",
      "   (0.0021006144, '$'),\n",
      "   (0.001266175, 'Q'),\n",
      "   (0.0009992127, '#'),\n",
      "   (0.00031160587, '\\\\'),\n",
      "   (0.00023707945, '^'),\n",
      "   (0.00020822271, '~'),\n",
      "   (0.0001933307, '`')],\n",
      "  -2.3481723695925023),\n",
      " ([(0.9493839, '$'),\n",
      "   (0.022059908, '!'),\n",
      "   (0.017207207, 'Q'),\n",
      "   (0.010904143, '@'),\n",
      "   (0.00021510647, '#'),\n",
      "   (7.643251e-05, '\\\\'),\n",
      "   (6.0196944e-05, '~'),\n",
      "   (5.8062033e-05, '^'),\n",
      "   (3.5005727e-05, '`')],\n",
      "  -2.3628379813461198)]\n"
     ]
    }
   ],
   "source": [
    "top_topics = model.top_topics(corpus) #, num_words=20)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "VOgn7Wl9swsB"
   },
   "source": [
    "## Word Movers' Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Demonstrates using Gensim's implemenation of the WMD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "RTN0tDhpswsI"
   },
   "outputs": [],
   "source": [
    "# Initialize logging.\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "sentence_obama = 'Obama speaks to the media in Illinois'\n",
    "sentence_president = 'The president greets the press in Chicago'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "TjIOnZ1CswsP"
   },
   "source": [
    "These sentences have very similar content, and as such the WMD should be low.\n",
    "Before we compute the WMD, we want to remove stopwords (\"the\", \"to\", etc.),\n",
    "as these do not contribute a lot to the information in the sentences.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "hidden": true,
    "id": "DPyuX3VmswsR",
    "outputId": "032c7107-227e-42fb-e35f-0f024fce22fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# Import and download stopwords from NLTK.\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "download('stopwords')  # Download stopwords list.\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def preprocess(sentence):\n",
    "    return [w for w in sentence.lower().split() if w not in stop_words]\n",
    "\n",
    "sentence_obama = preprocess(sentence_obama)\n",
    "sentence_president = preprocess(sentence_president)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "dDg7-4KkswsX"
   },
   "source": [
    "Now, as mentioned earlier, we will be using some downloaded pre-trained\n",
    "embeddings. We load these into a Gensim Word2Vec model class.\n",
    "\n",
    ".. Important::\n",
    "  The embeddings we have chosen here require a lot of memory.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "hidden": true,
    "id": "ZW-VlCMHswsZ",
    "outputId": "28af71b8-1fd9-4e04-a682-3823fb63a3c2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-23 11:22:21,731 : INFO : 'pattern' package not found; tag filters are not available for English\n",
      "2020-09-23 11:22:21,823 : INFO : Creating /root/gensim-data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==========================================--------] 84.8% 1409.9/1662.8MB downloadedBuffered data was truncated after reaching the output size limit."
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "eSeoEAz3swsf"
   },
   "source": [
    "So let's compute WMD using the ``wmdistance`` method.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "hidden": true,
    "id": "QQpDAHYhswsh",
    "outputId": "b9f5f061-2cff-4880-c18d-9aa1380ee365"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-23 11:28:25,225 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-09-23 11:28:25,226 : INFO : built Dictionary(8 unique tokens: ['illinois', 'media', 'obama', 'speaks', 'chicago']...) from 2 documents (total 8 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance = 3.3741\n"
     ]
    }
   ],
   "source": [
    "distance = model.wmdistance(sentence_obama, sentence_president)\n",
    "print('distance = %.4f' % distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "QqVqTgTXswsn"
   },
   "source": [
    "Let's try the same thing with two completely unrelated sentences. Notice that the distance is larger.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "hidden": true,
    "id": "LVe5tkUkswso",
    "outputId": "2b915ab9-79eb-430c-ab27-54e2eea7396c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-23 11:28:25,236 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-09-23 11:28:25,237 : INFO : built Dictionary(7 unique tokens: ['illinois', 'media', 'obama', 'speaks', 'favorite']...) from 2 documents (total 7 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance = 4.3802\n"
     ]
    }
   ],
   "source": [
    "sentence_orange = preprocess('Oranges are my favorite fruit')\n",
    "distance = model.wmdistance(sentence_obama, sentence_orange)\n",
    "print('distance = %.4f' % distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 146
    },
    "hidden": true,
    "id": "8Fm3r_8rswsu",
    "outputId": "6d814d42-a9b6-4317-fdab-f02f7a953099"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-23 11:28:25,247 : INFO : precomputing L2-norms of word weight vectors\n",
      "2020-09-23 11:28:49,398 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-09-23 11:28:49,400 : INFO : built Dictionary(8 unique tokens: ['illinois', 'media', 'obama', 'speaks', 'chicago']...) from 2 documents (total 8 corpus positions)\n",
      "2020-09-23 11:28:49,402 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-09-23 11:28:49,403 : INFO : built Dictionary(7 unique tokens: ['illinois', 'media', 'obama', 'speaks', 'favorite']...) from 2 documents (total 7 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance: 1.0174646259300113\n",
      "distance = 1.3663\n"
     ]
    }
   ],
   "source": [
    "model.init_sims(replace=True)  # Normalizes the vectors in the word2vec class.\n",
    "\n",
    "distance = model.wmdistance(sentence_obama, sentence_president)  # Compute WMD as normal.\n",
    "print('distance: %r' % distance)\n",
    "\n",
    "distance = model.wmdistance(sentence_obama, sentence_orange)\n",
    "print('distance = %.4f' % distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "Word Movers' Distance\n",
    "=====================\n",
    "\n",
    "Demonstrates using Gensim's implemenation of the WMD.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize logging.\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "sentence_obama = 'Obama speaks to the media in Illinois'\n",
    "sentence_president = 'The president greets the press in Chicago'\n",
    "\n",
    "These sentences have very similar content, and as such the WMD should be low.\n",
    "Before we compute the WMD, we want to remove stopwords (\"the\", \"to\", etc.),\n",
    "as these do not contribute a lot to the information in the sentences.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Import and download stopwords from NLTK.\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "download('stopwords')  # Download stopwords list.\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def preprocess(sentence):\n",
    "    return [w for w in sentence.lower().split() if w not in stop_words]\n",
    "\n",
    "sentence_obama = preprocess(sentence_obama)\n",
    "sentence_president = preprocess(sentence_president)\n",
    "\n",
    "Now, as mentioned earlier, we will be using some downloaded pre-trained\n",
    "embeddings. We load these into a Gensim Word2Vec model class.\n",
    "\n",
    ".. Important::\n",
    "  The embeddings we have chosen here require a lot of memory.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import gensim.downloader as api\n",
    "model = api.load('word2vec-google-news-300')\n",
    "\n",
    "So let's compute WMD using the ``wmdistance`` method.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "distance = model.wmdistance(sentence_obama, sentence_president)\n",
    "print('distance = %.4f' % distance)\n",
    "\n",
    "Let's try the same thing with two completely unrelated sentences. Notice that the distance is larger.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sentence_orange = preprocess('Oranges are my favorite fruit')\n",
    "distance = model.wmdistance(sentence_obama, sentence_orange)\n",
    "print('distance = %.4f' % distance)\n",
    "\n",
    "model.init_sims(replace=True)  # Normalizes the vectors in the word2vec class.\n",
    "\n",
    "distance = model.wmdistance(sentence_obama, sentence_president)  # Compute WMD as normal.\n",
    "print('distance: %r' % distance)\n",
    "\n",
    "distance = model.wmdistance(sentence_obama, sentence_orange)\n",
    "print('distance = %.4f' % distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "izA3-6kffbdT"
   },
   "source": [
    "## DistillBERT sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "hidden": true,
    "id": "To9ENLU90WGl",
    "outputId": "c337b24e-1e24-4fd2-e06e-cb760348fea9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
      "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "fvFvBLJV0Dkv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import torch\n",
    "import transformers as ppb\n",
    "import urllib.request\n",
    "import os\n",
    "import zipfile\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "hidden": true,
    "id": "KDW7swH8-Py5",
    "outputId": "3baea8e8-55b8-41b9-bbdc-04c327921953"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.1+cu101\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "PkHh_CVP-BAE"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', delimiter='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "gTM3hOHW4hUY"
   },
   "outputs": [],
   "source": [
    "batch_1 = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "6t-svuwwGo3K"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "hidden": true,
    "id": "RUHzX36pSg3v",
    "outputId": "da372666-b09e-4003-b232-415ae91f8f07"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a stirring , funny and finally transporting re imagining of beauty and the beast and 1930s horror films</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apparently reassembled from the cutting room floor of any given daytime soap</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>they presume their audience wo n't sit still for a sociology lesson , however entertainingly presented , so they trot out the conventional science fiction elements of bug eyed monsters and futuristic women in skimpy clothes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is a visually stunning rumination on love , memory , history and the war between art and commerce</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jonathan parker 's bartleby should have been the be all end all of the modern office anomie films</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                 0  1\n",
       "0                                                                                                                          a stirring , funny and finally transporting re imagining of beauty and the beast and 1930s horror films  1\n",
       "1                                                                                                                                                     apparently reassembled from the cutting room floor of any given daytime soap  0\n",
       "2  they presume their audience wo n't sit still for a sociology lesson , however entertainingly presented , so they trot out the conventional science fiction elements of bug eyed monsters and futuristic women in skimpy clothes  0\n",
       "3                                                                                                                           this is a visually stunning rumination on love , memory , history and the war between art and commerce  1\n",
       "4                                                                                                                                jonathan parker 's bartleby should have been the be all end all of the modern office anomie films  1"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "hidden": true,
    "id": "jGvcfcCP5xpZ",
    "outputId": "0665636b-10f5-4dc6-a779-2d345fc7e3cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3610\n",
       "0    3310\n",
       "Name: 1, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_1[1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "q1InADgf5xm2"
   },
   "outputs": [],
   "source": [
    "# DistilBERT:\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "\n",
    "## Bert\n",
    "#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "\n",
    "# chargement modèle et tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "Dg82ndBA5xlN"
   },
   "outputs": [],
   "source": [
    "# Tokenization de nos phrases pour distillBERT\n",
    "tokenized = batch_1[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "URn-DWJt5xhP"
   },
   "outputs": [],
   "source": [
    "# Padding afin que chaque phrase fasse la même taille\n",
    "\n",
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "hidden": true,
    "id": "jdi7uXo95xeq",
    "outputId": "71667bf4-c66d-4b12-a505-db89f0a1d1a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6920, 67)"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(padded).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "hidden": true,
    "id": "4K_iGRNa_Ozc",
    "outputId": "64c3547c-a0cb-444a-ef7a-ae10956537fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6920, 67)"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On cache ce padding\n",
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "39UVjAV56PJz"
   },
   "outputs": [],
   "source": [
    "# On applique le modèle sur nos token avec le masque\n",
    "model.eval()\n",
    "input_ids = torch.tensor(padded)  \n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "hidden": true,
    "id": "su7oNNFXce0f",
    "outputId": "49347c75-d30c-4493-e3dd-7986bb084ce5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6920, 67, 768])"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(last_hidden_states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "C9t60At16PVs"
   },
   "outputs": [],
   "source": [
    "# Chaque token obtient un vecteur, ici, seul le token special 'cls' en position 1 nous intéresse\n",
    "features = last_hidden_states[0][:,0,:].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "JD3fX2yh6PTx"
   },
   "outputs": [],
   "source": [
    "labels = batch_1[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "iaoEvM2evRx1"
   },
   "source": [
    "### Suite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Entrainnement d'une reg log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "ddAqbkoU6PP9"
   },
   "outputs": [],
   "source": [
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "hidden": true,
    "id": "gG-EVWx4CzBc",
    "outputId": "d0539784-43b4-470f-dfe8-ffe6133dc11d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf = LogisticRegression(C=1)\n",
    "lr_clf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "hidden": true,
    "id": "iCoyxRJ7ECTA",
    "outputId": "862b9f89-ae20-4850-bbc8-4a041483f67c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8601156069364162"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf.score(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Comparaison des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
