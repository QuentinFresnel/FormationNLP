{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "0XJc38cLugS-"
   },
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "KjXq4o9SugS_"
   },
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# !pip install numpy\n",
    "# !pip install pandas\n",
    "# !pip install spacy\n",
    "# !pip install sklearn\n",
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "3oRtjzFQYbMv"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from spacy.lang.fr import French\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "vWzzZG9JYiwB"
   },
   "outputs": [],
   "source": [
    "texteNLP = \"\"\"Le traitement automatique du langage naturel (abr. TALN), ou traitement automatique de la langue naturelle, \n",
    " ou encore traitement automatique des langues (abr. TAL) est un domaine multidisciplinaire impliquant la linguistique,\n",
    " l'informatique et l'intelligence artificielle, qui vise à créer des outils de traitement de la langue naturelle pour diverses applications.\n",
    " Il ne doit pas être confondu avec la linguistique informatique, qui vise à comprendre les langues au moyen d'outils informatiques.\n",
    " Le TALN est sorti des laboratoires de recherche pour être progressivement mis en œuvre dans des applications informatiques nécessitant\n",
    " l'intégration du langage humain à la machine. Aussi le TALN est-il parfois appelé ingénierie linguistique.\n",
    " En France, le traitement automatique de la langue naturelle a sa revue, Traitement automatique des langues,\n",
    " publiée par l’Association pour le traitement automatique des langues (ATALA).\"\"\"\n",
    "\n",
    "texteDataScience = \"\"\"En termes généraux, la science des données est l'extraction de connaissance d'ensembles de données.\n",
    " Elle emploie des techniques et des théories tirées de plusieurs autres domaines plus larges des mathématiques, analyse,\n",
    " optimisation et statistique principalement, la théorie de l'information et la technologie de l'information, notamment le traitement de signal,\n",
    " des modèles probabilistes, l'apprentissage automatique, l'apprentissage statistique, la programmation informatique, l'ingénierie de données,\n",
    " la reconnaissance de formes et l'apprentissage, la visualisation, l'analytique prophétique, la modélisation d'incertitude, le stockage de données,\n",
    " la géo-visualisation, la compression de données et le calcul à haute performance.\n",
    " Les méthodes qui s'adaptent aux données de masse sont particulièrement intéressantes dans la science des données,\n",
    " bien que la discipline ne soit généralement pas considérée comme limitée à ces données.\n",
    "\n",
    "La science des données (en anglais data science) est une discipline qui s'appuie sur des outils mathématiques, de statistiques,\n",
    " d'informatique (cette science est principalement une « science des données numériques ») et de visualisation des données.\n",
    " Elle est en plein développement, dans le monde universitaire ainsi que dans le secteur privé et le secteur public.\n",
    " Moore en 1991 a défini la statistique comme la science des données6 (définition reprise par d'autres dont James T. McClave et al. en 1997)\n",
    " et U. Beck en 2001 oppose la science des données à la science de l'expérience, voyant une dissociation croissante entre ces deux types de science,\n",
    " que tendrait selon lui à encourager une société de la gestion du risque au sein d'une « civilisation du danger ».\"\"\"\n",
    "\n",
    "texteEconometrie = \"\"\"L'économétrie est une branche de la science économique qui a pour objectif d'estimer et de tester les modèles économiques.\n",
    " L'économétrie en tant que discipline naît dans les années 1930 avec la création de la société d'économétrie par Irving Fisher et Ragnar Frisch (1930)\n",
    " et la création de la revue Econometrica (1933). Depuis lors, l'économétrie n'a cessé de se développer et de prendre une importance croissante\n",
    " au sein de la science économique.\n",
    "\n",
    "L'économétrie théorique se focalise essentiellement sur deux questions, l'identification et l'estimation statistique. L'économétrie appliquée\n",
    " utilise les méthodes économétriques pour comprendre des domaines de l'économie comme l'analyse du marché du travail,\n",
    " l'économie de l'éducation ou encore tester la pertinence empirique des modèles de croissance.\n",
    "\n",
    "L'économétrie appliquée utilise aussi bien des données issues d'un protocole expérimental, que ce soit une expérience de laboratoire\n",
    " ou une expérience de terrain, que des données issues directement de l'observation du réel sans manipulation du chercheur.\n",
    " Lorsque l'économètre utilise des données issues directement de l'observation du réel, il est fréquent d'identifier des expériences naturelles\n",
    " pour retrouver une situation quasi-expérimentale. On parle parfois de révolution de crédibilité, terme controversé, pour désigner l'essor fulgurant\n",
    " de ces méthodes de recherche dans la discipline, et en économie en général.\"\"\"\n",
    "\n",
    "texteHistoire = \"\"\"L’histoire, souvent écrit avec la première lettre majuscule,\n",
    " est à la fois l’étude et l'écriture des faits et des événements passés quelles que soient leur variété et leur complexité.\n",
    " L'histoire est également une science humaine et sociale. On désigne aussi couramment sous le terme d’histoire (par synecdoque) le passé lui-même,\n",
    " comme dans les leçons de l'histoire. L'histoire est un récit écrit par lequel des hommes et des femmes (les historiens et historiennes)\n",
    " s'efforcent de faire connaître les temps révolus. Ces tentatives ne sont jamais entièrement indépendantes de conditionnements étrangers au domaine\n",
    " telle que la vision du monde de leur auteur ou de sa culture, mais elles sont censées être élaborées à partir de sources plutôt que guidées\n",
    " par la spéculation ou l'idéologie.\n",
    "\n",
    "Au cours des siècles, les historiens ont façonné leurs méthodes ainsi que les champs d'intervention, tout en réévaluant leurs sources,\n",
    " leur origine et leur exploitation. La discipline universitaire d'étude et écriture de l'histoire, y comprise la critique des méthodes,\n",
    " est l'historiographie. Elle s'appuie sur diverses sciences auxiliaires complétant selon les travaux menés la compétence générale de l'historien.\n",
    " Elle reste malgré tout une construction humaine, inévitablement inscrite dans son époque, susceptible d'être utilisée en dehors de son domaine,\n",
    " notamment à des fins d'ordre politique. \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "UH2LKmshYkDM"
   },
   "outputs": [],
   "source": [
    "nlp = French()\n",
    "\n",
    "documentNLP = nlp(texteNLP)\n",
    "documentDataScience = nlp(texteDataScience)\n",
    "documentEconometrie = nlp(texteEconometrie)\n",
    "documentHistoire = nlp(texteHistoire)\n",
    "\n",
    "bagOfWordsNLP = []\n",
    "bagOfWordsDataScience = []\n",
    "bagOfWordsEconometrie = []\n",
    "bagOfWordsHistoire = []\n",
    "\n",
    "for token in documentNLP:\n",
    "    bagOfWordsNLP.append(token.text)\n",
    "\n",
    "for token in documentDataScience:\n",
    "    bagOfWordsDataScience.append(token.text)\n",
    "\n",
    "for token in documentEconometrie:\n",
    "    bagOfWordsEconometrie.append(token.text)\n",
    "\n",
    "for token in documentHistoire:\n",
    "    bagOfWordsHistoire.append(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "DUmZrE2lYk42"
   },
   "outputs": [],
   "source": [
    "uniqueWords = set(bagOfWordsNLP).union(set(bagOfWordsDataScience)).union(set(bagOfWordsEconometrie)).union(set(bagOfWordsHistoire))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "U2k-oq2MYlFy"
   },
   "outputs": [],
   "source": [
    "numOfWordsNLP = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsNLP:\n",
    "    numOfWordsNLP[word] += 1\n",
    "\n",
    "numOfWordsDataScience = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsDataScience:\n",
    "    numOfWordsDataScience[word] += 1\n",
    "\n",
    "numOfWordsEconometrie = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsEconometrie:\n",
    "    numOfWordsEconometrie[word] += 1\n",
    "\n",
    "numOfWordsHistoire = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsHistoire:\n",
    "    numOfWordsHistoire[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "U4UZsBqBYlac"
   },
   "outputs": [],
   "source": [
    "def computeTF(wordDict, bagOfWords):\n",
    "    tfDict = {}\n",
    "    bagOfWordsCount = len(bagOfWords)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count / float(bagOfWordsCount)\n",
    "    return tfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "1eGebIApYlje"
   },
   "outputs": [],
   "source": [
    "tfNLP = computeTF(numOfWordsNLP, bagOfWordsNLP)\n",
    "tfDataScience = computeTF(numOfWordsDataScience, bagOfWordsDataScience)\n",
    "tfEconometrie = computeTF(numOfWordsEconometrie, bagOfWordsEconometrie)\n",
    "tfHistoire = computeTF(numOfWordsHistoire, bagOfWordsHistoire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "yw240CWgYlsr"
   },
   "outputs": [],
   "source": [
    "def computeIDF(documents):\n",
    "    import math\n",
    "    N = len(documents)\n",
    "    \n",
    "    idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
    "    for document in documents:\n",
    "        for word, val in document.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log(N / float(val))\n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "C7_ejnuOZWez"
   },
   "outputs": [],
   "source": [
    "idfs = computeIDF([numOfWordsNLP, numOfWordsDataScience, numOfWordsEconometrie, numOfWordsHistoire])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "vW2xwamlZWmq"
   },
   "outputs": [],
   "source": [
    "def computeTFIDF(tfBagOfWords, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBagOfWords.items():\n",
    "        tfidf[word] = val * idfs[word]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "TB2O-vKAZWtm"
   },
   "outputs": [],
   "source": [
    "tfidfNLP = computeTFIDF(tfNLP, idfs)\n",
    "tfidfDataScience = computeTFIDF(tfDataScience, idfs)\n",
    "tfidfEconometrie = computeTFIDF(tfEconometrie, idfs)\n",
    "tfidfHistoire = computeTFIDF(tfHistoire, idfs)\n",
    "df = pd.DataFrame([tfidfNLP, tfidfDataScience, tfidfEconometrie, tfidfHistoire])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "4KiDyXbubBF1"
   },
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "oLiRFXtzZWzE"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform([texteNLP, texteDataScience, texteEconometrie, texteHistoire])\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "df = pd.DataFrame(denselist, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "w6LHAKsYZW37"
   },
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "rmYJ_oGYZW8Y"
   },
   "outputs": [],
   "source": [
    "df.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "3o4KAtR6ZXAs"
   },
   "outputs": [],
   "source": [
    "dist_NLP_DataScience = np.linalg.norm(df.loc[0] - df.loc[1])\n",
    "dist_NLP_Econometrie = np.linalg.norm(df.loc[0] - df.loc[2])\n",
    "dist_NLP_Histoire = np.linalg.norm(df.loc[0] - df.loc[3])\n",
    "dist_DataScience_Econometrie = np.linalg.norm(df.loc[1] - df.loc[2])\n",
    "dist_DataScience_Histoire = np.linalg.norm(df.loc[1] - df.loc[3])\n",
    "dist_Econometrie_Histoire = np.linalg.norm(df.loc[2] - df.loc[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "qVQzDI2EZXE3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(dist_NLP_DataScience)\n",
    "print(dist_NLP_Econometrie)\n",
    "print(dist_NLP_Histoire)\n",
    "print(dist_DataScience_Econometrie)\n",
    "print(dist_DataScience_Histoire)\n",
    "print(dist_Econometrie_Histoire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "czg_laqsugUU"
   },
   "outputs": [],
   "source": [
    "data = df.values\n",
    "X = data.data\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "mds = MDS(2,random_state=0)\n",
    "X_2d = mds.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "L8p1DFP9ugUa"
   },
   "outputs": [],
   "source": [
    "x = [row[0] for row in X_2d]\n",
    "y = [row[1] for row in X_2d]\n",
    "\n",
    "group = np.array(['NLP', 'DataScience', 'Econometrie', 'Histoire'])\n",
    "cdict = {0: 'red', 1: 'green', 2: 'blue', 3: 'orange'}\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for i in range(0, 4):\n",
    "    ax.scatter(x[i], y[i], c = cdict[i], label = group[i])\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "bafsUlFOugUe"
   },
   "source": [
    "# Embeddings de mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "WMGgjIn2k1Ig"
   },
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "nAftgmbGaCey"
   },
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "YEu7vZHiugUg"
   },
   "outputs": [],
   "source": [
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "dww0XB9-ZXXm"
   },
   "outputs": [],
   "source": [
    "import gensim \n",
    "from gensim.models import Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "xYaB2OBaZXbG"
   },
   "outputs": [],
   "source": [
    "texte1 = \"\"\"L’exploration de données, connue aussi sous l'expression de fouille de données, forage de données, prospection de données, data mining,\n",
    " ou encore extraction de connaissances à partir de données, a pour objet l’extraction d'un savoir ou d'une connaissance à partir de grandes quantités\n",
    " de données, par des méthodes automatiques ou semi-automatiques.\n",
    "Elle se propose d'utiliser un ensemble d'algorithmes issus de disciplines scientifiques diverses telles que les statistiques,\n",
    " l'intelligence artificielle ou l'informatique, pour construire des modèles à partir des données,\n",
    " c'est-à-dire trouver des structures intéressantes ou des motifs selon des critères fixés au préalable,\n",
    " et d'en extraire un maximum de connaissances.\n",
    "L'utilisation industrielle ou opérationnelle de ce savoir dans le monde professionnel permet de résoudre des problèmes très divers,\n",
    " allant de la gestion de la relation client à la maintenance préventive, en passant par la détection de fraudes ou encore l'optimisation de sites web.\n",
    "C'est aussi le mode de travail du journalisme de données.\n",
    "L'exploration de données fait suite, dans l'escalade de l'exploitation des données de l'entreprise, à l'informatique décisionnelle.\n",
    "Celle-ci permet de constater un fait, tel que le chiffre d'affaires, et de l'expliquer comme le chiffre d'affaires décliné par produits,\n",
    " tandis que l'exploration de données permet de classer les faits et de les prévoir dans une certaine mesure ou encore de les éclairer en révélant\n",
    " par exemple les variables ou paramètres qui pourraient faire comprendre pourquoi le chiffre d'affaires de tel point de vente est supérieur\n",
    " à celui de tel autre. \"\"\"\n",
    "\n",
    "texte2 = \"\"\"En statistique, les analyses multivariées ont pour caractéristique de s'intéresser à des lois de probabilité à plusieurs variables.\n",
    "Les analyses bivariées sont des cas particuliers à deux variables.\n",
    "Les analyses multivariées sont très diverses selon l'objectif recherché, la nature des variables et la mise en œuvre formelle.\n",
    "On peut identifier deux grandes familles : celle des méthodes descriptives (visant à structurer et résumer l'information)\n",
    " et celle des méthodes explicatives visant à expliquer une ou des variables dites « dépendantes » (variables à expliquer) par un ensemble de variables\n",
    " dites « indépendantes » (variables explicatives).\n",
    "Les méthodes appelées en français analyse des données en sont un sous-ensemble. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "js32mArVugUp"
   },
   "outputs": [],
   "source": [
    "texte = nlp(texte1 + texte2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "Xd-JukqaZXeg"
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "# iterate through each sentence in the file \n",
    "for sent in texte: \n",
    "    temp = []\n",
    "      \n",
    "    # tokenize the sentence into words \n",
    "    for token in texte: \n",
    "        temp.append(token.text.lower()) \n",
    "  \n",
    "    data.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "hH25xm_jZXh7"
   },
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "3QFXtLWm6wch"
   },
   "source": [
    "### Continuous bag of words (CBOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "b4tlfQB561cS"
   },
   "source": [
    "Le modèle CBOW prédit le mot courant étant donné les mots de contexte dans une fenêtre autour du mot courant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "A5jZxGw0ZXlm"
   },
   "outputs": [],
   "source": [
    "# Create CBOW model \n",
    "model1 = gensim.models.Word2Vec(data, min_count = 1,  \n",
    "                              size = 100, window = 5,\n",
    "                              sg = 0)\n",
    "\n",
    "# Print results\n",
    "print(\"Cosine similarity between 'données' \" + \n",
    "               \"and 'connaissance' - CBOW : \", \n",
    "    model1.wv.similarity('données', 'connaissance'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "NSBSotcP7gna"
   },
   "source": [
    "### Skip gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "OM3PLzyv7hQb"
   },
   "source": [
    "La méthode \"skip gram\" fait le contraire de ce que fait la méthode \"cbow\" : elle prédit les mots de contexte d'un mot donné."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "6A0xo1WJZXpV"
   },
   "outputs": [],
   "source": [
    "# Create Skip Gram model \n",
    "model2 = gensim.models.Word2Vec(data, min_count = 1,\n",
    "                                size = 100, window = 5,\n",
    "                                sg = 1) \n",
    "\n",
    "# Print results \n",
    "print(\"Cosine similarity between 'données' \" +\n",
    "          \"and 'connaissance' - Skip Gram : \", \n",
    "    model2.wv.similarity('données', 'connaissance')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "SAEF7KdfZXwJ"
   },
   "outputs": [],
   "source": [
    "word_vectors = model1.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "J740GCysZXtC"
   },
   "outputs": [],
   "source": [
    "print(word_vectors['données'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "VgLh1oxr-0vv"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# api.info(\"glove-wiki-gigaword-100\")\n",
    "word_vectors = api.load(\"glove-wiki-gigaword-100\")  # load pre-trained word-vectors from gensim-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "j_FlOX5e-19a"
   },
   "outputs": [],
   "source": [
    "result = word_vectors.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "print(\"{}: {:.4f}\".format(*result[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "sszMfjlFjVnp"
   },
   "outputs": [],
   "source": [
    "result = word_vectors.most_similar(positive=['female', 'lion'], negative=['male'])\n",
    "print(\"{}: {:.4f}\".format(*result[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "08wJCNXF-2Xy"
   },
   "outputs": [],
   "source": [
    "print(word_vectors.doesnt_match(\"breakfast cereal dinner lunch\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "8uWeTuU2-2g0"
   },
   "outputs": [],
   "source": [
    "result = word_vectors.similar_by_word(\"cat\")\n",
    "print(\"{}: {:.4f}\".format(*result[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "z0DVkHDjsS4z"
   },
   "source": [
    "### FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "SMRAVtne-2ny"
   },
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "# from gensim.test.utils import common_texts\n",
    "# print(common_texts[0])\n",
    "# ['human', 'interface', 'computer']\n",
    "\n",
    "model = FastText(data, size=100, window=5, min_count=5, workers=4, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "oPPHel-8-2ts"
   },
   "outputs": [],
   "source": [
    "model.wv.most_similar(\"données\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "2WRTrD6Sv1nz"
   },
   "outputs": [],
   "source": [
    "!pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "LHBYQpJJ-2zC"
   },
   "outputs": [],
   "source": [
    "import fasttext.util\n",
    "\n",
    "fasttext.util.download_model('en', if_exists='ignore')\n",
    "ft = fasttext.load_model('cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "PFt0bOct-27l"
   },
   "outputs": [],
   "source": [
    "ft.get_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "6RRvkNjD-2_n"
   },
   "outputs": [],
   "source": [
    "# fasttext.util.reduce_model(ft, 100)\n",
    "# ft.get_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "FtvT4wQV-3DT"
   },
   "outputs": [],
   "source": [
    "ft.get_word_vector('hello').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "YFUESWkK-3HX"
   },
   "outputs": [],
   "source": [
    "ft.get_nearest_neighbors('hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "ZgdS6lYaugWD"
   },
   "source": [
    "## Embeddings keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "SiwVPArJugWE"
   },
   "outputs": [],
   "source": [
    "# import keras\n",
    "# import spacy\n",
    "# from keras.preprocessing.text import one_hot\n",
    "# from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "y_JRBXhCLX4g"
   },
   "outputs": [],
   "source": [
    "# Ignore  the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# data visualisation and manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "#configure\n",
    "# sets matplotlib to inline and displays graphs below the corressponding cell.\n",
    "%matplotlib inline  \n",
    "style.use('fivethirtyeight')\n",
    "sns.set(style='whitegrid',color_codes=True)\n",
    "\n",
    "#nltk\n",
    "import nltk\n",
    "\n",
    "#stop-words\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# tokenizing\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "\n",
    "#keras\n",
    "import keras\n",
    "from keras.preprocessing.text import one_hot,Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense , Flatten ,Embedding,Input\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "P6SIjdm6ugWH"
   },
   "source": [
    "Création d'un corpus d'exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "jdWm6_gpugWL"
   },
   "outputs": [],
   "source": [
    "sample_text_1=\"bitty bought a bit of butter\"\n",
    "sample_text_2=\"but the bit of butter was a bit bitter\"\n",
    "sample_text_3=\"so she bought some better butter to make the bitter butter better\"\n",
    "\n",
    "corp=[sample_text_1,sample_text_2,sample_text_3]\n",
    "no_docs=len(corp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "rkSjCyn2ugWO"
   },
   "source": [
    "Encodage du corpus en one-hot à l'aide de la fonction keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "uQQWfWYBugWQ"
   },
   "outputs": [],
   "source": [
    "vocab_size=50 \n",
    "encod_corp=[]\n",
    "for i,doc in enumerate(corp):\n",
    "    # taille de vocab 50 pour être sur que chaque mot est encodé sur un entier unique.\n",
    "    encod_corp.append(one_hot(doc,50))\n",
    "    print(\"The encoding for document\",i+1,\" is : \",one_hot(doc,50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "n4Db_qIbugWV"
   },
   "source": [
    "Padding des documents : la couche d'embedding de keras nécessite des entrées de la même longueur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "Y5RMH3OBugWV"
   },
   "outputs": [],
   "source": [
    "#!python -m spacy download fr_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "zMKxL5ENugWX"
   },
   "outputs": [],
   "source": [
    "# nlp = spacy.load('fr_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "R63nhyPbugWd"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "fSru2I-jugWg"
   },
   "outputs": [],
   "source": [
    "# length of maximum document. will be nedded whenever create embeddings for the words\n",
    "maxlen = -1\n",
    "for doc in corp:\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    if(maxlen < len(tokens)):\n",
    "        maxlen = len(tokens)\n",
    "print(\"The maximum number of words in any document is : \",maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "ug4-iq48ugWn"
   },
   "outputs": [],
   "source": [
    "# now to create embeddings all of our docs need to be of same length. hence we can pad the docs with zeros.\n",
    "pad_corp=pad_sequences(encod_corp,maxlen=maxlen,padding='post',value=0.0)\n",
    "print(\"No of padded documents: \",len(pad_corp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "eoISsYrGugWq"
   },
   "outputs": [],
   "source": [
    "for i,doc in enumerate(pad_corp):\n",
    "     print(\"The padded encoding for document\",i+1,\" is : \",doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "yiIvcDDYugWy"
   },
   "source": [
    "Création d'une couche d'embedding de dimension 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "GdOdRysAugWy"
   },
   "outputs": [],
   "source": [
    "# specifying the input shape\n",
    "input=Input(shape=(no_docs,maxlen),dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "M4hjleycugW3"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "shape of input. \n",
    "each document has 12 element or words which is the value of our maxlen variable.\n",
    "\n",
    "'''\n",
    "word_input=Input(shape=(maxlen,),dtype='float64')  \n",
    "\n",
    "# creating the embedding\n",
    "word_embedding=Embedding(input_dim=vocab_size,output_dim=8,input_length=maxlen)(word_input)\n",
    "\n",
    "word_vec=Flatten()(word_embedding) # flatten\n",
    "embed_model =Model([word_input],word_vec) # combining all into a Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "K5PoLHYCugW7"
   },
   "outputs": [],
   "source": [
    "embed_model.compile(optimizer=keras.optimizers.Adam(lr=1e-3),loss='binary_crossentropy',metrics=['acc']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "GR1xJNaOugW-"
   },
   "outputs": [],
   "source": [
    "print(type(word_embedding))\n",
    "print(word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "2eOGhpZWugXA"
   },
   "outputs": [],
   "source": [
    "print(embed_model.summary()) # summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "oozzCv4hugXI"
   },
   "outputs": [],
   "source": [
    "embeddings=embed_model.predict(pad_corp) # finally getting the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "hvMcmP1uugXK"
   },
   "outputs": [],
   "source": [
    "print(\"Shape of embeddings : \",embeddings.shape)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "Pv_NYOc5ugXN"
   },
   "outputs": [],
   "source": [
    "embeddings=embeddings.reshape(-1,maxlen,8)\n",
    "print(\"Shape of embeddings : \",embeddings.shape) \n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "HfvmBJJWugXP"
   },
   "outputs": [],
   "source": [
    "for i,doc in enumerate(embeddings):\n",
    "    for j,word in enumerate(doc):\n",
    "        print(\"The encoding for \",j+1,\"th word\",\"in\",i+1,\"th document is : \\n\\n\",word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "fa_jPXzmugXU"
   },
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "UE1nsX6GaLVf"
   },
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "OeNLky0gStWm"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "19f-J2RgugXV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import CamembertTokenizer, CamembertConfig\n",
    "from transformers import CamembertForTokenClassification\n",
    "from tqdm import tqdm, trange\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "tUu3j8t1awJ8"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "webUrl  = urllib.request.urlopen('https://www.youtube.com/user/guru99com')\n",
    "\n",
    "#get the result code and print it\n",
    "print (\"result code: \" + str(webUrl.getcode()))\n",
    "\n",
    "# read the data from the URL and print it\n",
    "data = webUrl.read()\n",
    "print (data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "Sf3efIsbugXY"
   },
   "outputs": [],
   "source": [
    "sentences, labels = [], []\n",
    "with open(\"frwikinews-20130110-pages-articles.txt.tok.stanford-pos\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f.readlines():\n",
    "        sentence = []\n",
    "        sent_tag = []\n",
    "        tokens = line.replace(\"\\n\", \"\").split(\" \")\n",
    "        for token in tokens:\n",
    "            splits = token.split(\"_\")\n",
    "            if len(splits) != 2: continue\n",
    "            word, tag = splits\n",
    "            sentence.append(word)\n",
    "            sent_tag.append(tag)\n",
    "        sentences.append(\" \".join(sentence))\n",
    "        labels.append(sent_tag)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "V0iIY9ySugXc"
   },
   "outputs": [],
   "source": [
    "tags_val = list(set().union(*labels))\n",
    "tag2idx = {t:i for i,t in enumerate(tags_val)}\n",
    "# tag2idx[\"<PAD>\"] = len(tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "CMz9-jBOugXe"
   },
   "outputs": [],
   "source": [
    "lens = np.array(list(map(len, sentences)))\n",
    "lens.min(), lens.max(), lens.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "QjM22x6uugXf"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 150\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "iWro_bBmugXh"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "Jy45Etr_ugXk"
   },
   "outputs": [],
   "source": [
    "tokenizer = CamembertTokenizer.from_pretrained('camembert-base/', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "b3uQ47jPugXo"
   },
   "outputs": [],
   "source": [
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "print(tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "l3cCmcKwugXr"
   },
   "outputs": [],
   "source": [
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "qPTcMEE4ugXw"
   },
   "outputs": [],
   "source": [
    "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
    "                     maxlen=MAX_LEN, value=tag2idx[\"PONCT\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "GgteGnQgugXy"
   },
   "outputs": [],
   "source": [
    "attention_masks = [[float(i>0) for i in ii] for ii in input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "Cw5On4hBugX4"
   },
   "outputs": [],
   "source": [
    "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags, \n",
    "                                                            random_state=2018, test_size=0.1)\n",
    "tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2018, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "Vl1qkKxJugX5"
   },
   "outputs": [],
   "source": [
    "tr_inputs = torch.tensor(tr_inputs)\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "tr_tags = torch.tensor(tr_tags)\n",
    "val_tags = torch.tensor(val_tags)\n",
    "tr_masks = torch.tensor(tr_masks)\n",
    "val_masks = torch.tensor(val_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "hYeyPKN0ugX7"
   },
   "outputs": [],
   "source": [
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "2kb0kIfrugX-"
   },
   "outputs": [],
   "source": [
    "model = CamembertForTokenClassification.from_pretrained(\"camembert-base/\", num_labels=len(tag2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "G_5wO3IZugYB"
   },
   "outputs": [],
   "source": [
    "FULL_FINETUNING = True #True\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters()) \n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "09_ILR9kugYD"
   },
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "HF4C40kqugYG"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "8uiDsB5JugYI"
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "max_grad_norm = 1.0\n",
    "total_t0 = time.time()\n",
    "\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    t0 = time.time()\n",
    "    # TRAIN loop\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        \n",
    "        if step % 10 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "            \n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # forward pass\n",
    "        loss, something_else = model(b_input_ids, token_type_ids=None,\n",
    "                     attention_mask=b_input_mask, labels=b_labels)\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # track train loss\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "    # print train loss per epoch\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    # VALIDATION on validation set\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions , true_labels = [], []\n",
    "    for batch in valid_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n",
    "                                  attention_mask=b_input_mask, labels=b_labels)[0]\n",
    "            logits = model(b_input_ids, token_type_ids=None,\n",
    "                           attention_mask=b_input_mask)[0]\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        true_labels.append(label_ids)\n",
    "        \n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        \n",
    "        nb_eval_examples += b_input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "    eval_loss = eval_loss/nb_eval_steps\n",
    "    print(\"Validation loss: {}\".format(eval_loss))\n",
    "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "    pred_tags = [tags_val[p_i] for p in predictions for p_i in p]\n",
    "    valid_tags = [tags_val[l_ii] for l in true_labels for l_i in l for l_ii in l_i]\n",
    "    print(\"F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "md9HL4PNugYM"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "for batch in valid_dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n",
    "                              attention_mask=b_input_mask, labels=b_labels)[0]\n",
    "        logits = model(b_input_ids, token_type_ids=None,\n",
    "                       attention_mask=b_input_mask)[0]\n",
    "        \n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    true_labels.append(label_ids)\n",
    "#     tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "\n",
    "    eval_loss += tmp_eval_loss.mean().item()\n",
    "#     eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    nb_eval_examples += b_input_ids.size(0)\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "pred_tags = [[tags_val[p_i] for p_i in p] for p in predictions]\n",
    "valid_tags = [[tags_val[l_ii] for l_ii in l_i] for l in true_labels for l_i in l ]\n",
    "print(\"Validation loss: {}\".format(eval_loss/nb_eval_steps))\n",
    "# print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "print(\"Validation F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "ZoLXBSpDugYQ"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model, open('CamemBERT_POS', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "PYikfpbLugYS"
   },
   "source": [
    "# Embeddings de doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "BjCIw55xugYU"
   },
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "_YKBbdbB8V3B"
   },
   "source": [
    "Utilisation du modèle Doc2Vec de Gensim  \n",
    "\n",
    "Demonstration sur le corpus lee Background, 314 documents obtenus à partir d'un service de résumé d'article par mail australien sur une grande variété de sujets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "CRKcnSLzLKs5"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "0izPSUqCLKtE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "# Set file names for train and test data\n",
    "test_data_dir = os.path.join(gensim.__path__[0], 'test', 'test_data')\n",
    "lee_train_file = os.path.join(test_data_dir, 'lee_background.cor')\n",
    "lee_test_file = os.path.join(test_data_dir, 'lee.cor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "Ydji5yVtLKtM"
   },
   "outputs": [],
   "source": [
    "import smart_open\n",
    "\n",
    "def read_corpus(fname, tokens_only=False):\n",
    "    with smart_open.open(fname, encoding=\"iso-8859-1\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            tokens = gensim.utils.simple_preprocess(line)\n",
    "            if tokens_only:\n",
    "                yield tokens\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n",
    "\n",
    "train_corpus = list(read_corpus(lee_train_file))\n",
    "test_corpus = list(read_corpus(lee_test_file, tokens_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "Z8gIsgByLKtT"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "Aperçu du jeu de train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "6hvPFckPLKtU"
   },
   "outputs": [],
   "source": [
    "print(train_corpus[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "mJweA-XfLKtb"
   },
   "source": [
    "Aperçu du jeu de test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "oMrYDqBhLKtd"
   },
   "outputs": [],
   "source": [
    "print(test_corpus[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "0fFw9-GNLKtk"
   },
   "source": [
    "### Entrainement\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "1hMFvbhJLKtl"
   },
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "AawhBdwXLKtp"
   },
   "source": [
    "Construction du vocabulaire\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "IsYsdsARLKtr"
   },
   "outputs": [],
   "source": [
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "ZMTrLw3LCSjT"
   },
   "source": [
    "Entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "NwQWJNdGLKt0"
   },
   "outputs": [],
   "source": [
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "yZRrhB2nLKt5"
   },
   "source": [
    "la fonction ``model.infer_vector`` nous permet maintenant de générer l'embeddings.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "TO9U7nxCLKt7"
   },
   "outputs": [],
   "source": [
    "vector = model.infer_vector(['only', 'you', 'can', 'prevent', 'forest', 'fires', 'really', 'da'])\n",
    "print(vector, len(vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "g424ZE1WLKuC"
   },
   "source": [
    "### Essai sur nos données d'entrainement\n",
    "-------------------\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "y4tC54xDLKuE"
   },
   "outputs": [],
   "source": [
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(train_corpus)):\n",
    "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "    second_ranks.append(sims[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "P9_hmTRKEh2u"
   },
   "outputs": [],
   "source": [
    "second_ranks[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "ineM8KXNB3G3"
   },
   "outputs": [],
   "source": [
    "print(sims[:5], '\\n', sims[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "4uI9ofS6LKuO"
   },
   "outputs": [],
   "source": [
    "display('Document ({}): «{}»'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "print()\n",
    "display(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:' % model)\n",
    "print()\n",
    "for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    display(u'%s %s: «%s»' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "nywYZwvkLKuT"
   },
   "source": [
    "Le document le plus proche est le même ... Cohérent, mais il est pertinent de regarder le second plus proche, pour juger de la pertinence de l'algorithme, ainsi que le plus éloigné.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "uV5KV7T7LKuT"
   },
   "outputs": [],
   "source": [
    "# Pick a random document from the corpus and infer a vector from the model\n",
    "import random\n",
    "doc_id = random.randint(0, len(train_corpus) - 1)\n",
    "# Compare and print the second-most-similar document\n",
    "display('Train Document ({}): «{}»'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "sim_id = second_ranks[doc_id]\n",
    "print()\n",
    "display('Similar Document {}: «{}»'.format(sim_id, ' '.join(train_corpus[sim_id[0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "i9Sppy4SLKuW"
   },
   "source": [
    "\n",
    "### Test\n",
    "-----------------\n",
    "\n",
    "On va créer l'embedding à partir d'un vecteur du jeu de test et trouver celui du dataset d'entrainement le plus proche\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "wrnlmeNwLKuX"
   },
   "outputs": [],
   "source": [
    "# Pick a random document from the test corpus and infer a vector from the model\n",
    "doc_id = random.randint(0, len(test_corpus) - 1)\n",
    "inferred_vector = model.infer_vector(test_corpus[doc_id])\n",
    "sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "\n",
    "# Compare and print the most/median/least similar documents from the train corpus\n",
    "display('Test Document ({}): «{}»'.format(doc_id, ' '.join(test_corpus[doc_id])))\n",
    "print()\n",
    "display(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:' % model)\n",
    "print()\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    display(u'%s %s: «%s»' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "TGp0qeXvtFeW"
   },
   "source": [
    "## LDA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "g424ZE1WLKuC"
   },
   "source": [
    "Assessing the Model\n",
    "-------------------\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "ulPfI3kv8V3y"
   },
   "source": [
    "Modele LDA de Gensim, avec le corpus NIPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "ToR80j6jYeGC"
   },
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "w2Hcagg7tFeY"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "eHfA8Ss9tFel"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os.path\n",
    "import re\n",
    "import tarfile\n",
    "import nltk\n",
    "\n",
    "import smart_open\n",
    "\n",
    "def extract_documents(url='https://cs.nyu.edu/~roweis/data/nips12raw_str602.tgz'):\n",
    "    fname = url.split('/')[-1]\n",
    "    \n",
    "    # Download the file to local storage first.\n",
    "    # We can't read it on the fly because of \n",
    "    # https://github.com/RaRe-Technologies/smart_open/issues/331\n",
    "    if not os.path.isfile(fname):\n",
    "        with smart_open.open(url, \"rb\") as fin:\n",
    "            with smart_open.open(fname, 'wb') as fout:\n",
    "                while True:\n",
    "                    buf = fin.read(io.DEFAULT_BUFFER_SIZE)\n",
    "                    if not buf:\n",
    "                        break\n",
    "                    fout.write(buf)\n",
    "                         \n",
    "    with tarfile.open(fname, mode='r:gz') as tar:\n",
    "        # Ignore directory entries, as well as files like README, etc.\n",
    "        files = [\n",
    "            m for m in tar.getmembers()\n",
    "            if m.isfile() and re.search(r'nipstxt/nips\\d+/\\d+\\.txt', m.name)\n",
    "        ]\n",
    "        for member in sorted(files, key=lambda x: x.name):\n",
    "            member_bytes = tar.extractfile(member).read()\n",
    "            yield member_bytes.decode('utf-8', errors='replace')\n",
    "\n",
    "docs = list(extract_documents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "1wUTd2bctFe1"
   },
   "outputs": [],
   "source": [
    "print(len(docs))\n",
    "print(docs[0][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "Lr2-gpuHtFe8"
   },
   "outputs": [],
   "source": [
    "# Tokenize the documents.\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Split the documents into tokens.\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for idx in range(len(docs)):\n",
    "    docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "    docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "# Remove numbers, but not words that contain numbers.\n",
    "docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "\n",
    "# Remove words that are only one character.\n",
    "docs = [[token for token in doc if len(token) > 1] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "p2D2GTM-tFfE"
   },
   "outputs": [],
   "source": [
    "# Lemmatize the documents.\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "_bwLiLmptFfL"
   },
   "outputs": [],
   "source": [
    "# Compute bigrams.\n",
    "from gensim.models import Phrases\n",
    "\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "CRNGpD0EtFfP"
   },
   "outputs": [],
   "source": [
    "# Remove rare and common tokens.\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "n_hZjiu3tFfV"
   },
   "outputs": [],
   "source": [
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "mJf9QhAatFfc"
   },
   "outputs": [],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "qKqG2x8ytFfj",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train LDA model.\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 10\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "AZr6b66itFfr",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_topics = model.top_topics(corpus) #, num_words=20)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "VOgn7Wl9swsB"
   },
   "source": [
    "## Word Movers' Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "clYphaxP8V4K"
   },
   "source": [
    "Modèle WMD de Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "Y8ZMoUZSYk3r"
   },
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "RTN0tDhpswsI"
   },
   "outputs": [],
   "source": [
    "# Initialize logging.\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "sentence_obama = 'Obama speaks to the media in Illinois'\n",
    "sentence_president = 'The president greets the press in Chicago'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "TjIOnZ1CswsP"
   },
   "source": [
    "These sentences have very similar content, and as such the WMD should be low.\n",
    "Before we compute the WMD, we want to remove stopwords (\"the\", \"to\", etc.),\n",
    "as these do not contribute a lot to the information in the sentences.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "loN6LYHbI295"
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "jrC0uLn2KDiQ"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "print(list(STOP_WORDS)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "7aFAmyu_Kaze"
   },
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    sentence=nlp(sentence)\n",
    "    return [token.text.lower() for token in sentence if not token.text.lower() in STOP_WORDS and not token.is_punct]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "kaVrbptAKwY8"
   },
   "outputs": [],
   "source": [
    "sentence_obama = preprocess(sentence_obama)\n",
    "sentence_president = preprocess(sentence_president)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "dDg7-4KkswsX"
   },
   "source": [
    "On va maintenant utiliser les poids pré entrainés d'un model Word2Vec de Gensim\n",
    "\n",
    "Attention cet embedding prend beaucoup de mémoire.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "ZW-VlCMHswsZ"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "eSeoEAz3swsf"
   },
   "source": [
    "On va maintenant calculer la distance avec la méthod ``wmdistance`` .\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "QQpDAHYhswsh"
   },
   "outputs": [],
   "source": [
    "distance = model.wmdistance(sentence_obama, sentence_president)\n",
    "print('distance = %.4f' % distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "QqVqTgTXswsn"
   },
   "source": [
    "Si on essaye avec des phrases dont le sens est différent\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "LVe5tkUkswso"
   },
   "outputs": [],
   "source": [
    "sentence_orange = preprocess('Oranges are my favorite fruit')\n",
    "distance = model.wmdistance(sentence_obama, sentence_orange)\n",
    "print('distance = %.4f' % distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "8Fm3r_8rswsu"
   },
   "outputs": [],
   "source": [
    "model.init_sims(replace=True)  # Normalizes the vectors in the word2vec class.\n",
    "\n",
    "distance = model.wmdistance(sentence_obama, sentence_president)  # Compute WMD as normal.\n",
    "print('distance: %r' % distance)\n",
    "\n",
    "distance = model.wmdistance(sentence_obama, sentence_orange)\n",
    "print('distance = %.4f' % distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "izA3-6kffbdT"
   },
   "source": [
    "## DistillBERT sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "uShDVUAFZUwz"
   },
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "To9ENLU90WGl"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "fvFvBLJV0Dkv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import torch\n",
    "import transformers as ppb\n",
    "import urllib.request\n",
    "import os\n",
    "import zipfile\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "KDW7swH8-Py5"
   },
   "outputs": [],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "PkHh_CVP-BAE"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', delimiter='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "gTM3hOHW4hUY"
   },
   "outputs": [],
   "source": [
    "batch_1 = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "6t-svuwwGo3K"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "RUHzX36pSg3v"
   },
   "outputs": [],
   "source": [
    "batch_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "jGvcfcCP5xpZ"
   },
   "outputs": [],
   "source": [
    "batch_1[1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "q1InADgf5xm2"
   },
   "outputs": [],
   "source": [
    "# DistilBERT:\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "\n",
    "## Bert\n",
    "#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "\n",
    "# chargement modèle et tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "Dg82ndBA5xlN"
   },
   "outputs": [],
   "source": [
    "# Tokenization de nos phrases pour distillBERT\n",
    "tokenized = batch_1[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "URn-DWJt5xhP"
   },
   "outputs": [],
   "source": [
    "# Padding afin que chaque phrase fasse la même taille\n",
    "\n",
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "jdi7uXo95xeq"
   },
   "outputs": [],
   "source": [
    "np.array(padded).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "4K_iGRNa_Ozc"
   },
   "outputs": [],
   "source": [
    "# On cache ce padding\n",
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "39UVjAV56PJz"
   },
   "outputs": [],
   "source": [
    "# On applique le modèle sur nos token avec le masque\n",
    "model.eval()\n",
    "input_ids = torch.tensor(padded)  \n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "su7oNNFXce0f"
   },
   "outputs": [],
   "source": [
    "np.shape(last_hidden_states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "C9t60At16PVs"
   },
   "outputs": [],
   "source": [
    "# Chaque token obtient un vecteur, ici, seul le token special 'cls' en position 1 nous intéresse\n",
    "features = last_hidden_states[0][:,0,:].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "JD3fX2yh6PTx"
   },
   "outputs": [],
   "source": [
    "labels = batch_1[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "0IKS9JAJugaX"
   },
   "source": [
    "### Entrainnement d'une reg log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "ddAqbkoU6PP9"
   },
   "outputs": [],
   "source": [
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "gG-EVWx4CzBc"
   },
   "outputs": [],
   "source": [
    "lr_clf = LogisticRegression(C=1)\n",
    "lr_clf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "iCoyxRJ7ECTA"
   },
   "outputs": [],
   "source": [
    "lr_clf.score(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "xR8P_BWfugah"
   },
   "source": [
    "# Comparaison des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "UF2kj4DSZeIS"
   },
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "nKLe8lENugai"
   },
   "outputs": [],
   "source": [
    "am\n",
    "a\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "elhJ6K3wugak"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Session 2 - embeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
