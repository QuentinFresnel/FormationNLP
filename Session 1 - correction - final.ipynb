{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette première session de NLP va vous donner les bases de la manipulation de texte en vue du data challenge qui vous attend après l'été !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download fr_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing avec spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement de la librairie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Importer la classe `French` de `spacy.lang.fr`\n",
    "- Créer l'objet `nlp` avec le constructeur de la classe `French`\n",
    "- Créer un `doc` et afficher son texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ceci est une phrase narcissique puisqu'elle ne parle que d'elle-même.\n"
     ]
    }
   ],
   "source": [
    "# Importez la classe de langage French\n",
    "from spacy.lang.fr import French\n",
    "\n",
    "# Créez l'objet nlp\n",
    "nlp = French()\n",
    "\n",
    "# Traitement du texte\n",
    "doc = nlp(\"Ceci est une phrase narcissique puisqu'elle \"\n",
    "          \"ne parle que d'elle-même.\")\n",
    "\n",
    "# Affichez le texte du document\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**L'objet NLP**\n",
    "\n",
    "- contient le 'pipeline' des calculs\n",
    "- inclut des règles spécifiques au langage pour la tokenization, etc.\n",
    "\n",
    "**L'objet doc**\n",
    "\n",
    "Contient un document, c'est-à-dire un ensemble de *tokens*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulation d'un objet `Doc`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objet `Doc`se manipule comme une liste. \n",
    "\n",
    "A l'aide des `[]`:\n",
    "- Accedez au premier token de doc\n",
    "- Selectionnez les mots `'est une'` dans une slice\n",
    "- Selectionnez les mots entre `'est une phrase narcissique'` dans une slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "est une\n",
      "est une phrase narcissique\n"
     ]
    }
   ],
   "source": [
    "# A slice of the Doc for 'est une'\n",
    "est_une = doc[1:3]\n",
    "print(est_une.text)\n",
    "\n",
    "# A slice of the Doc for 'est une phrase narcissique'\n",
    "est_une_phrase_narcissique = doc[1:5]\n",
    "print(est_une_phrase_narcissique.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.fr import French\n",
    "\n",
    "# Charger English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = French()\n",
    "\n",
    "text = \"\"\"L'apprentissage de la data science requiert de ne pas se décourager.\n",
    "Les défis et les revers ne sont pas des échecs, ils font simplement partie du voyage!\"\"\"\n",
    "\n",
    "# L'objet \"nlp\" est utilisé pour créer un document avec des annotations linguistiques\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"L'\", 'apprentissage', 'de', 'la', 'data', 'science', 'requiert', 'de', 'ne', 'pas', 'se', 'décourager', '.', '\\n', 'Les', 'défis', 'et', 'les', 'revers', 'ne', 'sont', 'pas', 'des', 'échecs', ',', 'ils', 'font', 'simplement', 'partie', 'du', 'voyage', '!']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization de mots\n",
    "\n",
    "# Créer la liste des tokens de mots\n",
    "token_list = []\n",
    "for token in doc:\n",
    "    token_list.append(token.text)\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cet exemple, vous allez utiliser les objets `Doc` et `Token` de spaCy, et des attributs lexicaux pour identifier des pourcentages dans un texte. Vous devez chercher deux tokens qui se suivent: un nombre et un signe pourcentage.\n",
    "\n",
    "- Utiliser l'attribut de token `like_num` pour voir si le token dans le document ressemble à un nombre.\n",
    "- Lire le token qui suit immédiatement ce token. L'index du token suivant dans le `doc` est `token.i + 1`.\n",
    "- Regarder si l'attribut `text` du token suivant est un signe `”%“`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the text\n",
    "doc = nlp(\n",
    "    \"En 1990, plus de 60% de la population d'Asie orientale vivait dans l'extrême pauvreté. \"\n",
    "    \"Cela représente désormais moins de 4%.\"\n",
    ")\n",
    "\n",
    "# Iterate over the tokens in the doc\n",
    "for token in doc:\n",
    "    # Check if the token resembles a number\n",
    "    if token.like_num:\n",
    "        # Get the next token in the document\n",
    "        next_token = doc[token.i + 1]\n",
    "        # Check if the next token's text equals \"%\"\n",
    "        if next_token.text == \"%\":\n",
    "            print(\"Pourcentage trouvé:\", token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suppression des stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "\n",
    "print(list(STOP_WORDS)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implémentation des stop words:\n",
    "filtered_sent=[]\n",
    "\n",
    "#  L'objet \"nlp\" est utilisé pour créer des documents avec annotations\n",
    "doc = nlp(text)\n",
    "\n",
    "# filtering stop words\n",
    "for word in doc:\n",
    "    if word.is_stop == False and word.is_punct == False:\n",
    "        filtered_sent.append(word)\n",
    "print(\"Filtered Sentence:\",filtered_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)\n",
    "tokens = [token for token in doc if not token.text in STOP_WORDS and not token.is_punct]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implémenter la lemmatization en anglais\n",
    "\n",
    "doc = nlp(\"Enedis entretient plus d'1,34 million de kilomètres de câbles électriques.\")\n",
    "\n",
    "for word in doc:\n",
    "    print(word, word.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy-lefff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implémenter la lemmatization en français\n",
    "\n",
    "import spacy\n",
    "from spacy_lefff import LefffLemmatizer, POSTagger\n",
    "\n",
    "## Sans POStagger\n",
    "# nlp = spacy.load('fr_core_news_md')\n",
    "# french_lemmatizer = LefffLemmatizer(default=True)\n",
    "# nlp.add_pipe(french_lemmatizer, name='lefff')\n",
    "\n",
    "## Avec POStagger\n",
    "pos = POSTagger()\n",
    "french_lemmatizer = LefffLemmatizer(after_melt = True, default = True)\n",
    "nlp.add_pipe(pos, name='pos')\n",
    "nlp.add_pipe(french_lemmatizer, name='lefff', after='pos')\n",
    "\n",
    "doc = nlp(\"Enedis entretient plus d'1,34 million de kilomètres de câbles électriques.\")\n",
    "\n",
    "for word in doc:\n",
    "    print(word.text, word._.lefff_lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etiquetage morpho-syntaxique (POS tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Que sont les modèles statistiques ?**\n",
    "\n",
    "- Ils permettent à spaCy de prédir les attributs linguistiques dans un contexte\n",
    "    - Tags Part-of-speech (POS)\n",
    "    - Dépendances syntaxiques\n",
    "    - Entités nommées\n",
    "- Ils sont entraînés sur des textes labellisés\n",
    "- Ils peuvent être mis à jour avec des exemples additionnels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "import fr_core_news_md\n",
    "\n",
    "nlp = fr_core_news_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Elle a mangé la pizza\")\n",
    "\n",
    "for token in doc:\n",
    "    # Afficher le texte et les 'part-of-speech' tags\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain(\"PRON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des dépendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)\n",
    "\n",
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain(\"nsubj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choisir une phrase et demander à spaCy d'afficher les POS tags et les relations entre les mots\n",
    "\n",
    "doc = nlp(\"En France les compteurs intelligents seront bientôt entièrement déployés.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)\n",
    "\n",
    "# Tester les styles 'dep' et 'ent'\n",
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choisir une phrase et demander à spaCy d'afficher les POS tags et les relations entre les mots\n",
    "\n",
    "doc = nlp(\"En France les compteurs intelligents seront bientôt entièrement déployés.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)\n",
    "\n",
    "# Changer la couleur en bleu et demander à spaCy d'afficher les groupes nominaux comme un même ensemble\n",
    "displacy.render(doc, style=\"dep\", options = {\"color\": \"blue\", \"collapse_phrases\" : True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Détection d'entités"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a text\n",
    "doc = nlp(\"Enedis cherche de nouveaux bureaux à Vaison-La-Romaine \"\n",
    "          \"avec l'aide de François Cordel.\")\n",
    "\n",
    "# Iterate over the predicted entities\n",
    "for ent in doc.ents:\n",
    "    # Print the entity text and its label\n",
    "    print(ent.text, ent.label_)\n",
    "    \n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain(\"ORG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain(\"LOC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain(\"PER\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking : noun chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Il est important pour les utilisateurs de véhicules électriques de disposer facilement \"\n",
    "          \"d'infrastructures de recharge pratiques.\")\n",
    "\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(\"Groupe nominal :\", chunk.text, \"; Nom : \", chunk.root.text, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nuage de mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q6UZFmRZlG2a"
   },
   "source": [
    "Nous allons scrapper la page wikipedia d'Enedis et créer un nuage de mots pour comprendre ce qu'il ressort de l'article. \n",
    "\n",
    "- Utilisez le code suivant pour charger la page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wfgPrBkwxqD2"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "r = requests.get(\"https://fr.wikipedia.org/wiki/Enedis\")\n",
    "html_doc = r.text\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "corpus = []\n",
    "for p in soup.find_all(\"p\"):\n",
    "    corpus.append(p.text)\n",
    "\n",
    "join_corpus = \" \".join(corpus)\n",
    "clean_corpus = join_corpus.strip().encode(\"utf-8\").decode(\"utf-8\")\n",
    "clean_corpus[:70]+\"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0tuzXlnM99ba"
   },
   "source": [
    "* Importez maintenant `fr_core_news_sm` --> [Documentation](https://spacy.io/models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ByvnYdaD7vBU"
   },
   "outputs": [],
   "source": [
    "# Import de Spacy et initialisation à Francais\n",
    "import fr_core_news_sm\n",
    "nlp = fr_core_news_sm.load()\n",
    "print(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ni8wH3DG-81a"
   },
   "source": [
    "* Incorporez maintenant votre corpus complet dans spacy via `fr_core_news_sm.load()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JyH8NsmN8fuQ"
   },
   "outputs": [],
   "source": [
    "# Création d'une variable doc contenant les textes du corpus\n",
    "doc = nlp(clean_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Installez la librairie wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zYxAusV8_Igf"
   },
   "source": [
    "* Avec la librairie `wordcloud`, créez un premier nuage de mots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1726,
     "status": "ok",
     "timestamp": 1582107372818,
     "user": {
      "displayName": "Antoine Krajnc",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mC4XzNDVGvURzl4T5duDbMr6bUdhkYkDul_37G0OA=s64",
      "userId": "08465960390418158788"
     },
     "user_tz": -60
    },
    "id": "23x9RYa-8isd",
    "outputId": "8b5ffd2c-308f-473b-804f-7328eec81f0a"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "wd = WordCloud()\n",
    "cloud = wd.generate(doc.text)\n",
    "plt.imshow(cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XZyM_xpj_NHs"
   },
   "source": [
    "* Importez maintenant les stop-words présents dans `spacy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l4Zl2vxP7w17"
   },
   "outputs": [],
   "source": [
    "# Import des Stop words\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J0DoC9FL_SQ2"
   },
   "source": [
    "* Faites à nouveau un nuage de mots sans les stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = WordCloud(stopwords=STOP_WORDS)\n",
    "cloud = wd.generate(doc.text)\n",
    "plt.imshow(cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z6JG4SSp_WFG"
   },
   "source": [
    "* Enfin nous aurions besoin d'avoir un corpus de mots lemmatisés, recréez un corpus de mots lemmatisés et visualisez votre nouveau nuage de mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = [token.lemma_ for token in doc if token.lemma_ not in STOP_WORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4YEXGT9MHODh"
   },
   "outputs": [],
   "source": [
    "corpus_lemma = \" \".join(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2032,
     "status": "ok",
     "timestamp": 1582110215852,
     "user": {
      "displayName": "Antoine Krajnc",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mC4XzNDVGvURzl4T5duDbMr6bUdhkYkDul_37G0OA=s64",
      "userId": "08465960390418158788"
     },
     "user_tz": -60
    },
    "id": "ADnFJsRjG-4f",
    "outputId": "ad82bb24-2462-4fc3-99db-456e0b112384"
   },
   "outputs": [],
   "source": [
    "wd = WordCloud()\n",
    "cloud = wd.generate(corpus_lemma)\n",
    "plt.imshow(cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification de texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm\n",
    "#!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python -m spacy download fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "from spacy.lang.fr import French\n",
    "\n",
    "# Création de la liste de signes de ponctuation\n",
    "punctuations = string.punctuation\n",
    "stop_words = spacy.lang.fr.stop_words.STOP_WORDS\n",
    "\n",
    "# Charger English tokenizer, tagger, parser, NER and word vectors\n",
    "parser = French()\n",
    "\n",
    "# Création de la fonction de tokenization\n",
    "def spacy_tokenizer(sentence):\n",
    "    # Création du token, utilisé pour créer des documents avec des annotations linguistiques\n",
    "    mytokens = parser(sentence)\n",
    "\n",
    "    # Lemmatisation de chaque token et conversion en caractères minuscules\n",
    "    mytokens = [word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "\n",
    "    # Suppression des stop words\n",
    "    mytokens = [word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "\n",
    "    # On retourne la liste des tokens préparés\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer personnalisé grâce à spaCy\n",
    "class predictors(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        # Nettoyage du texte\n",
    "        return [clean_text(text) for text in X]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "\n",
    "# Fonction basique pour nettoyer le texte\n",
    "def clean_text(text):\n",
    "    # Suppression des espaces et conversion des caractères en minuscules\n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('restaurants.csv')\n",
    "\n",
    "X = df['texte'] # les données que l'on veut analyser\n",
    "ylabels = df['label_2'] # les labels, ou réponses, pour lesquels on veut tester notre modèle\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "# Création du pipeline en utilisant des Bag of Words\n",
    "pipe = Pipeline([('cleaner', predictors()),\n",
    "                 ('vectorizer', bow_vector),\n",
    "                 ('classifier', classifier)])\n",
    "\n",
    "# Génération du modèle\n",
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "# Predicting avec un dataset de test\n",
    "predicted = pipe.predict(X_test)\n",
    "\n",
    "# Précision du modèle\n",
    "print(\"Confusion matrix:\\n\",metrics.confusion_matrix(y_test, predicted))\n",
    "print(\"\\nLogistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "# print(\"Logistic Regression Precision and Recall:\",metrics.precision_score(y_test, predicted, average = None))\n",
    "# print(\"Logistic Regression Precision and Recall:\",metrics.recall_score(y_test, predicted, average = None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarque**\n",
    "\n",
    "La qualité des données initiales a un rôle fondamental.\n",
    "\n",
    "*Exemple*\n",
    "- `' '.join(sentences)` --> 75% accuracy\n",
    "- `'. '.join(sentences)` --> 80% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-SNE sur TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['texte'] # les données que l'on veut analyser\n",
    "ylabels = df['label_3'] # les labels, ou réponses, pour lesquels on veut tester notre modèle\n",
    "print(df.loc[:, ['texte','label_3']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On calcule le tf-idf pour chaque mot de chaque texte\n",
    "tfidf = tfidf_vector.fit_transform(X)\n",
    "print(tfidf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des résultats pour le prmeier commentaire\n",
    "\n",
    "# Premier commentaire\n",
    "print(df.loc[0, 'texte'])\n",
    "\n",
    "# TF-IDF pour le premier commentaire\n",
    "df_tfidf = pd.DataFrame(tfidf[0].T.todense(), index = tfidf_vector.get_feature_names(), columns=[\"tf_idf\"])\n",
    "df_tfidf.sort_values(by=[\"tf_idf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un dataframe regroupant les tf-idf et les labels\n",
    "df_tfidf = pd.DataFrame(tfidf.todense(), columns = tfidf_vector.get_feature_names())\n",
    "df_tfidf = pd.concat([df_tfidf, ylabels], axis=1)\n",
    "print(df_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE sur les données\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "tsne = TSNE(n_components=2, init = 'random', verbose=1, perplexity=7, n_iter=1000)\n",
    "tsne_results = tsne.fit_transform(df_tfidf.drop(['label_3'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On considère les deux dimensions les plus importantes\n",
    "df_tfidf['Dim 1'] = tsne_results[:,0]\n",
    "df_tfidf['Dim 2'] = tsne_results[:,1]\n",
    "\n",
    "# On projette les données sur les deux axes, en colorant les points avec les labels\n",
    "plt.figure(figsize=(16,10))\n",
    "sns.scatterplot(\n",
    "    x = \"Dim 1\", y = \"Dim 2\",\n",
    "    hue = \"label_3\",\n",
    "    palette = sns.color_palette(\"hls\", 3),\n",
    "    data = df_tfidf,\n",
    "    legend = \"full\",\n",
    "    alpha = 1.0\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
